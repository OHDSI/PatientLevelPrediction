[{"path":"/articles/AddingCustomFeatureEngineering.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Adding Custom Feature Engineering Functions","text":"vignette describes can add custom function feature engineering Observational Health Data Sciences Informatics (OHDSI) PatientLevelPrediction package. vignette assumes read comfortable building single patient level prediction models described vignette('BuildingPredictiveModels'). invite share new feature engineering functions OHDSI community GitHub repository.","code":""},{"path":"/articles/AddingCustomFeatureEngineering.html","id":"feature-engineering-function-code-structure","dir":"Articles","previous_headings":"","what":"Feature Engineering Function Code Structure","title":"Adding Custom Feature Engineering Functions","text":"make custom feature engineering function can used within PatientLevelPrediction need write two different functions. ‘create’ function ‘implement’ function. ‘create’ function, e.g., create<FeatureEngineeringFunctionName>, takes parameters feature engineering ‘implement’ function input, checks valid outputs list class ‘featureEngineeringSettings’ ‘fun’ attribute specifying ‘implement’ function call. ‘implement’ function, e.g., implement<FeatureEngineeringFunctionName>, must take input: trainData - list containing: covariateData: plpData$covariateDatarestricted training patients labels: data frame contain rowId(patient identifier) outcomeCount (class labels) folds: data.frame contains rowId (patient identifier) index (cross validation fold) featureEngineeringSettings - output create<FeatureEngineeringFunctionName> ‘implement’ function can manipulation trainData (adding new features removing features) must output trainData object containing new covariateData, labels folds training data patients.","code":""},{"path":"/articles/AddingCustomFeatureEngineering.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Adding Custom Feature Engineering Functions","text":"Let’s consider situation wish create age spline feature. make custom feature engineering function need write ‘create’ ‘implement’ R functions.","code":""},{"path":"/articles/AddingCustomFeatureEngineering.html","id":"create-function","dir":"Articles","previous_headings":"Example","what":"Create function","title":"Adding Custom Feature Engineering Functions","text":"age spline feature function create new feature using plpData$cohorts$ageYear column. implement restricted cubic spline requires specifying number knots. Therefore, inputs : knots - integer/double specifying number knots. now need create ‘implement’ function implementAgeSplines()","code":"createAgeSpline <- function(knots = 5) {   # create list of inputs to implement function   featureEngineeringSettings <- list(     knots = knots   )    # specify the function that will implement the sampling   attr(featureEngineeringSettings, \"fun\") <- \"implementAgeSplines\"    # make sure the object returned is of class \"sampleSettings\"   class(featureEngineeringSettings) <- \"featureEngineeringSettings\"   return(featureEngineeringSettings) }"},{"path":"/articles/AddingCustomFeatureEngineering.html","id":"implement-function","dir":"Articles","previous_headings":"Example","what":"Implement function","title":"Adding Custom Feature Engineering Functions","text":"‘implement’ functions must take input trainData featureEngineeringSettings (output ‘create’ function). must return trainData object containing new covariateData, labels folds. example, createAgeSpline() return list ‘knots’. featureEngineeringSettings therefore contains .","code":"implementAgeSplines <- function(trainData, featureEngineeringSettings, model = NULL) {   # if there is a model, it means this function is called through applyFeatureengineering, meaning it   # should apply the model fitten on training data to the test data   if (is.null(model)) {     knots <- featureEngineeringSettings$knots     ageData <- trainData$labels     y <- ageData$outcomeCount     X <- ageData$ageYear     model <- mgcv::gam(       y ~ s(X, bs = \"cr\", k = knots, m = 2)     )     newData <- data.frame(       rowId = ageData$rowId,       covariateId = 2002,       covariateValue = model$fitted.values     )   } else {     ageData <- trainData$labels     X <- trainData$labels$ageYear     y <- ageData$outcomeCount     newData <- data.frame(y = y, X = X)     yHat <- predict(model, newData)     newData <- data.frame(       rowId = trainData$labels$rowId,       covariateId = 2002,       covariateValue = yHat     )   }    # remove existing age if in covariates   trainData$covariateData$covariates <- trainData$covariateData$covariates |>     dplyr::filter(!.data$covariateId %in% c(1002))    # update covRef   Andromeda::appendToTable(     trainData$covariateData$covariateRef,     data.frame(       covariateId = 2002,       covariateName = \"Cubic restricted age splines\",       analysisId = 2,       conceptId = 2002     )   )    # update covariates   Andromeda::appendToTable(trainData$covariateData$covariates, newData)    featureEngineering <- list(     funct = \"implementAgeSplines\",     settings = list(       featureEngineeringSettings = featureEngineeringSettings,       model = model     )   )    attr(trainData$covariateData, \"metaData\")$featureEngineering <- listAppend(     attr(trainData$covariateData, \"metaData\")$featureEngineering,     featureEngineering   )    return(trainData) }"},{"path":"/articles/AddingCustomFeatureEngineering.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Adding Custom Feature Engineering Functions","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/AddingCustomModels.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Adding Custom Patient-Level Prediction Algorithms","text":"vignette describes can add custom algorithms Observational Health Data Sciencs Informatics (OHDSI) PatientLevelPrediction package. allows fully leverage OHDSI PatientLevelPrediction framework model development validation. vignette assumes read comfortable building single patient level prediction models described vignette('BuildingPredictiveModels'). invite share new algorithms OHDSI community GitHub repository.","code":""},{"path":"/articles/AddingCustomModels.html","id":"algorithm-code-structure","dir":"Articles","previous_headings":"","what":"Algorithm Code Structure","title":"Adding Custom Patient-Level Prediction Algorithms","text":"algorithm package implemented <Name>.R file, e.g. KNN.R, containing set<Name> function, fit<Name> function predict<Name> function. Occasionally fit prediction functions may reused (using R classifier see RClassifier.R using scikit-learn classifier see SklearnClassifier.R). now describe functions detail .","code":""},{"path":"/articles/AddingCustomModels.html","id":"set","dir":"Articles","previous_headings":"Algorithm Code Structure","what":"Set","title":"Adding Custom Patient-Level Prediction Algorithms","text":"set<Name> function takes input different hyper-parameter values grid search training. output functions needs list class modelSettings containing: param - combinations hyper-parameter values input fitFunction - string specifying function call fit model param object can setttings attribute containing extra settings. example specify model name seed used reproducibility: example, adding model called madeUp two hyper-parameters set function :","code":"attr(param, \"settings\") <- list(   seed = 12,   modelName = \"Special classifier\" ) setMadeUp <- function(a = c(1, 4, 10), b = 2, seed = NULL) {   # add input checks here...    param <- split(     expand.grid(       a = a,       b = b     ),     1:(length(a) * length(b))   )    attr(param, \"settings\") <- list(     modelName = \"Made Up\",     requiresDenseMatrix = TRUE,     seed = seed   )    # now create list of all combinations:   result <- list(     fitFunction = \"fitMadeUp\", # this will be called to train the made up model     param = param   )   class(result) <- \"modelSettings\"    return(result) }"},{"path":"/articles/AddingCustomModels.html","id":"fit","dir":"Articles","previous_headings":"Algorithm Code Structure","what":"Fit","title":"Adding Custom Patient-Level Prediction Algorithms","text":"function train custom model parameter entry, pick best parameters train final model setting. fit<Model> inputs: trainData - list containing covariateData, labels folds training population param - hyper-parameters list combinations search - type hyper-parameter search analysisId - identifier analysis fit function return list class plpModel following objects: model - trained model (location model R object) prediction - data.frame object trainData$labels plus extra column name ‘value’ corresponding predicted risk outcome time--risk. featureEngineering - feature engineering settings e.g., attr(trainData$covariateData, \"metaData\")$featureEngineering, tidyCovariates - preprocessing settings e.g., attr(trainData$covariateData, \"metaData\")$tidyCovariateDataSettings, requireDenseMatrix - model require dense matrix? e.g., attr(param, ‘settings’)$requiresDenseMatrix, targetId - id target cohort outcomeId - id outcome cohort plpDataSettings - plpData settings e.g., attr(trainData, “metaData”)$plpDataSettings covariateSettings - covariate settings e.g., attr(trainData, “metaData”)$covariateSettings populationSettings - population settings e.g., attr(trainData, “metaData”)$populationSettings, featureEngineeringSettings - feature engineering settings e.g., attr(trainData$covariateData, \"metaData\")$featureEngineeringSettings, preprocessSettings - preprocessing settings e.g., attr(trainData$covariateData, \"metaData\")$preprocessSettings, modelSettings = list containing: model (model name), param (hyper-parameter search list), finalModelParameters (final model hyper-parameters), extraSettings (extra settings) splitSettings - split settings e.g., attr(trainData, “metaData”)$splitSettings, sampleSettings - sample settings e.g., attr(trainData, “metaData”)$sampleSettings analysisId - identifier analysis developmentDatabase - database used develop model attrition - attrition trainingTime - long took train model trainingDate - date model training hyperParamSearch - hyper-parameter search used train model objects specific training covariateImportance - data.frame containing columns ‘covariateId’, ‘covariateValue’ (variable importance) ‘columnId’ (column number variable need mapped implementing model) additon plpModel requires two attributes: predictionFunction - name function used make predictions modelType - whether model ‘binary’ ‘survival’ example attr(result, 'predictionFunction') <- 'madeupPrediction' means model applied new data, ‘madeupPrediction’ function called make predictions. doesnt exist, model fail. attribute modelType attr(result, 'modelType') <- 'binary' needed evaluating model ensure correct evaluation applied. Currently evaluation supports ‘binary’ ‘survival’ modelType. Note: new modelType desired, evalaution code within PatientLevelPrediction must updated specify new type evaluated. requires making edits PatientLevelPrediction making pull request PatientLevelPrediction github. evaluation one customization evaluation must standardized enable comparison across similar models. full example custom ‘binary’ classifier fit function : make fitMadeUp function cleaner adding helper function MadeUp.R file called fit function (example function run cross validation). important ensure valid prediction function (one specified attr(result, \"predictionFunction\") <- \"madeupPrediction\" madeupPrediction()) specified .","code":"fitMadeUp <- function(trainData, modelSettings, search, analysisId) {   param <- modelSettings$param    # **************** code to train the model here   # trainedModel <- this code should apply each hyper-parameter combination   # (param[[i]]) using the specified search (e.g., cross validation)   #                 then pick out the best hyper-parameter setting   #                 and finally fit a model on the whole train data using the   #                 optimal hyper-parameter settings   # ****************    # **************** code to apply the model to trainData   # prediction <- code to apply trainedModel to trainData   # ****************    # **************** code to get variable importance (if possible)   # varImp <- code to get importance of each variable in trainedModel   # ****************     # construct the standard output for a model:   result <- list(     model = trainedModel,     prediction = prediction, # the train and maybe the cross validation predictions for the trainData     preprocessing = list(       featureEngineering = attr(trainData$covariateData, \"metaData\")$featureEngineering,       tidyCovariates = attr(trainData$covariateData, \"metaData\")$tidyCovariateDataSettings,       requireDenseMatrix = attr(param, \"settings\")$requiresDenseMatrix,     ),     modelDesign = list(       outcomeId = attr(trainData, \"metaData\")$outcomeId,       targetId = attr(trainData, \"metaData\")$targetId,       plpDataSettings = attr(trainData, \"metaData\")$plpDataSettings,       covariateSettings = attr(trainData, \"metaData\")$covariateSettings,       populationSettings = attr(trainData, \"metaData\")$populationSettings,       featureEngineeringSettings = attr(trainData$covariateData, \"metaData\")$featureEngineeringSettings,       prerocessSettings = attr(trainData$covariateData, \"metaData\")$prerocessSettings,       modelSettings = list(         model = attr(param, \"settings\")$modelName, # the model name         param = param,         finalModelParameters = param[[bestInd]], # best hyper-parameters         extraSettings = attr(param, \"settings\")       ),       splitSettings = attr(trainData, \"metaData\")$splitSettings,       sampleSettings = attr(trainData, \"metaData\")$sampleSettings     ),     trainDetails = list(       analysisId = analysisId,       developmentDatabase = attr(trainData, \"metaData\")$cdmDatabaseSchema,       attrition = attr(trainData, \"metaData\")$attrition,       trainingTime = timeToTrain, # how long it took to train the model       trainingDate = Sys.Date(),       hyperParamSearch = hyperSummary # the hyper-parameters and performance data.frame     ),     covariateImportance = merge(trainData$covariateData$covariateRef, varImp, by = \"covariateId\") # add variable importance to covariateRef if possible   )   class(result) <- \"plpModel\"   attr(result, \"predictionFunction\") <- \"madeupPrediction\"   attr(result, \"modelType\") <- \"binary\"   return(result) }"},{"path":"/articles/AddingCustomModels.html","id":"predict","dir":"Articles","previous_headings":"Algorithm Code Structure","what":"Predict","title":"Adding Custom Patient-Level Prediction Algorithms","text":"prediction function takes input plpModel returned fit, new data corresponding cohort. returns data.frame columns cohort additional column: value - predicted risk plpModel patient cohort example:","code":"madeupPrediction <- function(plpModel, data, cohort) {   # ************* code to do prediction for each rowId in cohort   # predictionValues <- code to do prediction here returning the predicted risk   #               (value) for each rowId in cohort   #**************    prediction <- merge(cohort, predictionValues, by = \"rowId\")   attr(prediction, \"metaData\") <- list(modelType = attr(plpModel, \"modelType\"))   return(prediction) }"},{"path":"/articles/AddingCustomModels.html","id":"algorithm-example","dir":"Articles","previous_headings":"","what":"Algorithm Example","title":"Adding Custom Patient-Level Prediction Algorithms","text":"fully functional algorithm example given, however highly recommend look available algorithms package (see GradientBoostingMachine.R set function, RClassifier.R fit prediction function R classifiers).","code":""},{"path":"/articles/AddingCustomModels.html","id":"set-1","dir":"Articles","previous_headings":"Algorithm Example","what":"Set","title":"Adding Custom Patient-Level Prediction Algorithms","text":"","code":"setMadeUp <- function(a = c(1, 4, 6), b = 2, seed = NULL) {   # add input checks here...    if (is.null(seed)) {     seed <- sample(100000, 1)   }    param <- split(     expand.grid(       a = a,       b = b     ),     1:(length(a) * length(b))   )    attr(param, \"settings\") <- list(     modelName = \"Made Up\",     requiresDenseMatrix = TRUE,     seed = seed   )    # now create list of all combinations:   result <- list(     fitFunction = \"fitMadeUp\", # this will be called to train the made up model     param = param   )   class(result) <- \"modelSettings\"    return(result) }"},{"path":"/articles/AddingCustomModels.html","id":"fit-1","dir":"Articles","previous_headings":"Algorithm Example","what":"Fit","title":"Adding Custom Patient-Level Prediction Algorithms","text":"","code":"fitMadeUp <- function(trainData, modelSettings, search, analysisId) {   # set the seed for reproducibility   param <- modelSettings$param   set.seed(attr(param, \"settings\")$seed)    # add folds to labels:   trainData$labels <- merge(trainData$labels, trainData$folds, by = \"rowId\")   # convert data into sparse R Matrix:   mappedData <- toSparseM(trainData, map = NULL)   matrixData <- mappedData$dataMatrix   labels <- mappedData$labels   covariateRef <- mappedData$covariateRef    # ============= STEP 1 ======================================   # pick the best hyper-params and then do final training on all data...   writeLines(\"Cross validation\")   paramSel <- lapply(     param,     function(x) {       do.call(         madeUpModel,         list(           param = x,           final = FALSE,           data = matrixData,           labels = labels         )       )     }   )   hyperSummary <- do.call(rbind, lapply(paramSel, function(x) x$hyperSum))   hyperSummary <- as.data.frame(hyperSummary)   hyperSummary$auc <- unlist(lapply(paramSel, function(x) x$auc))   paramSel <- unlist(lapply(paramSel, function(x) x$auc))   bestInd <- which.max(paramSel)    # get cross val prediction for best hyper-parameters   prediction <- param.sel[[bestInd]]$prediction   prediction$evaluationType <- \"CV\"    writeLines(\"final train\")   finalResult <- do.call(     madeUpModel,     list(       param = param[[bestInd]],       final = TRUE,       data = matrixData,       labels = labels     )   )    trainedModel <- finalResult$model    # prediction risk on training data:   finalResult$prediction$evaluationType <- \"Train\"    # get CV and train prediction   prediction <- rbind(prediction, finalResult$prediction)    varImp <- covariateRef %>% dplyr::collect()   # no feature importance available   vqrImp$covariateValue <- 0    timeToTrain <- Sys.time() - start    # construct the standard output for a model:   result <- list(     model = trainedModel,     prediction = prediction,     preprocessing = list(       featureEngineering = attr(trainData$covariateData, \"metaData\")$featureEngineering,       tidyCovariates = attr(trainData$covariateData, \"metaData\")$tidyCovariateDataSettings,       requireDenseMatrix = attr(param, \"settings\")$requiresDenseMatrix,     ),     modelDesign = list(       outcomeId = attr(trainData, \"metaData\")$outcomeId,       targetId = attr(trainData, \"metaData\")$targetId,       plpDataSettings = attr(trainData, \"metaData\")$plpDataSettings,       covariateSettings = attr(trainData, \"metaData\")$covariateSettings,       populationSettings = attr(trainData, \"metaData\")$populationSettings,       featureEngineeringSettings = attr(trainData$covariateData, \"metaData\")$featureEngineeringSettings,       prerocessSettings = attr(trainData$covariateData, \"metaData\")$prerocessSettings,       modelSettings = list(         model = attr(param, \"settings\")$modelName, # the model name         param = param,         finalModelParameters = param[[bestInd]], # best hyper-parameters         extraSettings = attr(param, \"settings\")       ),       splitSettings = attr(trainData, \"metaData\")$splitSettings,       sampleSettings = attr(trainData, \"metaData\")$sampleSettings     ),     trainDetails = list(       analysisId = analysisId,       developmentDatabase = attr(trainData, \"metaData\")$cdmDatabaseSchema,       attrition = attr(trainData, \"metaData\")$attrition,       trainingTime = timeToTrain, # how long it took to train the model       trainingDate = Sys.Date(),       hyperParamSearch = hyperSummary # the hyper-parameters and performance data.frame     ),     covariateImportance = merge(trainData$covariateData$covariateRef, varImp, by = \"covariateId\") # add variable importance to covariateRef if possible   )   class(result) <- \"plpModel\"   attr(result, \"predictionFunction\") <- \"madeupPrediction\"   attr(result, \"modelType\") <- \"binary\"   return(result) }"},{"path":"/articles/AddingCustomModels.html","id":"helpers","dir":"Articles","previous_headings":"Algorithm Example","what":"Helpers","title":"Adding Custom Patient-Level Prediction Algorithms","text":"fit model helper function madeUpModel called, function trains model given data, labels hyper-parameters.","code":"madeUpModel <- function(param, data, final = FALSE, labels) {   if (final == FALSE) {     # add value column to store all predictions     labels$value <- rep(0, nrow(labels))     attr(labels, \"metaData\") <- list(modelType = \"binary\")      foldPerm <- c() # this holds CV aucs     for (index in 1:max(labels$index)) {       model <- madeup::model(         x = data[labels$index != index, ], # remove left out fold         y = labels$outcomeCount[labels$index != index],         a = param$a,         b = param$b       )        # predict on left out fold       pred <- stats::predict(model, data[labels$index == index, ])       labels$value[labels$index == index] <- pred        # calculate auc on help out fold       aucVal <- computeAuc(labels[labels$index == index, ])       foldPerm <- c(foldPerm, aucVal)     }     auc <- computeAuc(labels) # overal AUC   } else {     model <- madeup::model(       x = data,       y = labels$outcomeCount,       a = param$a,       b = param$b     )      pred <- stats::predict(model, data)     labels$value <- pred     attr(labels, \"metaData\") <- list(modelType = \"binary\")     auc <- computeAuc(labels)     foldPerm <- auc   }    result <- list(     model = model,     auc = auc,     prediction = labels,     hyperSum = c(a = a, b = b, fold_auc = foldPerm)   )    return(result) }"},{"path":"/articles/AddingCustomModels.html","id":"predict-1","dir":"Articles","previous_headings":"Algorithm Example","what":"Predict","title":"Adding Custom Patient-Level Prediction Algorithms","text":"final step create predict function model. example predeiction function attr(result, 'predictionFunction') <- 'madeupPrediction' madeupPrediction, madeupPrediction function required applying model. predict function needs take input plpModel returned fit function, new data apply model cohort specifying patients interest make prediction . madeup model uses standard R prediction, prediction function xgboost, added new prediction function instead made predictionFunction result returned fitMadeUpModel attr(result, 'predictionFunction') <- 'predictXgboost'.","code":"madeupPrediction <- function(plpModel, data, cohort) {   if (class(data) == \"plpData\") {     # convert     matrixObjects <- toSparseM(       plpData = data,       cohort = cohort,       map = plpModel$covariateImportance %>%         dplyr::select(\"columnId\", \"covariateId\")     )      newData <- matrixObjects$dataMatrix     cohort <- matrixObjects$labels   } else {     newData <- data   }    if (class(plpModel) == \"plpModel\") {     model <- plpModel$model   } else {     model <- plpModel   }    cohort$value <- stats::predict(model, newData)    # fix the rowIds to be the old ones   # now use the originalRowId and remove the matrix rowId   cohort <- cohort %>%     dplyr::select(-\"rowId\") %>%     dplyr::rename(rowId = \"originalRowId\")    attr(cohort, \"metaData\") <- list(modelType = attr(plpModel, \"modelType\"))   return(cohort) }"},{"path":"/articles/AddingCustomModels.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Adding Custom Patient-Level Prediction Algorithms","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/AddingCustomSamples.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Adding Custom Sampling Functions","text":"vignette describes can add custom function sampling target population Observational Health Data Sciencs Informatics (OHDSI) PatientLevelPrediction package. vignette assumes read comfortable building single patient level prediction models described vignette('BuildingPredictiveModels'). invite share new sample functions OHDSI community GitHub repository.","code":""},{"path":"/articles/AddingCustomSamples.html","id":"sample-function-code-structure","dir":"Articles","previous_headings":"","what":"Sample Function Code Structure","title":"Adding Custom Sampling Functions","text":"make sampling function can used within PatientLevelPrediction need write two different functions. ‘create’ function ‘implement’ function. ‘create’ function, e.g., create<SampleFunctionName>, takes parameters sample ‘implement’ function input, checks valid outputs list class ‘sampleSettings’ ‘fun’ attribute specifying ‘implement’ function call. ‘implement’ function, e.g., implement<SampleFunctionName>, must take input: * trainData - list containing: - covariateData: plpData$covariateData restricted training patients - labels: data frame contain rowId (patient identifier) outcomeCount (class labels) - folds: data.frame contains rowId (patient identifier) index (cross validation fold) * sampleSettings - output create<SampleFunctionName> ‘implement’ function can manipulation trainData (undersampling oversampling) must output trainData object containing covariateData, labels folds new training data sample.","code":""},{"path":"/articles/AddingCustomSamples.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Adding Custom Sampling Functions","text":"Let’s consider situation wish take random sample training data population. make custom sampling function need write ‘create’ ‘implement’ R functions.","code":""},{"path":"/articles/AddingCustomSamples.html","id":"create-function","dir":"Articles","previous_headings":"Example","what":"Create function","title":"Adding Custom Sampling Functions","text":"random sampling function randomly sample n patients trainData. Therefore, inputs : * n integer/double specifying number patients sample * sampleSeed integer/double specifying seed reproducibility now need create ‘implement’ function implementRandomSampleSettings()","code":"createRandomSampleSettings <- function(n = 10000,                                        sampleSeed = sample(10000, 1)) {   # add input checks   checkIsClass(n, c(\"numeric\", \"integer\"))   checkHigher(n, 0)   checkIsClass(sampleSeed, c(\"numeric\", \"integer\"))    # create list of inputs to implement function   sampleSettings <- list(     n = n,     sampleSeed = sampleSeed   )    # specify the function that will implement the sampling   attr(sampleSettings, \"fun\") <- \"implementRandomSampleSettings\"    # make sure the object returned is of class \"sampleSettings\"   class(sampleSettings) <- \"sampleSettings\"   return(sampleSettings) }"},{"path":"/articles/AddingCustomSamples.html","id":"implement-function","dir":"Articles","previous_headings":"Example","what":"Implement function","title":"Adding Custom Sampling Functions","text":"‘implement’ functions must take input trainData sampleSettings (output ‘create’ function). must return trainData object containing covariateData, labels folds. example, createRandomSampleSettings() return list ‘n’ ‘sampleSeed’. sampleSettings therefore contains .","code":"implementRandomSampleSettings <- function(trainData, sampleSettings) {   n <- sampleSettings$n   sampleSeed <- sampleSettings$sampleSeed    if (n > nrow(trainData$labels)) {     stop(\"Sample n bigger than training population\")   }    # set the seed for the randomization   set.seed(sampleSeed)    # now implement the code to do your desired sampling    sampleRowIds <- sample(trainData$labels$rowId, n)    sampleTrainData <- list()    sampleTrainData$labels <- trainData$labels %>%     dplyr::filter(.data$rowId %in% sampleRowIds) %>%     dplyr::collect()    sampleTrainData$folds <- trainData$folds %>%     dplyr::filter(.data$rowId %in% sampleRowIds) %>%     dplyr::collect()    sampleTrainData$covariateData <- Andromeda::andromeda()   sampleTrainData$covariateData$covariateRef <- trainData$covariateData$covariateRef   sampleTrainData$covariateData$covariates <- trainData$covariateData$covariates %>% dplyr::filter(.data$rowId %in% sampleRowIds)    # update metaData$populationSize   metaData <- attr(trainData$covariateData, \"metaData\")   metaData$populationSize <- n   attr(sampleTrainData$covariateData, \"metaData\") <- metaData    # make the cocvariateData the correct class   class(sampleTrainData$covariateData) <- \"CovariateData\"    # return the updated trainData   return(sampleTrainData) }"},{"path":"/articles/AddingCustomSamples.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Adding Custom Sampling Functions","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/AddingCustomSplitting.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Adding Custom Data Splitting","text":"vignette describes can add custom function splitting labelled data training data validation data Observational Health Data Sciencs Informatics (OHDSI) PatientLevelPrediction package. vignette assumes read comfortable building single patient level prediction models described vignette('BuildingPredictiveModels). invite share new data splitting functions OHDSI community GitHub repository.","code":""},{"path":"/articles/AddingCustomSplitting.html","id":"data-splitting-function-code-structure","dir":"Articles","previous_headings":"","what":"Data Splitting Function Code Structure","title":"Adding Custom Data Splitting","text":"make custom data splitting function can used within PatientLevelPrediction need write two different functions. ‘create’ function ‘implement’ function. ‘create’ function, e.g., create<DataSplittingFunction>, takes parameters data splitting ‘implement’ function input, checks valid outputs list class ‘splitSettings’ ‘fun’ attribute specifying ‘implement’ function call. ‘implement’ function, e.g., implement<DataSplittingFunction>, must take input: * population: data frame contain rowId (patient identifier), ageYear, gender outcomeCount (class labels) * splitSettings - output create<DataSplittingFunction> ‘implement’ function needs implement code assign rowId population splitId (<0 means train data, 0 means used >0 means training data value defining cross validation fold).","code":""},{"path":"/articles/AddingCustomSplitting.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Adding Custom Data Splitting","text":"Let’s consider situation wish create split females used train model males used evaluate model.","code":""},{"path":"/articles/AddingCustomSplitting.html","id":"create-function","dir":"Articles","previous_headings":"Example","what":"Create function","title":"Adding Custom Data Splitting","text":"gender split function requires single parameter, number folds used cross validation. Therefore create function single nfold input returns list class ‘splitSettings’ ‘fun’ attribute specifying ‘implement’ function use. now need create ‘implement’ function implementGenderSplit()","code":"createGenderSplit <- function(nfold) {   # create list of inputs to implement function   splitSettings <- list(nfold = nfold)    # specify the function that will implement the sampling   attr(splitSettings, \"fun\") <- \"implementGenderSplit\"    # make sure the object returned is of class \"sampleSettings\"   class(splitSettings) <- \"splitSettings\"   return(splitSettings) }"},{"path":"/articles/AddingCustomSplitting.html","id":"implement-function","dir":"Articles","previous_headings":"Example","what":"Implement function","title":"Adding Custom Data Splitting","text":"‘implement’ functions data splitting must take input population splitSettings (output ‘create’ function). must return data.frame containing columns: rowId index. index used determine whether patient (identifed rowId) test set (index = -1) train set (index > 0). train set, value corresponds cross validation fold. example, rowId 2 assigned index 5, means patient rowId 2 used train model fold 5.","code":"implementGenderSplit <- function(population, splitSettings) {   # find the people who are male:   males <- population$rowId[population$gender == 8507]   females <- population$rowId[population$gender == 8532]    splitIds <- data.frame(     rowId = c(males, females),     index = c(       rep(-1, length(males)),       sample(1:splitSettings$nfold, length(females), replace = TRUE)     )   )    # return the updated trainData   return(splitIds) }"},{"path":"/articles/AddingCustomSplitting.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Adding Custom Data Splitting","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/BenchmarkTasks.html","id":"benchmark-tasks-for-large-scale-empirical-analyses","dir":"Articles","previous_headings":"","what":"Benchmark Tasks For Large-Scale Empirical Analyses","title":"Benchmark Tasks","text":"provide set diverse prediction tasks can used evaluating impact model design choice developing models using observational data.","code":""},{"path":[]},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"paper, propose standardised framework patient-level prediction utilizes OMOP CDM standardized vocabularies, describe open-source software developed implementing framework’s pipeline. framework first enforce existing best practice guidelines enable open dissemination models can extensively validated across network OHDSI collaborators. One best practices see selection models study setting emperical question, .e. use data-driven approach try many settings. vignette describes can use Observational Health Data Sciencs Informatics (OHDSI) PatientLevelPrediction package automatically build multiple patient-level predictive models, e.g. different population settings, covariate settings, modelsetting. vignette assumes read comfortable building single patient level prediction models described vignette('BuildingPredictiveModels'). Note also possible generate Study Package directly Atlas allows multiple patient-level prediction analyses --scope vignette.","code":""},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"creating-a-model-design","dir":"Articles","previous_headings":"","what":"Creating a model design","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"first step specify model wish develop using createModelDesign function. function requires following: inputs model design","code":""},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"model-design-example-1","dir":"Articles","previous_headings":"Creating a model design","what":"Model design example 1","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"example, wanted predict outcome (id 2) occuring first time within 180 days target population index date (id 1). interested index dates betwrrn 2018-2020. Finally, want use age, gender 5 year buckets conditions features. model can specified :","code":"# Model 1 is only using data between 2018-2020: restrictPlpDataSettings <- createRestrictPlpDataSettings(   studyStartDate = \"20180101\",    studyEndDate = \"20191231\"   )  # predict outcome within 1 to 180 days after index # remove people with outcome prior and with < 365 days observation populationSettings <- createStudyPopulationSettings(   binary = TRUE,    firstExposureOnly = TRUE,    washoutPeriod = 365,    removeSubjectsWithPriorOutcome = TRUE,   priorOutcomeLookback = 9999,   requireTimeAtRisk = FALSE,    riskWindowStart = 1,    riskWindowEnd = 180 )  # use age/gender in groups and condition groups as features covariateSettings <- FeatureExtraction::createCovariateSettings(   useDemographicsGender = TRUE,    useDemographicsAgeGroup = TRUE,    useConditionGroupEraAnyTimePrior = TRUE )  modelDesign1 <- createModelDesign(   targetId = 1,    outcomeId = 2,    restrictPlpDataSettings = restrictPlpDataSettings,    populationSettings = populationSettings,    covariateSettings = covariateSettings,    featureEngineeringSettings = createFeatureEngineeringSettings(),   sampleSettings = createSampleSettings(),    splitSettings = createDefaultSplitSetting(),    preprocessSettings = createPreprocessSettings(),    modelSettings = setLassoLogisticRegression()   )"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"model-design-example-2","dir":"Articles","previous_headings":"Creating a model design","what":"Model design example 2","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"second example, want predict outcome (id 2) occuring first time within 730 days target population index date (id 1). want train random forest classifier. Finally, want use age, gender 5 year buckets, drug ingredients (groups) conditions features. model can specified :","code":"# Model 2 has no restrictions when extracting data restrictPlpDataSettings <- createRestrictPlpDataSettings(   )  # predict outcome within 1 to 730 days after index # remove people with outcome prior and with < 365 days observation populationSettings <- createStudyPopulationSettings(   binary = TRUE,    firstExposureOnly = TRUE,    washoutPeriod = 365,    removeSubjectsWithPriorOutcome = TRUE,   priorOutcomeLookback = 9999,   requireTimeAtRisk = FALSE,    riskWindowStart = 1,    riskWindowEnd = 730 )  # use age/gender in groups and condition/drug groups as features covariateSettings <- FeatureExtraction::createCovariateSettings(   useDemographicsGender = TRUE,    useDemographicsAgeGroup = TRUE,    useConditionGroupEraAnyTimePrior = TRUE,    useDrugGroupEraAnyTimePrior = TRUE  )  modelDesign2 <- createModelDesign(   targetId = 1,    outcomeId = 2,    restrictPlpDataSettings = restrictPlpDataSettings,    populationSettings = populationSettings,    covariateSettings = covariateSettings,    featureEngineeringSettings = createRandomForestFeatureSelection(ntrees = 500, maxDepth = 7),   sampleSettings = createSampleSettings(),    splitSettings = createDefaultSplitSetting(),    preprocessSettings = createPreprocessSettings(),    modelSettings = setRandomForest()   )"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"model-design-example-3","dir":"Articles","previous_headings":"Creating a model design","what":"Model design example 3","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"third example, want predict outcome (id 5) occuring cohort exposure target population (id 1). want train gradient boosting machine. Finally, want use age, gender 5 year buckets indications measurements taken features. model can specified :","code":"# Model 3 has no restrictions when extracting data restrictPlpDataSettings <- createRestrictPlpDataSettings(   )  # predict outcome during target cohort start/end  # remove people with  < 365 days observation populationSettings <- createStudyPopulationSettings(   binary = TRUE,    firstExposureOnly = TRUE,    washoutPeriod = 365,    removeSubjectsWithPriorOutcome = FALSE,   requireTimeAtRisk = FALSE,    riskWindowStart = 0,   startAnchor =  \"cohort start\",   riskWindowEnd = 0,    endAnchor = \"cohort end\" )  # use age/gender in groups and measurement indicators as features covariateSettings <- FeatureExtraction::createCovariateSettings(   useDemographicsGender = TRUE,    useDemographicsAgeGroup = TRUE,    useMeasurementAnyTimePrior = TRUE,   endDays = -1 )  modelDesign3 <- createModelDesign(   targetId = 1,    outcomeId = 5,    restrictPlpDataSettings = restrictPlpDataSettings,    populationSettings = populationSettings,    covariateSettings = covariateSettings,    featureEngineeringSettings = createFeatureEngineeringSettings(),   sampleSettings = createSampleSettings(),    splitSettings = createDefaultSplitSetting(),    preprocessSettings = createPreprocessSettings(),    modelSettings = setGradientBoostingMachine()   )"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"running-multiple-models","dir":"Articles","previous_headings":"","what":"Running multiple models","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"downloading loads data multiple plp analysis useful set Andromeda temp folder directory write access plenty space. options(andromedaTempFolder = \"c:/andromedaTemp\") run study requires setting connectionDetails object Next need specify cdmDatabaseSchema cdm database found workDatabaseSchema target population outcome cohorts need specify label database name: string shareable name database (shown OHDSI researchers results get transported). Now can run multiple patient-level prediction analysis: save plpData objects study “./PlpMultiOutput/plpData_T1_L” results “./PlpMultiOutput/Analysis_”. csv file named settings.csv found “./PlpMultiOutput” row prediction model developed points plpData settings used model development, also descriptions cohorts input user. Note reason run interrupted, e.g. error, new call runMultiplePlp continue restart remove output folder.","code":"dbms <- \"your dbms\" user <- \"your username\" pw <- \"your password\" server <- \"your server\" port <- \"your port\"  connectionDetails <- DatabaseConnector::createConnectionDetails(dbms = dbms,                                                                 server = server,                                                                 user = user,                                                                 password = pw,                                                                 port = port) cdmDatabaseSchema <- \"your cdmDatabaseSchema\" workDatabaseSchema <- \"your workDatabaseSchema\" cdmDatabaseName <- \"your cdmDatabaseName\" cohortTable <- \"your cohort table\"  databaseDetails <- createDatabaseDetails(   connectionDetails = connectionDetails,    cdmDatabaseSchema = cdmDatabaseSchema,    cdmDatabaseName = cdmDatabaseName,    cohortDatabaseSchema = workDatabaseSchema,    cohortTable = cohortTable,    outcomeDatabaseSchema = workDatabaseSchema,    outcomeTable = cohortTable,   cdmVersion = 5     ) results <- runMultiplePlp(   databaseDetails = databaseDetails,   modelDesignList = list(     modelDesign1,     modelDesign2,     modelDesign3   ),   onlyFetchData = FALSE,   logSettings = createLogSettings(),   saveDirectory = \"./PlpMultiOutput\" )"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"validating-multiple-models","dir":"Articles","previous_headings":"","what":"Validating multiple models","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"access multiple databases server different schemas evaluate accross using call: saves external validation results Validation folder main study (outputLocation used runPlpAnalyses).","code":"validationDatabaseDetails <- createDatabaseDetails(   connectionDetails = connectionDetails,   cdmDatabaseSchema = \"new cdm schema\",   cdmDatabaseName = \"validation database\",   cohortDatabaseSchema = workDatabaseSchema,   cohortTable = cohortTable,   outcomeDatabaseSchema = workDatabaseSchema,   outcomeTable = cohortTable,   cdmVersion = 5 )  val <- validateMultiplePlp(   analysesLocation = \"./PlpMultiOutput\",   valdiationDatabaseDetails = validationDatabaseDetails,   validationRestrictPlpDataSettings = createRestrictPlpDataSettings(),   recalibrate = NULL,   saveDirectory = \"./PlpMultiOutput/Validation\" )"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"viewing-the-results","dir":"Articles","previous_headings":"","what":"Viewing the results","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"view results multiple prediction analysis: validation directory “./PlpMultiOutput” sqlite results database, external validation also displayed.","code":"viewMultiplePlp(analysesLocation = \"./PlpMultiOutput\")"},{"path":"/articles/BuildingMultiplePredictiveModels.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Automatically Build Multiple Patient-Level Predictive Models","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/BuildingPredictiveModels.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Building patient-level predictive models","text":"Observational healthcare data, administrative claims electronic health records, increasingly used clinical characterization disease progression, quality improvement, population-level effect estimation medical product safety surveillance comparative effectiveness. Advances machine learning large dataset analysis led increased interest applying patient-level prediction type data. Patient-level prediction offers potential medical practice move beyond average treatment effects consider personalized risks part clinical decision-making. However, many published efforts patient-level-prediction follow model development guidelines, fail perform extensive external validation, provide insufficient model details limits ability independent researchers reproduce models perform external validation. makes hard fairly evaluate predictive performance models reduces likelihood model used appropriately clinical practice. improve standards, several papers written detailing guidelines best practices developing reporting prediction models. Transparent Reporting multivariable prediction model Individual Prognosis Diagnosis (TRIPOD) statement provides clear recommendations reporting prediction model development validation addresses concerns related transparency. However, data structure heterogeneity inconsistent terminologies still make collaboration model sharing difficult different researchers often required write new code extract data databases may define variables differently. paper, propose standardised framework patient-level prediction utilizes OMOP Common Data Model (CDM) standardized vocabularies, describe open-source software developed implementing framework’s pipeline. framework first support existing best practice guidelines enable open dissemination models can extensively validated across network OHDSI collaborators. Figure 1, illustrates prediction problem address. Among population risk, aim predict patients defined moment time (t = 0) experience outcome time--risk. Prediction done using information patients observation window prior moment time. shown Figure 2, define prediction problem define t=0 Target Cohort (T), outcome like predict outcome cohort (O), time--risk (TAR). Furthermore, make design choices model like develop, determine observational datasets perform internal external validation. conceptual framework works type prediction problems, example presented Figure 3. vignette describes can use PatientLevelPrediction package build patient-level predictive models. package enables data extraction, model building, model evaluation using data databases translated OMOP CDM. vignette assume installed package correctly using vignette('InstallationGuide').","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"study-specification","dir":"Articles","previous_headings":"","what":"Study specification","title":"Building patient-level predictive models","text":"clearly specify study upfront able implement . means need define prediction problem like address, population build model, model build evaluate performance. guide process use “Disease onset progression” prediction type example.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"problem-definition-1-stroke-in-atrial-fibrilation-patients","dir":"Articles","previous_headings":"Study specification","what":"Problem definition 1: Stroke in atrial fibrilation patients","title":"Building patient-level predictive models","text":"Atrial fibrillation disease characterized irregular heart rate can cause poor blood flow. Patients atrial fibrillation increased risk ischemic stroke. Anticoagulation recommended prophylaxis treatment strategy patients high risk stroke, though underuse anticoagulants persistent severity ischemic stroke represents substantial unmet medical need. Various strategies developed predict risk ischemic stroke patients atrial fibrillation. CHADS2 (Gage JAMA 2001) developed risk score based history congestive heart failure, hypertension, age>=75, diabetes stroke. CHADS2 initially derived using Medicare claims data, achieved good discrimination (AUC=0.82). However, subsequent external validation studies revealed CHADS2 substantially lower predictive accuracy (Keogh Thromb Haemost 2011). Subsequent stroke risk calculators developed evaluated, including extension CHADS2Vasc. management atrial fibrillation evolved substantially last decade, various reasons include introduction novel oral anticoagulants. innovations come renewed interest greater precision medicine stroke prevention. apply PatientLevelPrediction package observational healthcare data address following patient-level prediction question: Among patients newly diagnosed Atrial Fibrillation, patients go Ischemic Stroke within 1 year? define ‘patients newly diagnosed Atrial Fibrillation’ first condition record cardiac arrhythmia, followed another cardiac arrhythmia condition record, least two drug records drug used treat arrhythmias, procedure treat arrhythmias. define ‘Ischemic stroke events’ ischemic stroke condition records inpatient ER visit; successive records > 180 day gap considered independent episodes.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"problem-definition-2-angioedema-in-ace-inhibitor-users","dir":"Articles","previous_headings":"Study specification","what":"Problem definition 2: Angioedema in ACE inhibitor users","title":"Building patient-level predictive models","text":"Angiotensin converting enzyme inhibitors (ACE inhibitors) medications used patients hypertension widen blood vessles therefore increse amount blood pumped heart decreases blood pressure. Ace inhibitors reduce patients risk cardiovasular disease can lead drug-induced angioedema. apply PatientLevelPrediction package observational healthcare data address following patient-level prediction question: Amongt patients newly dispensed ACE inhibitor, patients go angioedema within 1 year? define ‘patients newly dispensed ACE inhibitor’ first drug record sny ACE inhibitor, […]followed another cardiac arrhythmia condition record, least two drug records drug used treat arrhythmias, procedure treat arrhythmias. define ‘angioedema’ angioedema condition record.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"study-population-definition","dir":"Articles","previous_headings":"Study specification","what":"Study population definition","title":"Building patient-level predictive models","text":"final study population develop model often subset Target population, e.g. apply criteria dependent T O want sensitivity analyses subpopulations T. answer following questions: minimum amount observation time require start target cohort? choice depend available patient time training data, also time expect available data sources want apply model future. longer minimum observation time, baseline history time available person use feature extraction, fewer patients qualify analysis. Moreover, clinical reasons choose short longer lookback period. example, use prior history lookback period (washout period). Can patients enter target cohort multiple times? target cohort definition, person may qualify cohort multiple times different spans time, example different episodes disease separate periods exposure medical product. cohort definition necessarily apply restriction let patients enter , context particular patient-level prediction problem, user may want restrict cohort first qualifying episode. example, person enter target cohort since criteria based first occurrence atrial fibrillation. allow persons enter cohort experienced outcome ? allow persons enter target cohort experienced outcome qualifying target cohort? Depending particular patient-level prediction problem, may desire predict ‘incident’ first occurrence outcome, case patients previously experienced outcome ‘-risk’ first occurrence therefore excluded target cohort. circumstances, may desire predict ‘prevalent’ episodes, whereby patients prior outcomes can included analysis prior outcome can predictor future outcomes. prediction example, answer question ‘Yes, allow persons prior outcomes’ know CHADS2 score prior strokes predictive future strokes. answer ‘’ also decide long look back previous occurrences outcome. define period predict outcome relative target cohort start? actually make two decisions answer question. First, time--risk window start date start target cohort later? Arguments make start later want avoid outcomes entered late record actually occurred start target cohort want leave gap interventions prevent outcome theoretically implemented. Second, need define time--risk setting risk window end, specification days offset relative target cohort start end dates. problem predict ‘time--risk’ window starting 1 day start target cohort 365 days later (look 1-year risk following atrial fibrillation diagnosis). require minimum amount time--risk? decide want include patients experience outcome leave database earlier end time--risk period. patients may experience outcome observe . prediction problem decide answer question ‘Yes, require minimum time--risk’ reason. Furthermore, decide constraint also applies persons experienced outcome include persons outcome irrespective total time risk. example, outcome death, persons outcome likely censored full time--risk period complete.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"model-development-settings","dir":"Articles","previous_headings":"Study specification","what":"Model development settings","title":"Building patient-level predictive models","text":"develop model decide algorithm(s) like train. see selection best algorithm certain prediction problem empirical question, .e. need let data speak try different approaches find best one. algorithm work best problems (free lunch). package therefore aim implement many algorithms. Furthermore, made system modular can add custom algorithms described detail vignette('AddingCustomModels'). package currently contains following algorithms choose : Furthermore, decide covariates use train model. choice can driven domain knowledge available computational resources. example, like add Gender, Age, Conditions, Drugs Groups, Visit Count. also specify time windows look decide look year time prior. Finally, define train test model data, .e. perform internal validation. decide divide dataset training testing dataset randomly assign patients two sets. Dependent size training set can decide much data like use training, typically 75%, 25% split. large datasets can use data training. randomly assign patients training testing set, two commonly used approaches: split person. case random seed used assign patient either sets. split time. case time point used split persons, e.g. 75% data 25% date. advantage take consideration health care system changed time. now completely defined studies implement : See example 1: Stroke Atrial fibrillation patients See example 2: Angioedema ACE inhibitor new users","code":""},{"path":[]},{"path":"/articles/BuildingPredictiveModels.html","id":"study-specification-1","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients","what":"Study Specification","title":"Building patient-level predictive models","text":"first prediction model decide start Regularized Logistic Regression use default parameters. 75%-25% split person. According best practices need make protocol completely specifies plan execute study. protocol assessed governance boards participating data sources network study. template used prefer automate process much possible adding functionality automatically generate study protocol study specification. discuss detail later.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"study-implementation","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients","what":"Study implementation","title":"Building patient-level predictive models","text":"Now completely design study implement study. generate target outcome cohorts need develop R code run CDM execute full study.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"cohort-instantiation","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Cohort instantiation","title":"Building patient-level predictive models","text":"study need know person enters target outcome cohorts. stored table server contains cohort start date cohort end date subjects specific cohort definition. cohort table simple structure shown : cohort_definition_id, unique identifier distinguishing different types cohorts, e.g. cohorts interest outcome cohorts. subject_id, unique identifier corresponding person_id CDM. cohort_start_date, date subject enters cohort. cohort_end_date, date subject leaves cohort. fill table according cohort definitions? two options : use interactive cohort builder tool ATLAS can used create cohorts based inclusion criteria automatically populate cohort table. write custom SQL statements fill cohort table. methods described example prediction problem.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"atlas-cohort-builder","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"ATLAS cohort builder","title":"Building patient-level predictive models","text":"ATLAS allows define cohorts interactively specifying cohort entry cohort exit criteria. Cohort entry criteria involve selecting one initial events, determine start date cohort entry, optionally specifying additional inclusion criteria filter qualifying events. Cohort exit criteria applied cohort entry record determine end date person’s episode longer qualifies cohort. outcome cohort end date less relevant. example, Figure 4 shows created Atrial Fibrillation cohort Figure 5 shows created stroke cohort ATLAS. T O cohorts can found : Atrial Fibrillaton (T): https://atlas-demo.ohdsi.org/#/cohortdefinition/1770326 Stroke (O) : https://atlas-demo.ohdsi.org/#/cohortdefinition/1769448 depth explanation cohort creation ATLAS scope vignette can found OHDSI wiki pages (link). Note cohort created ATLAS cohortId needed extract data R. cohortId can found top ATLAS screen, e.g. 1769447 Figure 4.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"custom-cohorts","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Custom cohorts","title":"Building patient-level predictive models","text":"also possible create cohorts without use ATLAS. Using custom cohort code (SQL) can make advanced cohorts needed. example study, need create table hold cohort data need create SQL code instantiate table AF Stroke cohorts. Therefore, create file called AfStrokeCohorts.sql following contents: parameterized SQL can used SqlRender package. use parameterized SQL pre-specify names CDM result schemas. way, want run SQL different schema, need change parameter values; change SQL code. also making use translation functionality SqlRender, can make sure SQL code can run many different environments. execute sql CDM first need tell R connect server. PatientLevelPrediction uses DatabaseConnector package, provides function called createConnectionDetails(). Type ?createConnectionDetails specific settings required various database management systems (DBMS). example, one might connect PostgreSQL database using code: last three lines define cdmDatabaseSchema cohortsDatabaseSchema variables, well CDM version. use later tell R data CDM format live, want create cohorts interest, version CDM used. Note Microsoft SQL Server, need specify database schema, example cdmDatabaseSchema <- \"my_cdm_data.dbo\". code, first read SQL file memory. next line, replace four parameter names actual values. translate SQL dialect appropriate DBMS already specified connectionDetails. Next, connect server, submit rendered translated SQL. went well, now table events interest. can see many events per type:","code":"/*********************************** File AfStrokeCohorts.sql  ***********************************/ /* Create a table to store the persons in the T and C cohort */  IF OBJECT_ID('@cohortsDatabaseSchema.AFibStrokeCohort', 'U') IS NOT NULL  DROP TABLE @cohortsDatabaseSchema.AFibStrokeCohort;  CREATE TABLE @cohortsDatabaseSchema.AFibStrokeCohort  (  cohort_definition_id INT,  subject_id BIGINT, cohort_start_date DATE,  cohort_end_date DATE );   /* T cohort:  [PatientLevelPrediction vignette]:  T : patients who are newly  diagnosed with Atrial fibrillation - persons with a condition occurrence record of 'Atrial fibrillation' or  any descendants, indexed at the first diagnosis - who have >1095 days of prior observation before their first diagnosis - and have no warfarin exposure any time prior to first AFib diagnosis */ INSERT INTO @cohortsDatabaseSchema.AFibStrokeCohort (cohort_definition_id,  subject_id,  cohort_start_date,  cohort_end_date) SELECT 1 AS cohort_definition_id, AFib.person_id AS subject_id, AFib.condition_start_date AS cohort_start_date, observation_period.observation_period_end_date AS cohort_end_date FROM (   SELECT person_id, min(condition_start_date) as condition_start_date   FROM @cdmDatabaseSchema.condition_occurrence   WHERE condition_concept_id IN (SELECT descendant_concept_id FROM    @cdmDatabaseSchema.concept_ancestor WHERE ancestor_concept_id IN    (313217 /*atrial fibrillation*/))   GROUP BY person_id ) AFib   INNER JOIN @cdmDatabaseSchema.observation_period   ON AFib.person_id = observation_period.person_id   AND AFib.condition_start_date >= dateadd(dd,1095,    observation_period.observation_period_start_date)   AND AFib.condition_start_date <= observation_period.observation_period_end_date   LEFT JOIN   (   SELECT person_id, min(drug_exposure_start_date) as drug_exposure_start_date   FROM @cdmDatabaseSchema.drug_exposure   WHERE drug_concept_id IN (SELECT descendant_concept_id FROM    @cdmDatabaseSchema.concept_ancestor WHERE ancestor_concept_id IN    (1310149 /*warfarin*/))   GROUP BY person_id   ) warfarin   ON Afib.person_id = warfarin.person_id   AND Afib.condition_start_date > warfarin.drug_exposure_start_date   WHERE warfarin.person_id IS NULL   ;      /*   C cohort:  [PatientLevelPrediction vignette]:  O: Ischemic stroke events   - inpatient visits that include a condition occurrence record for    'cerebral infarction' and descendants, 'cerebral thrombosis',    'cerebral embolism', 'cerebral artery occlusion'    */   INSERT INTO @cohortsDatabaseSchema.AFibStrokeCohort (cohort_definition_id,    subject_id,    cohort_start_date,    cohort_end_date)   SELECT 2 AS cohort_definition_id,   visit_occurrence.person_id AS subject_id,   visit_occurrence.visit_start_date AS cohort_start_date,   visit_occurrence.visit_end_date AS cohort_end_date   FROM     (   SELECT person_id, condition_start_date   FROM @cdmDatabaseSchema.condition_occurrence   WHERE condition_concept_id IN (SELECT DISTINCT descendant_concept_id FROM    @cdmDatabaseSchema.concept_ancestor WHERE ancestor_concept_id IN    (443454 /*cerebral infarction*/) OR descendant_concept_id IN    (441874 /*cerebral thrombosis*/, 375557 /*cerebral embolism*/,    372924 /*cerebral artery occlusion*/))   ) stroke   INNER JOIN @cdmDatabaseSchema.visit_occurrence   ON stroke.person_id = visit_occurrence.person_id   AND stroke.condition_start_date >= visit_occurrence.visit_start_date   AND stroke.condition_start_date <= visit_occurrence.visit_end_date   AND visit_occurrence.visit_concept_id IN (9201, 262 /*'Inpatient Visit'  or    'Emergency Room and Inpatient Visit'*/)   GROUP BY visit_occurrence.person_id, visit_occurrence.visit_start_date,    visit_occurrence.visit_end_date   ; library(DatabaseConnector) connectionDetails <- createConnectionDetails(   dbms = \"postgresql\",   server = \"localhost/ohdsi\",   user = \"joe\",   password = \"supersecret\" )  cdmDatabaseSchema <- \"cdm\" cohortsDatabaseSchema <- \"cohorts\" cdmVersion <- \"5\" library(SqlRender) sql <- readSql(\"AfStrokeCohorts.sql\") sql <- render(sql,   cdmDatabaseSchema = cdmDatabaseSchema,   cohortsDatabaseSchema = cohortsDatabaseSchema ) sql <- translate(sql, targetDialect = connectionDetails$dbms)  connection <- connect(connectionDetails) executeSql(connection, sql) sql <- paste(   \"SELECT cohort_definition_id, COUNT(*) AS count\",   \"FROM @cohortsDatabaseSchema.AFibStrokeCohort\",   \"GROUP BY cohort_definition_id\" ) sql <- render(sql, cohortsDatabaseSchema = cohortsDatabaseSchema) sql <- translate(sql, targetDialect = connectionDetails$dbms)  querySql(connection, sql) ##   cohort_definition_id  count ## 1                    1 527616 ## 2                    2 221555"},{"path":"/articles/BuildingPredictiveModels.html","id":"study-script-creation","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Study script creation","title":"Building patient-level predictive models","text":"section assume cohorts created either using ATLAS custom SQL script. first explain create R script execute study defined earlier.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"data-extraction","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Data extraction","title":"Building patient-level predictive models","text":"Now can tell PatientLevelPrediction extract necessary data analysis. done using FeatureExtraction package. short FeatureExtraction package allows specify features (covariates) need extracted, e.g. conditions drug exposures. also supports creation custom covariates. detailed information FeatureExtraction package see vignettes. example study decided use settings: final step extracting data run getPlpData() function input connection details, database schema cohorts stored, cohort definition ids cohort outcome, washoutPeriod minimum number days prior cohort index date person must observed included data, finally input previously constructed covariateSettings. Note cohorts created ATLAS corresponding cohort database schema needs selected. many additional parameters getPlpData() function documented PatientLevelPrediction manual. resulting plpData object uses package Andromeda (uses SQLite) store information way ensures R run memory, even data large. Creating plpData object can take considerable computing time, probably good idea save future sessions. plpData uses Andromeda, use R’s regular save function. Instead, ’ll use savePlpData() function: can use loadPlpData() function load data future session.","code":"library(FeatureExtraction) covariateSettings <- createCovariateSettings(   useDemographicsGender = TRUE,   useDemographicsAge = TRUE,   useConditionGroupEraLongTerm = TRUE,   useConditionGroupEraAnyTimePrior = TRUE,   useDrugGroupEraLongTerm = TRUE,   useDrugGroupEraAnyTimePrior = TRUE,   useVisitConceptCountLongTerm = TRUE,   longTermStartDays = -365,   endDays = -1 ) library(PatientLevelPrediction) databaseDetails <- createDatabaseDetails(   connectionDetails = connectionDetails,   cdmDatabaseSchema = cdmDatabaseSchema,   cdmDatabaseName = \"\",   cohortDatabaseSchema = cohortsDatabaseSchema,   cohortTable = \"AFibStrokeCohort\",   targetId = 1,   outcomeDatabaseSchema = cohortsDatabaseSchema,   outcomeTable = \"AFibStrokeCohort\",   outcomeIds = 2,   cdmVersion = 5 )  # here you can define whether you want to sample the target cohort and add any # restrictions based on minimum prior observation, index date restrictions # or restricting to first index date (if people can be in target cohort multiple times) restrictPlpDataSettings <- createRestrictPlpDataSettings(sampleSize = 10000)  plpData <- getPlpData(   databaseDetails = databaseDetails,   covariateSettings = covariateSettings,   restrictPlpDataSettings = restrictPlpDataSettings ) savePlpData(plpData, \"stroke_in_af_data\")"},{"path":"/articles/BuildingPredictiveModels.html","id":"additional-inclusion-criteria","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Additional inclusion criteria","title":"Building patient-level predictive models","text":"completely define prediction problem final study population obtained applying additional constraints two earlier defined cohorts, e.g., minimum time risk can enforced (requireTimeAtRisk, minTimeAtRisk) can specify also applies patients outcome (includeAllOutcomes). also specify start end risk window relative target cohort start. example, like risk window start 30 days -risk cohort start end year later can set riskWindowStart = 30 riskWindowEnd = 365. example settings defined study imposed:","code":"populationSettings <- createStudyPopulationSettings(   washoutPeriod = 1095,   firstExposureOnly = FALSE,   removeSubjectsWithPriorOutcome = FALSE,   priorOutcomeLookback = 1,   riskWindowStart = 1,   riskWindowEnd = 365,   startAnchor = \"cohort start\",   endAnchor = \"cohort start\",   minTimeAtRisk = 364,   requireTimeAtRisk = TRUE,   includeAllOutcomes = TRUE )"},{"path":"/articles/BuildingPredictiveModels.html","id":"splitting-the-data-into-trainingvalidationtesting-datasets","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Splitting the data into training/validation/testing datasets","title":"Building patient-level predictive models","text":"developing prediction model using supervised learning (features paired labels set patients), first step design development/internal validation process. requires specifying select model hyper-parameters, learn model parameters fairly evaluate model. general, validation set used pick hyper-parameters, training set used learn model parameters test set used perform fair internal validation. However, cross-validation can implemented pick hyper-parameters training data (validation data set required). Cross validation can also used estimate internal validation (testing data set required). small data best approach internal validation shown bootstrapping. However, big data (many patients many features) bootstrapping generally feasible. big data research shown adequate form fair evaluation (use test set cross validation). full details see BMJ open paper. PatientLevelPrediction package, splitSettings define plpData partitioned training/validation/testing data. created createDefaultSplitSetting(). Cross validation always done, using test set optional (data small, may optimal use test set). splitSettings can use type (stratified/time/subject) testFraction parameters split data 75%-25% split run patient-level prediction pipeline: Note: possible add custom method specify plpData partitioned training/validation/testing data, see vignette('AddingCustomSplitting')","code":"splitSettings <- createDefaultSplitSetting(   trainFraction = 0.75,   testFraction = 0.25,   type = \"stratified\",   nfold = 2,   splitSeed = 1234 )"},{"path":"/articles/BuildingPredictiveModels.html","id":"preprocessing-the-training-data","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Preprocessing the training data","title":"Building patient-level predictive models","text":"numerous data processing settings user must specify developing prediction model. : * Whether -sample -sample training data (may useful class imballance (e.g., outcome rare common)) * Whether perform feature engineering feature selection (e.g., create latent variables observed data reduce dimensionality data) * Whether remove redundant features normalize data (required models) default sample settings nothing, simply returns trainData input, see : However, current package contains methods -sampling non-outcome patients. perform undersampling, type input ‘underSample’ numberOutcomestoNonOutcomes must specified (integer specifying number non-outcomes per outcome). possible add custom function /sampling, see vignette('AddingCustomSamples'). possible specify combination feature engineering functions take input trainData output new trainData different features. default feature engineering setting nothing: However, possible add custom feature engineering functions pipeline, see vignette('AddingCustomFeatureEngineering'). Finally, preprocessing setting required. setting user can define minFraction, removes features observed training data less 0.01 fraction patients. , minFraction = 0.01 feature seen less 1 percent target population removed. input normalize specifies whether features scaled 0 1, required certain models (e.g., LASSO logistic regression). input removeRedundancy specifies whether features observed target population removed.","code":"sampleSettings <- createSampleSettings() featureEngineeringSettings <- createFeatureEngineeringSettings() preprocessSettingsSettings <- createPreprocessSettings(   minFraction = 0.01,   normalize = TRUE,   removeRedundancy = TRUE )"},{"path":"/articles/BuildingPredictiveModels.html","id":"model-development","dir":"Articles","previous_headings":"Example 1: Stroke in Atrial fibrillation patients > Study implementation","what":"Model Development","title":"Building patient-level predictive models","text":"set function algorithm user can specify list eligible values hyper-parameter. possible combinations hyper-parameters included -called grid search using cross-validation training set. user specify value default value used instead. example, use following settings gradientBoostingMachine: ntrees=c(100,200), maxDepth=4 grid search apply gradient boosting machine algorithm ntrees=100 maxDepth=4 plus default settings hyper-parameters ntrees=200 maxDepth=4 plus default settings hyper-parameters. hyper-parameters lead bestcross-validation performance chosen final model. problem choose build logistic regression model default hyper-parameters runPlP function requires plpData, outcomeId specifying outcome predicted settings: populationSettings, splitSettings, sampleSettings, featureEngineeringSettings, preprocessSettings modelSettings train evaluate model. hood package now use Cyclops package fit large-scale regularized regression using 74% data evaluate model remaining 25%. results data structure returned containing information model, performance etc. can save model using: can load model using: can also save full results structure using: load full results structure use:","code":"lrModel <- setLassoLogisticRegression() lrResults <- runPlp(   plpData = plpData,   outcomeId = 2,   analysisId = \"singleDemo\",   analysisName = \"Demonstration of runPlp for training single PLP models\",   populationSettings = populationSettings,   splitSettings = splitSettings,   sampleSettings = sampleSettings,   featureEngineeringSettings = featureEngineeringSettings,   preprocessSettings = preprocessSettings,   modelSettings = lrModel,   logSettings = createLogSettings(),   executeSettings = createExecuteSettings(     runSplitData = TRUE,     runSampleData = TRUE,     runFeatureEngineering = TRUE,     runPreprocessData = TRUE,     runModelDevelopment = TRUE,     runCovariateSummary = TRUE   ),   saveDirectory = file.path(tempdir(), \"singlePlp\") ) savePlpModel(lrResults$model, dirPath = file.path(tempdir(), \"model\")) plpModel <- loadPlpModel(file.path(tempdir(), \"model\")) savePlpResult(lrResults, location = file.path(tempdir(), \"lr\")) lrResults <- loadPlpResult(file.path(tempdir(), \"lr\"))"},{"path":[]},{"path":"/articles/BuildingPredictiveModels.html","id":"study-specification-2","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users","what":"Study Specification","title":"Building patient-level predictive models","text":"According best practices need make protocol completely specifies plan execute study. protocol assessed governance boards participating data sources network study. template used prefer automate process much possible adding functionality automatically generate study protocol study specification. discuss detail later.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"study-implementation-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users","what":"Study implementation","title":"Building patient-level predictive models","text":"Now completely design study implement study. generate target outcome cohorts need develop R code run CDM execute full study.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"cohort-instantiation-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Cohort instantiation","title":"Building patient-level predictive models","text":"study need know person enters target outcome cohorts. stored table server contains cohort start date cohort end date subjects specific cohort definition. cohort table simple structure shown : cohort_definition_id, unique identifier distinguishing different types cohorts, e.g. cohorts interest outcome cohorts. subject_id, unique identifier corresponding person_id CDM. cohort_start_date, date subject enters cohort. cohort_end_date, date subject leaves cohort. fill table according cohort definitions? two options : use interactive cohort builder tool ATLAS can used create cohorts based inclusion criteria automatically populate cohort table. write custom SQL statements fill cohort table. methods described example prediction problem.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"atlas-cohort-builder-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"ATLAS cohort builder","title":"Building patient-level predictive models","text":"ATLAS allows define cohorts interactively specifying cohort entry cohort exit criteria. Cohort entry criteria involve selecting one initial events, determine start date cohort entry, optionally specifying additional inclusion criteria filter qualifying events. Cohort exit criteria applied cohort entry record determine end date person’s episode longer qualifies cohort. outcome cohort end date less relevant. example, Figure 6 shows created ACE inhibitors cohort Figure 7 shows created angioedema cohort ATLAS. T O cohorts can found : Ace inhibitors (T): https://atlas-demo.ohdsi.org/#/cohortdefinition/1770617 Angioedema (O) : https://atlas-demo.ohdsi.org/#/cohortdefinition/1770616 depth explanation cohort creation ATLAS scope vignette can found OHDSI wiki pages (link). Note cohort created ATLAS cohortid needed extract data R. cohortid can found top ATLAS screen, e.g. 1770617 Figure 6.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"custom-cohorts-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Custom cohorts","title":"Building patient-level predictive models","text":"also possible create cohorts without use ATLAS. Using custom cohort code (SQL) can make advanced cohorts needed. example study, need create table hold cohort data need create SQL code instantiate table AF Stroke cohorts. Therefore, create file called AceAngioCohorts.sql following contents: parameterized SQL can used SqlRender package. use parameterized SQL pre-specify names CDM result schemas. way, want run SQL different schema, need change parameter values; change SQL code. also making use translation functionality SqlRender, can make sure SQL code can run many different environments. execute sql CDM first need tell R connect server. PatientLevelPrediction uses DatabaseConnector package, provides function called createConnectionDetails. Type ?createConnectionDetails specific settings required various database management systems (DBMS). example, one might connect PostgreSQL database using code: last three lines define cdmDatabaseSchema cohortsDatabaseSchema variables, well CDM version. use later tell R data CDM format live, want create cohorts interest, version CDM used. Note Microsoft SQL Server, databaseschemas need specify database schema, example cdmDatabaseSchema <- \"my_cdm_data.dbo\". code, first read SQL file memory. next line, replace four parameter names actual values. translate SQL dialect appropriate DBMS already specified connectionDetails. Next, connect server, submit rendered translated SQL. went well, now table events interest. can see many events per type:","code":"/***********************************     File AceAngioCohorts.sql    ***********************************/     /*     Create a table to store the persons in the T and C cohort   */          IF OBJECT_ID('@resultsDatabaseSchema.PLPAceAngioCohort', 'U') IS NOT NULL    DROP TABLE @resultsDatabaseSchema.PLPAceAngioCohort;      CREATE TABLE @resultsDatabaseSchema.PLPAceAngioCohort    (      cohort_definition_id INT,      subject_id BIGINT,     cohort_start_date DATE,      cohort_end_date DATE   );         /*     T cohort:  [PatientLevelPrediction vignette]:  T : patients who are newly    dispensed an ACE inhibitor   - persons with a drug exposure record of any 'ACE inhibitor' or    any descendants, indexed at the first diagnosis   - who have >364 days of prior observation before their first dispensing   */     INSERT INTO @resultsDatabaseSchema.AceAngioCohort (cohort_definition_id,                                                         subject_id,                                                         cohort_start_date,                                                         cohort_end_date)   SELECT 1 AS cohort_definition_id,   Ace.person_id AS subject_id,   Ace.drug_start_date AS cohort_start_date,   observation_period.observation_period_end_date AS cohort_end_date   FROM   (     SELECT person_id, min(drug_exposure_date) as drug_start_date     FROM @cdmDatabaseSchema.drug_exposure     WHERE drug_concept_id IN (SELECT descendant_concept_id FROM                                @cdmDatabaseSchema.concept_ancestor WHERE ancestor_concept_id IN                                (1342439,1334456, 1331235, 1373225, 1310756, 1308216, 1363749, 1341927, 1340128, 1335471 /*ace inhibitors*/))     GROUP BY person_id   ) Ace   INNER JOIN @cdmDatabaseSchema.observation_period   ON Ace.person_id = observation_period.person_id   AND Ace.drug_start_date >= dateadd(dd,364,                                       observation_period.observation_period_start_date)   AND Ace.drug_start_date <= observation_period.observation_period_end_date   ;      /*     C cohort:  [PatientLevelPrediction vignette]:  O: Angioedema   */     INSERT INTO @resultsDatabaseSchema.AceAngioCohort (cohort_definition_id,                                                         subject_id,                                                         cohort_start_date,                                                         cohort_end_date)   SELECT 2 AS cohort_definition_id,   angioedema.person_id AS subject_id,   angioedema.condition_start_date AS cohort_start_date,   angioedema.condition_start_date AS cohort_end_date   FROM     (     SELECT person_id, condition_start_date     FROM @cdmDatabaseSchema.condition_occurrence     WHERE condition_concept_id IN (SELECT DISTINCT descendant_concept_id FROM                                     @cdmDatabaseSchema.concept_ancestor WHERE ancestor_concept_id IN                                     (432791 /*angioedema*/) OR descendant_concept_id IN                                     (432791 /*angioedema*/)     ) angioedema          ; connectionDetails <- createConnectionDetails(   dbms = \"postgresql\",   server = \"localhost/ohdsi\",   user = \"joe\",   password = \"supersecret\" )  cdmDatabaseSchema <- \"my_cdm_data\" cohortsDatabaseSchema <- \"my_results\" cdmVersion <- \"5\" library(SqlRender) sql <- readSql(\"AceAngioCohorts.sql\") sql <- render(sql,   cdmDatabaseSchema = cdmDatabaseSchema,   cohortsDatabaseSchema = cohortsDatabaseSchema ) sql <- translate(sql, targetDialect = connectionDetails$dbms)  connection <- connect(connectionDetails) executeSql(connection, sql) sql <- paste(   \"SELECT cohort_definition_id, COUNT(*) AS count\",   \"FROM @cohortsDatabaseSchema.AceAngioCohort\",   \"GROUP BY cohort_definition_id\" ) sql <- render(sql, cohortsDatabaseSchema = cohortsDatabaseSchema) sql <- translate(sql, targetDialect = connectionDetails$dbms)  querySql(connection, sql) ##   cohort_definition_id count ## 1                    1     0 ## 2                    2     0"},{"path":"/articles/BuildingPredictiveModels.html","id":"study-script-creation-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Study script creation","title":"Building patient-level predictive models","text":"section assume cohorts created either using ATLAS custom SQL script. first explain create R script execute study defined earlier.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"data-extraction-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Data extraction","title":"Building patient-level predictive models","text":"Now can tell PatientLevelPrediction extract necessary data analysis. done using FeatureExtractionPackage. short FeatureExtractionPackage allows specify features (covariates) need extracted, e.g. conditions drug exposures. also supports creation custom covariates. detailed information FeatureExtraction package see vignettes. example study decided use settings: final step extracting data run getPlpData function input connection details, database schema cohorts stored, cohort definition ids cohort outcome, washoutPeriod minimum number days prior cohort index date person must observed included data, finally input previously constructed covariate settings. Note cohorts created ATLAS corresponding cohort database schema needs selected. many additional parameters getPlpData function documented PatientLevelPrediction manual. resulting plpData object uses package ff store information way ensures R run memory, even data large. Creating plpData object can take considerable computing time, probably good idea save future sessions. plpData uses ff, use R’s regular save function. Instead, ’ll use savePlpData() function: can use loadPlpData() function load data future session.","code":"covariateSettings <- createCovariateSettings(   useDemographicsGender = TRUE,   useDemographicsAge = TRUE,   useConditionGroupEraLongTerm = TRUE,   useConditionGroupEraAnyTimePrior = TRUE,   useDrugGroupEraLongTerm = TRUE,   useDrugGroupEraAnyTimePrior = TRUE,   useVisitConceptCountLongTerm = TRUE,   longTermStartDays = -365,   endDays = -1 ) databaseDetails <- createDatabaseDetails(   connectionDetails = connectionDetails,   cdmDatabaseSchema = cdmDatabaseSchema,   cohortDatabaseSchema = resultsDatabaseSchema,   cohortTable = \"AceAngioCohort\",   cohortId = 1,   outcomeDatabaseSchema = resultsDatabaseSchema,   outcomeTable = \"AceAngioCohort\",   outcomeIds = 2 )  restrictPlpDataSettings <- createRestrictPlpDataSettings(   sampleSize = 10000 )  plpData <- getPlpData(   databaseDetails = databaseDetails,   covariateSettings = covariateSettings,   restrictPlpDataSettings = restrictPlpDataSettings ) savePlpData(plpData, \"angio_in_ace_data\")"},{"path":"/articles/BuildingPredictiveModels.html","id":"additional-inclusion-criteria-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Additional inclusion criteria","title":"Building patient-level predictive models","text":"completely define prediction problem final study population obtained applying additional constraints two earlier defined cohorts, e.g., minumim time risk can enforced (requireTimeAtRisk, minTimeAtRisk) can specify also applies patients outcome (includeAllOutcomes). also specify start end risk window relative target cohort start. example, like risk window start 30 days -risk cohort start end year later can set riskWindowStart = 30 riskWindowEnd = 365. cases risk window needs start cohort end date. can achieved setting addExposureToStart = TRUE adds cohort (exposure) time start date. Appendix 1, demonstrate effect settings subset persons target cohort end final study population. example settings defined study imposed:","code":"populationSettings <- createStudyPopulationSettings(   washoutPeriod = 364,   firstExposureOnly = FALSE,   removeSubjectsWithPriorOutcome = TRUE,   priorOutcomeLookback = 9999,   riskWindowStart = 1,   riskWindowEnd = 365,   minTimeAtRisk = 364,   startAnchor = \"cohort start\",   endAnchor = \"cohort start\",   requireTimeAtRisk = TRUE,   includeAllOutcomes = TRUE )"},{"path":"/articles/BuildingPredictiveModels.html","id":"spliting-the-data-into-trainingvalidationtesting-datasets","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Spliting the data into training/validation/testing datasets","title":"Building patient-level predictive models","text":"developing prediction model using supervised learning (features paired labels set patients), first step design development/internal validation process. requires specifying select model hyper-parameters, learn model parameters fairly evaluate model. general, validation set used pick hyper-parameters, training set used learn model parameters test set used perform fair internal validation. However, cross-validation can implemented pick hyper-parameters training data (validation data set required). Cross validation can also used estimate internal validation (testing data set required). small data best approach internal validation shown bootstrapping. However, big data (many patients many features) bootstrapping generally feasible. big data research shown just important form fair evaluation (use test set cross validation). full details see BMJ open paper. PatientLevelPrediction package, splitSettings define plpData partitioned training/validation/testing data. Cross validation always done, using test set optional (data small, may optimal use test set). splitSettings can use type (stratified/time/subject) testFraction parameters split data 75%-25% split run patient-level prediction pipeline: Note: possible add custom method specify plpData partitioned training/validation/testing data, see vignette('AddingCustomSplitting').","code":"splitSettings <- createDefaultSplitSetting(   trainFraction = 0.75,   testFraction = 0.25,   type = \"stratified\",   nfold = 2,   splitSeed = 1234 )"},{"path":"/articles/BuildingPredictiveModels.html","id":"preprocessing-the-training-data-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Preprocessing the training data","title":"Building patient-level predictive models","text":"numerous data processing settings user must specify developing prediction model. : * Whether -sample -sample training data (may useful class imballance (e.g., outcome rare common)) * Whether perform feature engineering feature selection (e.g., create latent variables observed data reduce dimensionality data) * Whether remove redundant features normalize data (required models) default sample settings nothing, simply returns trainData input, see : However, current package contains methods -sampling non-outcome patients. perform undersampling, type input ‘underSample’ numberOutcomestoNonOutcomes must specified (integer specifying number non-outcomes per outcome). possible add custom function /sampling, see vignette('AddingCustomSamples'). possible specify combination feature engineering functions take input trainData output new trainData different features. default feature engineering setting nothing: However, possible add custom feature engineering functions pipeline, see vignette('AddingCustomFeatureEngineering'). Finally, preprocessing setting required. setting user can define minFraction, removes features observed training data less 0.01 fraction patients. , minFraction = 0.01 feature seen less 1 percent target population removed. input normalize specifies whether features scaled 0 1, required certain models (e.g., LASSO logistic regression). input removeRedundancy specifies whether features observed target population removed.","code":"sampleSettings <- createSampleSettings() featureEngineeringSettings <- createFeatureEngineeringSettings() preprocessSettingsSettings <- createPreprocessSettings(   minFraction = 0.01,   normalize = TRUE,   removeRedundancy = TRUE )"},{"path":"/articles/BuildingPredictiveModels.html","id":"model-development-1","dir":"Articles","previous_headings":"Example 2: Angioedema in ACE inhibitor users > Study implementation","what":"Model Development","title":"Building patient-level predictive models","text":"set function algorithm user can specify list eligible values hyper-parameter. possible combinations hyper-parameters included -called grid search using cross-validation training set. user specify value default value used instead. example, use following settings gradientBoostingMachine: ntrees=c(100,200), maxDepth=4 grid search apply gradient boosting machine algorithm ntrees=100 maxDepth=4 plus default settings hyper-parameters ntrees=200 maxDepth=4 plus default settings hyper-parameters. hyper-parameters lead bestcross-validation performance chosen final model. problem choose build logistic regression model default hyper-parameters runPlP function requires plpData, outcomeId specifying outcome predicted settings: populationSettings, splitSettings, sampleSettings, featureEngineeringSettings, preprocessSettings modelSettings train evaluate model. hood package now use R xgboost package fit gradient boosting machine model using 75% data evaluate model remaining 25%. results data structure returned containing information model, performance etc. can save model using: can load model using: can also save full results structure using: load full results structure use:","code":"gbmModel <- setGradientBoostingMachine(ntrees = 5000, maxDepth = c(4, 7, 10), learnRate = c(0.001,     0.01, 0.1, 0.9)) gbmResults <- runPlp(   plpData = plpData,   outcomeId = 2,   analysisId = \"singleDemo2\",   analysisName = \"Demonstration of runPlp for training single PLP models\",   populationSettings = populationSettings,   splitSettings = splitSettings,   sampleSettings = sampleSettings,   featureEngineeringSettings = featureEngineeringSettings,   preprocessSettings = preprocessSettings,   modelSettings = gbmModel,   logSettings = createLogSettings(),   executeSettings = createExecuteSettings(     runSplitData = TRUE,     runSampleData = TRUE,     runFeatureEngineering = TRUE,     runPreprocessData = TRUE,     runModelDevelopment = TRUE,     runCovariateSummary = TRUE   ),   saveDirectory = file.path(tempdir(), \"singlePlpExample2\") ) savePlpModel(gbmResults$model, dirPath = file.path(tempdir(), \"model\")) plpModel <- loadPlpModel(file.path(tempdir(), \"model\")) savePlpResult(gbmResults, location = file.path(tempdir(), \"gbm\")) gbmResults <- loadPlpResult(file.path(file.path(tempdir(), \"gbm\"))"},{"path":"/articles/BuildingPredictiveModels.html","id":"study-package-creation","dir":"Articles","previous_headings":"","what":"Study package creation","title":"Building patient-level predictive models","text":"script created manually can also automatically created using powerful feature ATLAS. creating new prediction study (left menu) can select Target Outcome created ATLAS, set study parameters, can download R package can use execute study. really powerful can add multiple Ts, Os, covariate settings etc. package run combinations automatically separate analyses. screenshots explain process. Create new prediction study select target outcome cohorts.  Specify one analysis settings.  Specify trainings settigns  Specify execution settings  ATLAS can build R package execute full study CDM. steps explained ATLAS. utilities can find download. Click button review full study specification R package download functionality ATLAS now review indeed want run analyses (cartesian product settings T O combination. R package download functionality ATLAS agree, give package name, download package zipfile. opening R package R studio building package can run study using execute function. Theres also example CodeToRun.R script available extras folder package extra instructions.","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"internal-validation","dir":"Articles","previous_headings":"","what":"Internal validation","title":"Building patient-level predictive models","text":"execute study, runPlp() function returns trained model evaluation model train/test sets. can interactively view results running: viewPlp(runPlp=lrResults). generate Shiny App browser can view performance measures created framework shown figure . Summary performance measures analyses Furthermore, many interactive plots available Shiny App, example ROC curve can move plot see threshold corresponding sensitivity specificity values. Example interactive ROC curve generate save evaluation plots folder run following code: plots described detail next sections.","code":"plotPlp(lrResults, dirPath = file.path(tempdir(), \"plots\"))"},{"path":"/articles/BuildingPredictiveModels.html","id":"discrimination","dir":"Articles","previous_headings":"Internal validation","what":"Discrimination","title":"Building patient-level predictive models","text":"Receiver Operating Characteristics (ROC) plot shows sensitivity 1-specificity test set. plot illustrates well model able discriminate people outcome without. dashed diagonal line performance model randomly assigns predictions. higher area ROC plot better discrimination model. plot created changing probability threshold assign positive class. Receiver Operating Characteristic Plot ## Calibration calibration plot shows close predicted risk observed risk. diagonal dashed line thus indicates perfectly calibrated model. ten (fewer) dots represent mean predicted values quantile plotted observed fraction people quantile outcome (observed fraction). straight black line linear regression using 10 plotted quantile mean predicted vs observed fraction points. straight vertical lines represented 95% lower upper confidence intervals slope fitted line. Calibration Plot","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"smooth-calibration","dir":"Articles","previous_headings":"Internal validation","what":"Smooth Calibration","title":"Building patient-level predictive models","text":"Similar traditional calibration shown Smooth Calibration plot shows relationship predicted observed risk. major difference smooth fit allows fine grained examination . Whereas traditional plot heavily influenced areas highest density data smooth plot provide information region well accurate interpretation areas lower density. plot also contains information distribution outcomes relative predicted risk. However, increased information gain comes computational cost. recommended use traditional plot examination produce smooth plot final versions. create smooth calibarion plot run follow command: See help function information, set smoothing method etc. example another study better demonstrates impact using smooth calibration plot. default line fit highlight miss-calibration lower predicted probability levels well. Smooth Calibration plot ## Preference distribution preference distribution plots preference score distributions corresponding ) people test set outcome (red) ii) people test set without outcome (blue). Preference Plot ## Predicted probability distribution prediction distribution box plots predicted risks people test set outcome (class 1: blue) without outcome (class 0: red). box plots Figure show predicted probability outcome indeed higher outcome also overlap two distribution lead imperfect discrimination. Prediction Distribution Box Plot ## Test-Train similarity test-train similarity assessed plotting mean covariate values train set test set people without outcome. results example look promising since mean values covariates diagonal. Similarity plots train test set ## Variable scatter plot variable scatter plot shows mean covariate value people outcome mean covariate value people without outcome. color dots corresponds inclusion (green) exclusion model (blue), respectively. highly recommended use Shiny App since allows hoover covariate show details (name, value etc). plot shows mean covariates higher subjects outcome compared without. Variabel scatter Plot ## Precision recall Precision defined number true positives (Tp) number true positives plus number false positives (Fp). Recall defined number true positives (Tp) number true positives plus number false negatives (Fn). quantities also related (F1) score, defined harmonic mean precision recall. Note precision can either decrease increase threshold lowered. Lowering threshold classifier may increase denominator, increasing number results returned. threshold previously set high, new results may true positives, increase precision. previous threshold right low, lowering threshold introduce false positives, decreasing precision. Recall denominator depend classifier threshold (Tp+Fn constant). means lowering classifier threshold may increase recall, increasing number true positive results. also possible lowering threshold may leave recall unchanged, precision fluctuates. Precision Recall Plot ## Demographic summary plot shows females males expected observed risk different age groups together confidence area. results show model well calibrated across gender age groups. Demographic Summary Plot # External validation recommend always perform external validation, .e. apply final model much new datasets feasible evaluate performance. extract new plpData specified schemas cohort tables. apply population settings trained plp model. Finally, evaluate performance return standard output validation$performanceEvaluation also return prediction population validation$prediction. can inserted shiny app viewing model validation running: viewPlp(runPlp=plpResult, validatePlp=validation ).","code":"plotSmoothCalibration(lrResults) precision <- Tp/(Tp + Fp) recall <- Tp/(Tp + Fn) f1Score <- 2 * P * R/(P + R) # load the trained model plpModel <- loadPlpModel(file.path(tempdir(), \"model\"), \"model\")  # add details of new database validationDatabaseDetails <- createDatabaseDetails()  # to externally validate the model and perform recalibration run: externalValidateDbPlp(   plpModel = plpModel,   validationDatabaseDetails = validationDatabaseDetails,   validationRestrictPlpDataSettings = plpModel$settings$plpDataSettings,   settings = createValidationSettings(     recalibrate = \"weakRecalibration\"   ),   outputFolder = file.path(tempdir(), \"validation\") )"},{"path":"/articles/BuildingPredictiveModels.html","id":"other-functionality","dir":"Articles","previous_headings":"","what":"Other functionality","title":"Building patient-level predictive models","text":"package much functionality described vignette contributions made many persons OHDSI community. table provides overview:","code":""},{"path":"/articles/BuildingPredictiveModels.html","id":"demos","dir":"Articles","previous_headings":"","what":"Demos","title":"Building patient-level predictive models","text":"added several demos package run simulated data:","code":"# Show all demos in our package: demo(package = \"PatientLevelPrediction\")  # For example, to run the SingleModelDemo that runs Lasso and shows you how to run the Shiny App use this call demo(\"SingleModelDemo\", package = \"PatientLevelPrediction\")"},{"path":"/articles/BuildingPredictiveModels.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Building patient-level predictive models","text":"Considerable work dedicated provide PatientLevelPrediction package. , PatientLevelPrediction makes extensive use Cyclops package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   } citation(\"Cyclops\") ## To cite Cyclops in publications use: ##  ##   Suchard MA, Simpson SE, Zorych I, Ryan P, Madigan D (2013). \"Massive ##   parallelization of serial inference algorithms for complex ##   generalized linear models.\" _ACM Transactions on Modeling and ##   Computer Simulation_, *23*, 10. ##   <https://dl.acm.org/doi/10.1145/2414416.2414791>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {M. A. Suchard and S. E. Simpson and I. Zorych and P. Ryan and D. Madigan}, ##     title = {Massive parallelization of serial inference algorithms for complex generalized linear models}, ##     journal = {ACM Transactions on Modeling and Computer Simulation}, ##     volume = {23}, ##     pages = {10}, ##     year = {2013}, ##     url = {https://dl.acm.org/doi/10.1145/2414416.2414791}, ##   }"},{"path":"/articles/BuildingPredictiveModels.html","id":"appendix-1-study-population-settings-details","dir":"Articles","previous_headings":"","what":"Appendix 1: Study population settings details","title":"Building patient-level predictive models","text":"figures effect shown removeSubjectsWithPriorOutcome, requireTimAtRisk, includeAllOutcomes booleans final study population. start Target Cohort firstExposureOnly = false require washout period = 1095. subset target cohort based additional constraints. final study population Venn diagrams colored green. Require minimum time--risk person target cohort  Require minumum time--risk target cohort, except persons outcomes time--risk.  ) Include persons target cohort exclude persons prior outcomes  Require minimum time--risk target cohort, except persons outcomes time--risk, exclude persons prior outcomes  ) Include persons target cohort exclude persons prior outcomes  Include persons target cohort","code":""},{"path":[]},{"path":[]},{"path":"/articles/ConstrainedPredictors.html","id":"how-to-use-the-phenotypelibrary-r-package","dir":"Articles","previous_headings":"Constrained Predictors","what":"How to use the PhenotypeLibrary R package","title":"Constrained Predictors","text":"provide set phenotypes can used predictors prediction models best practice research. phenotypes can extracted PhenotypeLibrary R package. install R package run: extract cohort definition Alcoholism id 1165, just run: general can extract cohorts running:","code":"remotes::install_github(\"ohdsi/PhenotypeLibrary\") PhenotypeLibrary::getPlCohortDefinitionSet(1165) phenotypeDefinitions <- PhenotypeLibrary::getPlCohortDefinitionSet(1152:1215)"},{"path":[]},{"path":"/articles/CreatingLearningCurves.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Creating Learning Curves","text":"vignette describes can use Observational Health Data Sciences Informatics (OHDSI) PatientLevelPrediction package create learning curves. vignette assumes read comfortable building patient level prediction models described vignette('BuildingPredictiveModels'). Prediction models show overly-optimistic performance predicting data used training. Therefore, best-practice partition data training set testing set. train prediction model training set portion asses ability generalize unseen data measuring performance testing set. Learning curves assess effect training set size model performance training sequence prediction models successively larger subsets training set. learning curve plot can also help diagnosing bias variance problem explained . Figure 1, shows example learning curve plot vertical axis represents model performance horizontal axis training set size. training set size small, performance training set high, model can often fitted well limited number training examples. time, performance testing set poor, model trained limited number training examples generalize well unseen data testing set. training set size increases, performance model training set decrease. becomes difficult model find good fit training examples. Also, model trained representative portion training examples, making generalize better unseen data. can observed increasin testing set performance. learning curve can help us diagnosing bias variance problems classifier provide guidance improve model. can observe high variance (overfitting) prediction model performs well training set, poorly testing set (Figure 2). Adding additional data common approach counteract high variance. learning curve becomes apparent, adding additional data may improve performance testing set little , learning curve yet plateaued , thus, model saturated yet. Therefore, adding data decrease gap training set testing set, main indicator high variance problem. Furthermore, can observe high bias (underfitting) prediction model performs poorly training set well testing set (Figure 3). learning curves training set testing set flattened low performance small gap . Adding additional data case little impact model performance. Choosing another prediction algorithm can find complex (example non-linear) relationships data may alternative approach consider high bias situation.","code":""},{"path":"/articles/CreatingLearningCurves.html","id":"creating-the-learning-curve","dir":"Articles","previous_headings":"","what":"Creating the learning curve","title":"Creating Learning Curves","text":"Use PatientLevelPrediction package create plpData object . Alternatively, can make use data simulator. following code snippet creates data 12000 patients. Specify population settings (additional exclusions requiring minimum prior observation prior outcome well specifying time--risk period enable labels created): Specify prediction algorithm used. Specify split settings sequence training set fractions (ride splitSetting trainFraction). Alternatively, instead trainFractions, can provide sequence training events (trainEvents) instead training set fractions. recommended, research shown number events important determinant model performance. Make sure training set contains number events specified. Create learning curve object. Plot learning curve object (Figure 4). Specify one available metrics: AUROC, AUPRC, sBrier. Moreover, can specify metric put abscissa, number observations number events. recommend latter, events determinant model performance allow better compare learning curves across different prediction problems databases.","code":"set.seed(1234) data(simulationProfile) sampleSize <- 12000 plpData <- simulatePlpData(   plpDataSimulationProfile,   n = sampleSize ) populationSettings <- createStudyPopulationSettings(   binary = TRUE,   firstExposureOnly = FALSE,   washoutPeriod = 0,   removeSubjectsWithPriorOutcome = FALSE,   priorOutcomeLookback = 99999,   requireTimeAtRisk = FALSE,   minTimeAtRisk = 0,   riskWindowStart = 0,   riskWindowEnd = 365,   verbosity = \"INFO\" ) # Use LASSO logistic regression modelSettings <- setLassoLogisticRegression() splitSettings <- createDefaultSplitSetting(   testFraction = 0.2,   type = \"stratified\",   splitSeed = 1000 )  trainFractions <- seq(0.1, 0.8, 0.1) # Create eight training set fractions learningCurve <- createLearningCurve(   plpData = plpData,   outcomeId = 2,   parallel = TRUE,   cores = 4,   modelSettings = modelSettings,   saveDirectory = file.path(tempdir(), \"learningCurve\"),   analysisId = \"learningCurve\",   populationSettings = populationSettings,   splitSettings = splitSettings,   trainFractions = trainFractions,   trainEvents = NULL,   preprocessSettings = createPreprocessSettings(     minFraction = 0.001,     normalize = TRUE   ),   executeSettings = createExecuteSettings(     runSplitData = TRUE,     runSampleData = FALSE,     runFeatureEngineering = FALSE,     runPreprocessData = TRUE,     runModelDevelopment = TRUE,     runCovariateSummary = FALSE   ) ) plotLearningCurve(   learningCurve,   metric = \"AUROC\",   abscissa = \"events\",   plotTitle = \"Learning Curve\",   plotSubtitle = \"AUROC performance\" )"},{"path":"/articles/CreatingLearningCurves.html","id":"parallel-processing","dir":"Articles","previous_headings":"","what":"Parallel processing","title":"Creating Learning Curves","text":"learning curve object can created parallel, can reduce computation time significantly. Whether run code parallel specified using parallel input. Currently functionality available LASSO logistic regression gradient boosting machines. Depending number parallel workers may require significant amount memory. advise use parallelized learning curve function parameter search exploratory data analysis. running parrallel, R find number available processing cores automatically register required parallel backend. Alternatively, can provide number cores wish use via cores input.","code":""},{"path":"/articles/CreatingLearningCurves.html","id":"demo","dir":"Articles","previous_headings":"","what":"Demo","title":"Creating Learning Curves","text":"added demo learningcurve: note running demo can take considerable amount time (15 min Quad core running parallel)!","code":"# Show all demos in our package: demo(package = \"PatientLevelPrediction\")  # Run the learning curve demo(\"LearningCurveDemo\", package = \"PatientLevelPrediction\")"},{"path":"/articles/CreatingLearningCurves.html","id":"publication","dir":"Articles","previous_headings":"","what":"Publication","title":"Creating Learning Curves","text":"publication titled ‘little data need patient-level prediction?’ uses learning curve functionality package can accessed https://doi.org/10.1016/j.ijmedinf.2022.104762.","code":""},{"path":"/articles/CreatingLearningCurves.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Creating Learning Curves","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":"/articles/CreatingNetworkStudies.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Making patient-level predictive network study packages","text":"OHDSI Patient Level Prediction (PLP) package provides framework implement prediction models scale. can range developing large number models across sites (methodology study design insight) extensive external validation existing models OHDSI PLP framework (model insight). vignette describes can use PatientLevelPrediction package create network study package.","code":""},{"path":"/articles/CreatingNetworkStudies.html","id":"useful-publication","dir":"Articles","previous_headings":"","what":"Useful publication","title":"Making patient-level predictive network study packages","text":"open access publication standardized analytics pipeline reliable rapid development validation prediction models using observational health data details process used develop validate prediction models using OHDSI prediction framework tools. publication describes steps demonstrates focusing predicting death covid-19.","code":""},{"path":[]},{"path":"/articles/CreatingNetworkStudies.html","id":"step-1-developing-the-study","dir":"Articles","previous_headings":"Main steps for running a network study","what":"Step 1 – developing the study","title":"Making patient-level predictive network study packages","text":"Design study: target/outcome cohort logic, concept sets medical definitions, settings developing new model validation adding existing models framework. Suggestion: look literature validated definitions. Write protocol motivates study provides full details (sufficient people replicate study future). Write R package implementing study across diverse computational environments [see guidance structure package use skeleton github package : https://github.com/OHDSI/SkeletonPredictionStudy ]","code":""},{"path":"/articles/CreatingNetworkStudies.html","id":"step-2-implementing-the-study-part-1","dir":"Articles","previous_headings":"Main steps for running a network study","what":"Step 2 – implementing the study part 1","title":"Making patient-level predictive network study packages","text":"Get contributors install package dependencies. Ensure package installed correctly contributor asking run checkInstall functions (specified InstallationGuide). Get contributors run createCohort function inspect target/outcome definitions. definitions suitable site, go back step 1 revise cohort definitions.","code":""},{"path":"/articles/CreatingNetworkStudies.html","id":"step-3-implementing-the-study-part-2-make-sure-the-package-is-functioning-as-planned-and-the-definitions-are-valid-across-sites","dir":"Articles","previous_headings":"Main steps for running a network study","what":"Step 3 – implementing the study part 2 (make sure the package is functioning as planned and the definitions are valid across sites)","title":"Making patient-level predictive network study packages","text":"Get contributors run main.R settings configured environment Get contributors submit results","code":""},{"path":"/articles/CreatingNetworkStudies.html","id":"step-4-publication","dir":"Articles","previous_headings":"Main steps for running a network study","what":"Step 4 – Publication","title":"Making patient-level predictive network study packages","text":"study creator first option first author, /wish first author /can pick suitable person contributors. contributors listed authors paper. last author person lead/managed study, first author first author can pick suitable last author. authors first last author alphabetical last name.","code":""},{"path":"/articles/CreatingNetworkStudies.html","id":"package-skeleton---file-structure","dir":"Articles","previous_headings":"","what":"Package Skeleton - File Structure","title":"Making patient-level predictive network study packages","text":"DESCRIPTION: file describes R package dependencies NAMESPACE: file created automatically Roxygen Readme.md: file provide step step guidance implementing package R helpers.r: custom functions used package file (e.g., checkInstall) main.r: file call functions helpers.r execute full study submit.r: file called end submit compressed folder study creator/manager. Man: folder contain documentation functions helpers.r (automatically generated roxygen) Inst sql/sql_sever * targetCohort: target cohort parameterised sql code * outcomeCohort: outcome cohort parameterised sql code plp_models: place PLP models Extras","code":""},{"path":[]},{"path":"/articles/GISExample.html","id":"motivation","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building","what":"Motivation","title":"Integration of GIS Data Into OHDSI Model Building","text":"Although proposed GIS extension tables approved broader OHDSI community, yet integrated natively OHDSI tooling. tables data capture, however, can still referenced standard model building analytics workflows. purpose analytics demonstration show data EXPOSURE_OCCURRENCE table can utilized training evaluation OMOP-specific patient-level-prediction (PLP) model. analytic process executed describe relies GIS-version Tufts Synthetic Dataset EHR geospatial data integrated; see description dataset details contents access. Much work based detailed vignette describes custom feature engineering PLP vignette('AddingCustomFeatureEngineering')","code":""},{"path":[]},{"path":"/articles/GISExample.html","id":"step-1-create-target-outcome-cohorts","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building > Step-by-Step Process","what":"Step 1: Create Target & Outcome Cohorts","title":"Integration of GIS Data Into OHDSI Model Building","text":"defined target cohort within sampling procedures creating subset Tufts Synthetic Data, described process length elsewhere. purposes using PLP package, formalized group individuals cohort table using simple cohort definition including individuals “visit”, include atlas-compatible json definition reference. also created outcome cohort - case, patients COPD conceptual descendant thereof - Atlas shared json definition (also included equivalent SQL script).","code":""},{"path":"/articles/GISExample.html","id":"step-2-create-generic-plp-lasso-logistic-regression-model-in-r","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building > Step-by-Step Process","what":"Step 2: Create Generic PLP Lasso Logistic Regression Model in R","title":"Integration of GIS Data Into OHDSI Model Building","text":"possible create R package serves basis PLP model using Atlas, given Atlas yet support GIS extension, created demo model directly using PLP package. configuring environment (important!) appropriately, imported necessary packages defined relevant parameters data source: defining parameters, created database settings object launched runMultiplePLP function create base plpData object:","code":"library(PatientLevelPrediction) library(dplyr) outputFolder <- \"/ohdsi-gis/copdResultsPM25_NEW\" saveDirectory <- outputFolder ExecutionDateTime <- Sys.time() logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = T, logName =                                   \"runPlp Log\") analysisName = 'Generic PLP'  # Details for connecting to the server: connectionDetails <- DatabaseConnector::createConnectionDetails(         dbms = 'spark',         server = '/default',         connectionString = '<REDACTED>'     ) # Add the database containing the OMOP CDM data cdmDatabaseSchema <- 'gis_syn_dataset_5_4' # Add a sharebale name for the database containing the OMOP CDM data cdmDatabaseName <- 'TSD-GIS' # Add a database with read/write access as this is where the cohorts will be generated cohortDatabaseSchema <- 'gis_syn_dataset_5_4' tempEmulationSchema <- NULL # table name where the cohorts will be generated cohortTable <- 'cohort' databaseDetails <- PatientLevelPrediction::createDatabaseDetails(         connectionDetails = connectionDetails,          cdmDatabaseSchema = cdmDatabaseSchema,          cdmDatabaseName = cdmDatabaseName,          tempEmulationSchema = tempEmulationSchema,          cohortDatabaseSchema = cohortDatabaseSchema,          cohortTable = cohortTable,          outcomeDatabaseSchema = cohortDatabaseSchema,           outcomeTable = cohortTable,          cdmVersion = 5 )   # Run very simple LR model against two cohorts created in Atlas. Use model  # as basis for augmented model with pollutants below  runMultiplePlp(    databaseDetails = databaseDetails,    modelDesignList = list(createModelDesign(targetId = 9, outcomeId = 8, modelSettings =                                               setLassoLogisticRegression())),    onlyFetchData = F,    cohortDefinitions = NULL,    logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = T, logName =                                      \"runPlp Log\"),    saveDirectory = outputFolder,    sqliteLocation = file.path(saveDirectory, \"sqlite\")  )"},{"path":"/articles/GISExample.html","id":"step-3-split-plpdata-object-to-traintest-augment-labels-with-exposure_occurrence-values","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building > Step-by-Step Process","what":"Step 3: Split plpData object to train/test, augment labels with EXPOSURE_OCCURRENCE values","title":"Integration of GIS Data Into OHDSI Model Building","text":"labels sub-object within plpData contains per-individual data elements like gender age; added additional data element object derived EXPOSURE_OCCURRENCE table:","code":"cohortDefinitions <- NULL modelDesign <- createModelDesign(targetId = 9, outcomeId = 8, modelSettings = setLassoLogisticRegression()) populationSettings <- modelDesign$populationSettings splitSettings <- modelDesign$splitSettings  plpData <- loadPlpData(\"/ohdsi-gis/copdResultsPM25_B/targetId_9_L1\")  mySplit <- splitData (plpData = plpData,                       population = createStudyPopulation(plpData, 8, populationSettings),                       splitSettings = splitSettings)   labelTrain <- mySplit$Train$labels conn <- DatabaseConnector::connect(connectionDetails) pollutants <- DatabaseConnector::querySql(conn, \"SELECT person_id as subjectID, CAST(MEAN(value_as_number) AS DOUBLE) AS pmValue FROM gis_syn_dataset_5_4.exposure_occurrence WHERE value_as_number IS NOT NULL GROUP BY person_id;\") labelTrainPol <- merge(x=labelTrain, y=pollutants, by.x = \"subjectId\", by.y = \"SUBJECTID\")  mySplit$Train$labels <- labelTrainPol  labelTest <- mySplit$Test$labels labelTestPol <- merge(x=labelTest, y=pollutants, by.x = \"subjectId\", by.y = \"SUBJECTID\")  mySplit$Test$labels <- labelTestPol  trainData <- mySplit$Train  testData <- mySplit$Test"},{"path":"/articles/GISExample.html","id":"step-4-reference-augmented-label-objects-in-custom-feature-engineering-function","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building > Step-by-Step Process","what":"Step 4: Reference augmented label objects in custom feature engineering function","title":"Integration of GIS Data Into OHDSI Model Building","text":"like convert per-patient labels covariateData structure referenced PLP workflow. , able follow feature engineering vignette create two functions, createPollutants implementPollutants: can execute functions create training test data objects contain extended covariates: Note plot output model GAM fitting implementPollutants function, end plot aligns well underlying relationship Odds Ratio PM2.5 concentration used distribute synthetic data location:","code":"createPollutants <- function(                      method = 'QNCV'                      ){      # create list of inputs to implement function   featureEngineeringSettings <- list(     method = method     )      # specify the function that will implement the sampling   attr(featureEngineeringSettings, \"fun\") <- \"implementPollutants\"    # make sure the object returned is of class \"sampleSettings\"   class(featureEngineeringSettings) <- \"featureEngineeringSettings\"   return(featureEngineeringSettings)    }   implementPollutants <- function(trainData, featureEngineeringSettings, model=NULL) {   if (is.null(model)) {     method <- featureEngineeringSettings$method     gisData <- trainData$labels     y <- gisData$outcomeCount     X <- gisData$PMVALUE     model <- mgcv::gam(       y ~ s(X, bs='cr', k=5, m=2)     )     newData <- data.frame(       rowId = gisData$rowId,       covariateId = 2052499839,       covariateValue = model$fitted.values      )   }   else {     gisData <- trainData$labels     X <- gisData$PMVALUE     y <- gisData$outcomeCount     newData <- data.frame(y=y, X=X)     yHat <- predict(model, newData)     newData <- data.frame(       rowId = gisData$rowId,       covariateId = 2052499839,       covariateValue = yHat     )   }   # update covRef   Andromeda::appendToTable(trainData$covariateData$covariateRef,                             data.frame(covariateId=2052499839,                                       covariateName='Average PM2.5 Concentrations',                                       analysisId=1,                                       conceptId=2052499839))      # update covariates   Andromeda::appendToTable(trainData$covariateData$covariates, newData)      featureEngineering <- list(     funct = 'implementPollutants',     settings = list(       featureEngineeringSettings = featureEngineeringSettings,       model = model     )   )      attr(trainData$covariateData, 'metaData')$featureEngineering = listAppend(     attr(trainData$covariateData, 'metaData')$featureEngineering,     featureEngineering   )      trainData$model <- model      return(trainData) } featureEngineeringSettingsPol <- createPollutants('QNCV') trainDataPol <- implementPollutants(trainData, featureEngineeringSettings) testDataPol <- implementPollutants(testData, featureEngineeringSettings, trainDataPol$model)"},{"path":"/articles/GISExample.html","id":"step-5-apply-new-train-and-test-datasets-to-runplp-and-evaluate-output","dir":"Articles","previous_headings":"Integration of GIS Data into OHDSI Model Building > Step-by-Step Process","what":"Step 5: Apply new train and test datasets to runPlp and evaluate output","title":"Integration of GIS Data Into OHDSI Model Building","text":"","code":"analysisId <- '1' analysisPath = file.path(saveDirectory, analysisId)  settings <- list(   trainData = trainDataPol,    modelSettings = setLassoLogisticRegression(),   analysisId = analysisId,   analysisPath = analysisPath )  ParallelLogger::logInfo(sprintf('Training %s model',settings$modelSettings$name))   model <- tryCatch(   {     do.call(fitPlp, settings)   },   error = function(e) { ParallelLogger::logError(e); return(NULL)} )   prediction <- model$prediction # remove prediction from model model$prediction <- NULL  #apply to test data if exists: if('Test' %in% names(data)){ predictionTest <- tryCatch(   {     predictPlp(       plpModel = model,        plpData = testDataPol,       population = testDataPol$labels     )   },   error = function(e) { ParallelLogger::logError(e); return(NULL)} )  predictionTest$evaluationType <- 'Test'  if(!is.null(predictionTest)){   prediction <- rbind(predictionTest, prediction[, colnames(prediction)!='index']) }   }"},{"path":"/articles/InstallationGuide.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Patient-Level Prediction Installation Guide","text":"vignette describes need install Observational Health Data Science Informatics (OHDSI) PatientLevelPrediction package Windows, Mac, Linux.","code":""},{"path":[]},{"path":"/articles/InstallationGuide.html","id":"windows-users","dir":"Articles","previous_headings":"Software Prerequisites","what":"Windows Users","title":"Patient-Level Prediction Installation Guide","text":"Windows OHDSI Patient Level Prediction (PLP) package requires installing: R (https://cran.r-project.org/ ) - (R >= 4.0.0, latest recommended) Rstudio (https://posit.co/ ) Java RTools (https://cran.r-project.org/bin/windows/Rtools/)","code":""},{"path":"/articles/InstallationGuide.html","id":"maclinux-users","dir":"Articles","previous_headings":"Software Prerequisites","what":"Mac/Linux Users","title":"Patient-Level Prediction Installation Guide","text":"Mac Linux OHDSI Patient Level Prediction (PLP) package requires installing: R (https://cran.r-project.org/ ) - (R >= 4.0.0, latest recommended) Rstudio (https://posit.co/ ) Java Xcode command line tools(run terminal: xcode-select –install) [MAC USERS ]","code":""},{"path":"/articles/InstallationGuide.html","id":"installing-the-package","dir":"Articles","previous_headings":"","what":"Installing the Package","title":"Patient-Level Prediction Installation Guide","text":"preferred way install package using remotes, automatically install latest release latest dependencies. want official release install bleeding edge version package (latest develop branch). Note latest develop branch contain bugs, please report us experience problems.","code":""},{"path":"/articles/InstallationGuide.html","id":"installing-patientlevelprediction-using-remotes","dir":"Articles","previous_headings":"Installing the Package","what":"Installing PatientLevelPrediction using remotes","title":"Patient-Level Prediction Installation Guide","text":"install using remotes run: installing make sure close Rstudio sessions using PatientLevelPrediction dependency. Keeping Rstudio sessions open can cause locks prevent package installing.","code":"install.packages(\"remotes\") remotes::install_github(\"OHDSI/PatientLevelPrediction\")"},{"path":"/articles/InstallationGuide.html","id":"creating-python-reticulate-environment","dir":"Articles","previous_headings":"","what":"Creating Python Reticulate Environment","title":"Patient-Level Prediction Installation Guide","text":"Many classifiers PatientLevelPrediction use Python backend. set python environment run:","code":"library(PatientLevelPrediction) reticulate::install_miniconda() configurePython(envname = \"r-reticulate\", envtype = \"conda\")"},{"path":"/articles/InstallationGuide.html","id":"installation-issues","dir":"Articles","previous_headings":"","what":"Installation issues","title":"Patient-Level Prediction Installation Guide","text":"Installation issues need posted issue tracker: https://github.com/OHDSI/PatientLevelPrediction/issues list provides solutions common issues: error trying install package R saying ‘Dependancy X available …’ can sometimes fixed running install.packages('X') completes trying reinstall package error. found using github remotes install packages can impacted multiple R sessions open one session library open can cause library locked can prevent install package depends library.","code":""},{"path":[]},{"path":"/articles/InstallationGuide.html","id":"python-environment-maclinux-users","dir":"Articles","previous_headings":"Installation issues > Common issues","what":"python environment Mac/linux users:","title":"Patient-Level Prediction Installation Guide","text":"make sure R uses r-reticulate python environment may need edit .Rprofile location python binary PLP environment. Edit .Rprofile running: add file save. python bin location location returned e.g., PLP virtual environment location /anaconda3/envs/PLP/bin/python added: Sys.setenv(PATH = paste(“/anaconda3/envs/PLP/bin”, Sys.getenv(“PATH”), sep=“:”))","code":"usethis::edit_r_profile() Sys.setenv(PATH = paste(\"your python bin location\", Sys.getenv(\"PATH\"), sep = \":\")) reticulate::conda_list()"},{"path":"/articles/InstallationGuide.html","id":"acknowledgments","dir":"Articles","previous_headings":"","what":"Acknowledgments","title":"Patient-Level Prediction Installation Guide","text":"Considerable work dedicated provide PatientLevelPrediction package. Please reference paper use PLP Package work: Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. work supported part National Science Foundation grant IIS 1251151.","code":"citation(\"PatientLevelPrediction\") ## To cite PatientLevelPrediction in publications use: ##  ##   Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). \"Design ##   and implementation of a standardized framework to generate and ##   evaluate patient-level prediction models using observational ##   healthcare data.\" _Journal of the American Medical Informatics ##   Association_, *25*(8), 969-975. ##   <https://doi.org/10.1093/jamia/ocy032>. ##  ## A BibTeX entry for LaTeX users is ##  ##   @Article{, ##     author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek}, ##     title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data}, ##     journal = {Journal of the American Medical Informatics Association}, ##     volume = {25}, ##     number = {8}, ##     pages = {969-975}, ##     year = {2018}, ##     url = {https://doi.org/10.1093/jamia/ocy032}, ##   }"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Egill Fridgeirsson. Author, maintainer. Jenna Reps. Author. Martijn Schuemie. Author. Marc Suchard. Author. Patrick Ryan. Author. Peter Rijnbeek. Author. Observational Health Data Science Informatics. Copyright holder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek P (2018). “Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data.” Journal American Medical Informatics Association, 25(8), 969-975. https://doi.org/10.1093/jamia/ocy032.","code":"@Article{,   author = {J. M. Reps and M. J. Schuemie and M. A. Suchard and P. B. Ryan and P. Rijnbeek},   title = {Design and implementation of a standardized framework to generate and evaluate patient-level prediction models using observational healthcare data},   journal = {Journal of the American Medical Informatics Association},   volume = {25},   number = {8},   pages = {969-975},   year = {2018},   url = {https://doi.org/10.1093/jamia/ocy032}, }"},{"path":"/index.html","id":"patientlevelprediction","dir":"","previous_headings":"","what":"Develop Clinical Prediction Models Using the Common Data Model","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"PatientLevelPrediction part HADES.","code":""},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"PatientLevelPrediction R package building validating patient-level predictive models using data OMOP Common Data Model format. Reps JM, Schuemie MJ, Suchard MA, Ryan PB, Rijnbeek PR. Design implementation standardized framework generate evaluate patient-level prediction models using observational healthcare data. J Med Inform Assoc. 2018;25(8):969-975. figure illustrates prediction problem address. Among population risk, aim predict patients defined moment time (t = 0) experience outcome time--risk. Prediction done using information patients observation window prior moment time.  define prediction problem define t=0 Target Cohort (T), outcome like predict outcome cohort (O), time--risk (TAR). Furthermore, make design choices model like develop, determine observational datasets perform internal external validation. conceptual framework works type prediction problems, example presented (T=green, O=red).","code":""},{"path":"/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Takes one target cohorts (Ts) one outcome cohorts (Os) develops validates models T O combinations. Allows multiple prediction design options. Extracts necessary data database OMOP Common Data Model format multiple covariate settings. Uses large set covariates including example drugs, diagnoses, procedures, well age, comorbidity indexes, custom covariates. Allows add custom covariates cohort covariates. Includes large number state---art machine learning algorithms can used develop predictive models, including Regularized logistic regression, Random forest, Gradient boosting machines, Decision tree, Naive Bayes, K-nearest neighbours, Neural network, AdaBoost Support vector machines. Allows add custom algorithms. Allows add custom feature engineering Allows add custom /sampling (sampling) [note: based existing research recommended] Contains functionality externally validate models. Includes functions plot explore model performance (ROC + Calibration). Build ensemble models using EnsemblePatientLevelPrediction. Build Deep Learning models using DeepPatientLevelPrediction. Generates learning curves. Includes shiny app interactively view explore results. shiny app can create html file document (report protocol) containing study results.","code":""},{"path":"/index.html","id":"screenshots","dir":"","previous_headings":"","what":"Screenshots","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Demo Shiny Apps can found : Multiple Prediction Viewer Shiny App","code":""},{"path":"/index.html","id":"technology","dir":"","previous_headings":"","what":"Technology","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"PatientLevelPrediction R package, functions using python reticulate.","code":""},{"path":"/index.html","id":"system-requirements","dir":"","previous_headings":"","what":"System Requirements","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Requires R (version 4.0 higher). Installation Windows requires RTools. Libraries used PatientLevelPrediction require Java Python. python installation required machine learning algorithms. advise install Python 3.9 higher using Anaconda (https://www.continuum.io/downloads).","code":""},{"path":"/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting Started","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"install package please read Package Installation guide look video extensive demo package.  Please read main vignette package: Building Single Patient-Level Predictive Models addition created vignettes describe advanced functionality detail: Building Multiple Patient-Level Predictive Models Adding Custom Machine Learning Algorithms Building Deep Learning Models Building Ensemble Models Creating Learning Curves Package function reference: Reference","code":""},{"path":"/index.html","id":"user-documentation","dir":"","previous_headings":"","what":"User Documentation","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Documentation can found package website.","code":""},{"path":"/index.html","id":"support","dir":"","previous_headings":"","what":"Support","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Developer questions/comments/feedback: OHDSI Forum use GitHub issue tracker bugs/issues/enhancements","code":""},{"path":"/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"Read can contribute package.","code":""},{"path":"/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"PatientLevelPrediction licensed Apache License 2.0","code":""},{"path":"/index.html","id":"development","dir":"","previous_headings":"","what":"Development","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"PatientLevelPrediction developed R Studio.","code":""},{"path":"/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Develop Clinical Prediction Models Using the Common Data Model","text":"package maintained Egill Fridgeirsson Jenna Reps developed major contributions Peter Rijnbeek, Martijn Schuemie, Patrick Ryan, Marc Suchard. like thank following persons contributions package: Seng Chan , Ross Williams, Henrik John, Xiaoyong Pan, James Wiggins, Alexandros Rekkas project supported part National Science Foundation grant IIS 1251151.","code":""},{"path":"/reference/MapIds.html","id":null,"dir":"Reference","previous_headings":"","what":"Map covariate and row Ids so they start from 1 — MapIds","title":"Map covariate and row Ids so they start from 1 — MapIds","text":"functions takes covariate data cohort/population remaps covariate row ids, restricts pop saves/creates mapping","code":""},{"path":"/reference/MapIds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map covariate and row Ids so they start from 1 — MapIds","text":"","code":"MapIds(covariateData, cohort = NULL, mapping = NULL)"},{"path":"/reference/MapIds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map covariate and row Ids so they start from 1 — MapIds","text":"covariateData covariateData object cohort specified rowIds restricted ones cohort mapping pre defined mapping use","code":""},{"path":"/reference/MapIds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map covariate and row Ids so they start from 1 — MapIds","text":"new covariateData object remapped covariate row ids","code":""},{"path":"/reference/MapIds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map covariate and row Ids so they start from 1 — MapIds","text":"","code":"covariateData <- Andromeda::andromeda(   covariates = data.frame(rowId = c(1, 3, 5, 7, 9),                            covariateId = c(10, 20, 10, 10, 20),                           covariateValue = c(1, 1, 1, 1, 1)),   covariateRef = data.frame(covariateId = c(10, 20),                                covariateNames = c(\"covariateA\",                                                   \"covariateB\"),                               analysisId = c(1, 1))) mappedData <- MapIds(covariateData) #> starting to map the columns and rows #> finished MapCovariates # columnId and rowId are now starting from 1 and are consecutive mappedData$covariates #> # Source:   table<`covariates`> [?? x 4] #> # Database: sqlite 3.47.1 [/tmp/RtmpXFoGyv/file23585fe43f5b.sqlite] #>   covariateId covariateValue columnId rowId #>         <dbl>          <dbl>    <int> <int> #> 1          10              1        1     1 #> 2          20              1        2     2 #> 3          10              1        1     3 #> 4          10              1        1     4 #> 5          20              1        2     5"},{"path":"/reference/PatientLevelPrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"PatientLevelPrediction — PatientLevelPrediction","title":"PatientLevelPrediction — PatientLevelPrediction","text":"package running predictions using data OMOP CDM","code":""},{"path":[]},{"path":"/reference/PatientLevelPrediction.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"PatientLevelPrediction — PatientLevelPrediction","text":"Maintainer: Egill Fridgeirsson e.fridgeirsson@erasmusmc.nl Authors: Jenna Reps jreps@.jnj.com Martijn Schuemie Marc Suchard Patrick Ryan Peter Rijnbeek contributors: Observational Health Data Science Informatics [copyright holder]","code":""},{"path":"/reference/averagePrecision.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the average precision — averagePrecision","title":"Calculate the average precision — averagePrecision","text":"Calculate average precision","code":""},{"path":"/reference/averagePrecision.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the average precision — averagePrecision","text":"","code":"averagePrecision(prediction)"},{"path":"/reference/averagePrecision.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the average precision — averagePrecision","text":"prediction prediction object","code":""},{"path":"/reference/averagePrecision.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the average precision — averagePrecision","text":"average precision value","code":""},{"path":"/reference/averagePrecision.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the average precision — averagePrecision","text":"Calculates average precision predition object","code":""},{"path":"/reference/averagePrecision.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the average precision — averagePrecision","text":"","code":"prediction <- data.frame(   value = c(0.1, 0.2, 0.3, 0.4, 0.5),   outcomeCount = c(0, 1, 0, 1, 1) ) averagePrecision(prediction) #> [1] 0.9166667"},{"path":"/reference/brierScore.html","id":null,"dir":"Reference","previous_headings":"","what":"brierScore — brierScore","title":"brierScore — brierScore","text":"brierScore","code":""},{"path":"/reference/brierScore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"brierScore — brierScore","text":"","code":"brierScore(prediction)"},{"path":"/reference/brierScore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"brierScore — brierScore","text":"prediction prediction dataframe","code":""},{"path":"/reference/brierScore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"brierScore — brierScore","text":"list containing brier score scaled brier score","code":""},{"path":"/reference/brierScore.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"brierScore — brierScore","text":"Calculates brierScore prediction object","code":""},{"path":"/reference/brierScore.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"brierScore — brierScore","text":"","code":"prediction <- data.frame(   value = c(0.1, 0.2, 0.3, 0.4, 0.5),   outcomeCount = c(0, 1, 0, 1, 1)) brierScore(prediction) #> $brier #> [1] 0.27 #>  #> $brierScaled #> [1] -0.2857143 #>"},{"path":"/reference/calibrationInLarge.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the calibration in large — calibrationInLarge","title":"Calculate the calibration in large — calibrationInLarge","text":"Calculate calibration large","code":""},{"path":"/reference/calibrationInLarge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the calibration in large — calibrationInLarge","text":"","code":"calibrationInLarge(prediction)"},{"path":"/reference/calibrationInLarge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the calibration in large — calibrationInLarge","text":"prediction prediction dataframe","code":""},{"path":"/reference/calibrationInLarge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the calibration in large — calibrationInLarge","text":"data.frame meanPredictionRisk, observedRisk, N","code":""},{"path":"/reference/calibrationLine.html","id":null,"dir":"Reference","previous_headings":"","what":"calibrationLine — calibrationLine","title":"calibrationLine — calibrationLine","text":"calibrationLine","code":""},{"path":"/reference/calibrationLine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"calibrationLine — calibrationLine","text":"","code":"calibrationLine(prediction, numberOfStrata = 10)"},{"path":"/reference/calibrationLine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"calibrationLine — calibrationLine","text":"prediction prediction object numberOfStrata number groups split prediction ","code":""},{"path":"/reference/calibrationLine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"calibrationLine — calibrationLine","text":"list containing calibrationLine coefficients, aggregate data used fit line Hosmer-Lemeshow goodness fit test","code":""},{"path":"/reference/calibrationLine.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"calibrationLine — calibrationLine","text":"","code":"prediction <- data.frame(   value = c(0.1, 0.2, 0.3, 0.4, 0.5),   outcomeCount = c(0, 1, 0, 1, 1)) calibrationLine(prediction, numberOfStrata = 1) #> Warning: NaNs produced #> $lm #> Intercept  Gradient  #>      -0.3       3.0  #>  #> $aggregateLmData #>   group  obs pred #> 1     1 0.00 0.10 #> 2     2 0.75 0.35 #>  #> $hosmerlemeshow #>   Xsquared df pvalue #> 1 2.924298 -1    NaN #>"},{"path":"/reference/computeAuc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the area under the ROC curve — computeAuc","title":"Compute the area under the ROC curve — computeAuc","text":"Compute area ROC curve","code":""},{"path":"/reference/computeAuc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the area under the ROC curve — computeAuc","text":"","code":"computeAuc(prediction, confidenceInterval = FALSE)"},{"path":"/reference/computeAuc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the area under the ROC curve — computeAuc","text":"prediction prediction object generated using predict functions. confidenceInterval 95 percebt confidence intervals computed?","code":""},{"path":"/reference/computeAuc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the area under the ROC curve — computeAuc","text":"data.frame containing AUC optionally 95% confidence interval","code":""},{"path":"/reference/computeAuc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute the area under the ROC curve — computeAuc","text":"Computes area ROC curve predicted probabilities, given true observed outcomes.","code":""},{"path":"/reference/computeAuc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the area under the ROC curve — computeAuc","text":"","code":"prediction <- data.frame(   value = c(0.1, 0.2, 0.3, 0.4, 0.5),   outcomeCount = c(0, 1, 0, 1, 1)) computeAuc(prediction) #> [1] 0.8333333"},{"path":"/reference/computeGridPerformance.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes grid performance with a specified performance function — computeGridPerformance","title":"Computes grid performance with a specified performance function — computeGridPerformance","text":"Computes grid performance specified performance function","code":""},{"path":"/reference/computeGridPerformance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes grid performance with a specified performance function — computeGridPerformance","text":"","code":"computeGridPerformance(prediction, param, performanceFunct = \"computeAuc\")"},{"path":"/reference/computeGridPerformance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes grid performance with a specified performance function — computeGridPerformance","text":"prediction dataframe predictions outcomeCount per rowId param list hyperparameters performanceFunct string specifying performance function use . Default 'compute_AUC'","code":""},{"path":"/reference/computeGridPerformance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes grid performance with a specified performance function — computeGridPerformance","text":"list overview performance","code":""},{"path":"/reference/computeGridPerformance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Computes grid performance with a specified performance function — computeGridPerformance","text":"","code":"prediction <- data.frame(rowId = c(1, 2, 3, 4, 5),                          outcomeCount = c(0, 1, 0, 1, 0),                          value = c(0.1, 0.9, 0.2, 0.8, 0.3),                          index = c(1, 1, 1, 1, 1)) param <- list(hyperParam1 = 5, hyperParam2 = 100) computeGridPerformance(prediction, param, performanceFunct = \"computeAuc\") #> $metric #> [1] \"computeAuc\" #>  #> $cvPerformance #> [1] 1 #>  #> $cvPerformancePerFold #> [1] 1 #>  #> $param #> $param$hyperParam1 #> [1] 5 #>  #> $param$hyperParam2 #> [1] 100 #>  #>  #> $hyperSummary #>       metric fold value hyperParam1 hyperParam2 #> 1 computeAuc   CV     1           5         100 #> 2 computeAuc    1     1           5         100 #>"},{"path":"/reference/configurePython.html","id":null,"dir":"Reference","previous_headings":"","what":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","title":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","text":"Sets python environment use PLP (can conda venv)","code":""},{"path":"/reference/configurePython.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","text":"","code":"configurePython(envname = \"PLP\", envtype = NULL, condaPythonVersion = \"3.11\")"},{"path":"/reference/configurePython.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","text":"envname string name virtual environment (default 'PLP') envtype option specifying environment 'conda' 'python'.  NULL default 'conda' windows users 'python' non-windows users condaPythonVersion String, Python version use creating conda environment","code":""},{"path":"/reference/configurePython.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","text":"location created conda virtual python environment","code":""},{"path":"/reference/configurePython.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sets up a python environment to use for PLP (can be conda or venv) — configurePython","text":"function creates python environment can used PatientLevelPrediction installs required package dependancies.","code":""},{"path":[]},{"path":"/reference/covariateSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"covariateSummary — covariateSummary","title":"covariateSummary — covariateSummary","text":"Summarises covariateData calculate mean standard deviation per covariate labels given also stratifies class label trainRowIds testRowIds specifying patients train/test sets respectively input, values also stratified train test set","code":""},{"path":"/reference/covariateSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"covariateSummary — covariateSummary","text":"","code":"covariateSummary(   covariateData,   cohort,   labels = NULL,   strata = NULL,   variableImportance = NULL,   featureEngineering = NULL )"},{"path":"/reference/covariateSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"covariateSummary — covariateSummary","text":"covariateData covariateData part plpData extracted using getPlpData cohort patient cohort calculate summary labels data.frame columns rowId outcomeCount strata data.frame containing columns rowId, strataName variableImportance data.frame columns covariateId value (variable importance value) featureEngineering (currently used ) function list functions specifying feature engineering create covariates summarising","code":""},{"path":"/reference/covariateSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"covariateSummary — covariateSummary","text":"data.frame containing: CovariateCount, CovariateMean CovariateStDev specified stratification","code":""},{"path":"/reference/covariateSummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"covariateSummary — covariateSummary","text":"function calculates various metrics measure performance model","code":""},{"path":"/reference/covariateSummary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"covariateSummary — covariateSummary","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n = 100) #> Generating covariates #> Loading required namespace: FeatureExtraction #> Generating cohorts #> Generating outcomes covariateSummary <- covariateSummary(plpData$covariateData, plpData$cohorts) #> Calculating covariate summary @ 2025-04-14 09:15:36.576978 #> This can take a while... #> calculating subset of strata 1 #> Restricting to subgroup #> Calculating summary for subgroup  #> Aggregating with no labels or strata #> Finished covariate summary @ 2025-04-14 09:15:36.83715 #> Time to calculate covariate summary: 0.26 secs head(covariateSummary) #> # A tibble: 6 × 9 #>   covariateId covariateName     analysisId conceptId valueAsConceptId collisions #>         <dbl> <chr>                  <dbl>     <dbl>            <dbl>      <dbl> #> 1    80180102 condition_occurr…        102     80180               NA         NA #> 2    81893102 condition_occurr…        102     81893               NA         NA #> 3    30753102 condition_occurr…        102     30753               NA         NA #> 4  4285898102 condition_occurr…        102   4285898               NA         NA #> 5  4266809102 condition_occurr…        102   4266809               NA         NA #> 6  4310024102 condition_occurr…        102   4310024               NA         NA #> # ℹ 3 more variables: CovariateCount <int>, CovariateMean <dbl>, #> #   CovariateStDev <dbl>"},{"path":"/reference/createCohortCovariateSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts covariates based on cohorts — createCohortCovariateSettings","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"Extracts covariates based cohorts","code":""},{"path":"/reference/createCohortCovariateSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"","code":"createCohortCovariateSettings(   cohortName,   settingId,   cohortDatabaseSchema = NULL,   cohortTable = NULL,   cohortId,   startDay = -30,   endDay = 0,   count = FALSE,   ageInteraction = FALSE,   lnAgeInteraction = FALSE,   analysisId = 456 )"},{"path":"/reference/createCohortCovariateSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"cohortName Name cohort settingId unique id covariate time cohortDatabaseSchema schema database cohort. nothing specified cohortDatabaseSchema databaseDetails runtime used. cohortTable table name contains covariate cohort. nothing specified cohortTable databaseDetails runtime used. cohortId cohort id covariate cohort startDay number days prior index start observing cohort endDay number days prior index stop observing cohort count FALSE covariate value binary (1 means cohort occurred index+startDay index+endDay, 0 means ) TRUE covariate value number unique cohort_start_dates index+startDay index+endDay ageInteraction TRUE multiple covariate value patient's age years lnAgeInteraction TRUE multiple covariate value log patient's age years analysisId analysisId covariate","code":""},{"path":"/reference/createCohortCovariateSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"object class covariateSettings specifying create cohort covariate covariateId cohortId x 100000 + settingId x 1000 + analysisId","code":""},{"path":"/reference/createCohortCovariateSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"user specifies cohort time period covariate constructed whether cohort time periods relative target population cohort index","code":""},{"path":"/reference/createCohortCovariateSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extracts covariates based on cohorts — createCohortCovariateSettings","text":"","code":"createCohortCovariateSettings(cohortName=\"testCohort\",                               settingId=1,                               cohortId=1,                               cohortDatabaseSchema=\"cohorts\",                               cohortTable=\"cohort_table\") #> $covariateName #> [1] \"testCohort\" #>  #> $covariateId #> [1] 101456 #>  #> $cohortDatabaseSchema #> [1] \"cohorts\" #>  #> $cohortTable #> [1] \"cohort_table\" #>  #> $cohortIds #> [1] 1 #>  #> $startDay #> [1] -30 #>  #> $endDays #> [1] 0 #>  #> $count #> [1] FALSE #>  #> $ageInteraction #> [1] FALSE #>  #> $lnAgeInteraction #> [1] FALSE #>  #> $analysisId #> [1] 456 #>  #> attr(,\"fun\") #> [1] \"PatientLevelPrediction::getCohortCovariateData\" #> attr(,\"class\") #> [1] \"covariateSettings\""},{"path":"/reference/createDatabaseDetails.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","title":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","text":"Create setting holds details cdmDatabase connection data extraction","code":""},{"path":"/reference/createDatabaseDetails.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","text":"","code":"createDatabaseDetails(   connectionDetails,   cdmDatabaseSchema,   cdmDatabaseName,   cdmDatabaseId,   tempEmulationSchema = cdmDatabaseSchema,   cohortDatabaseSchema = cdmDatabaseSchema,   cohortTable = \"cohort\",   outcomeDatabaseSchema = cohortDatabaseSchema,   outcomeTable = cohortTable,   targetId = NULL,   outcomeIds = NULL,   cdmVersion = 5,   cohortId = NULL )"},{"path":"/reference/createDatabaseDetails.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","text":"connectionDetails R object type connectionDetails created using function createConnectionDetails DatabaseConnector package. cdmDatabaseSchema name database schema contains OMOP CDM instance. Requires read permissions database. SQL Server, specifiy database schema, example 'cdm_instance.dbo'. cdmDatabaseName string name database - used shiny app externally validating models name result list specify folder name saving validation results (defaults cdmDatabaseSchema specified) cdmDatabaseId string unique identifier database version - stored plp object future reference used shiny app (defaults cdmDatabaseSchema specified) tempEmulationSchema dmbs like Oracle : name database schema want temporary tables managed. Requires create/insert permissions database. cohortDatabaseSchema name database schema location target cohorts available.  Requires read permissions database. cohortTable tablename contains target cohorts.  Expectation cohortTable format COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE. outcomeDatabaseSchema name database schema location data used define outcome cohorts available. Requires read permissions database. outcomeTable tablename contains outcome cohorts.  Expectation outcomeTable format COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE. targetId integer specifying cohort id target cohort outcomeIds single integer vector integers specifying cohort ids outcome cohorts cdmVersion Define OMOP CDM version used: currently support \"4\" \"5\". cohortId (depreciated: use targetId) old input target cohort id","code":""},{"path":"/reference/createDatabaseDetails.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","text":"list database specific settings: connectionDetails: R object type connectionDetails created using function createConnectionDetails DatabaseConnector package. cdmDatabaseSchema: name database schema contains OMOP CDM instance. cdmDatabaseName: string name database - used shiny app externally validating models name result list specify folder name saving validation results (defaults cdmDatabaseSchema specified). cdmDatabaseId: string unique identifier database version - stored plp object future reference used shiny app (defaults cdmDatabaseSchema specified). tempEmulationSchema: name databae schema want temporary tables managed. Requires create/insert permissions database. cohortDatabaseSchema: name database schema location target cohorts available. Requires read permissions schema. cohortTable: tablename contains target cohorts. Expectation cohortTable format COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE. outcomeDatabaseSchema: name database schema location data used define outcome cohorts available. Requires read permissions database. outcomeTable: tablename contains outcome cohorts. Expectation outcomeTable format COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE. targetId: integer specifying cohort id target cohort outcomeIds: single integer vector integers specifying cohort ids outcome cohorts cdmVersion: Define OMOP CDM version used: currently support \"4\" \"5\".","code":""},{"path":"/reference/createDatabaseDetails.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a setting that holds the details about the cdmDatabase connection for data extraction — createDatabaseDetails","text":"function simply stores settings communicating cdmDatabase extracting target cohort outcomes","code":""},{"path":[]},{"path":"/reference/createDatabaseSchemaSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"function specifies results schema lets pick different schema cohorts databases","code":""},{"path":"/reference/createDatabaseSchemaSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"","code":"createDatabaseSchemaSettings(   resultSchema = \"main\",   tablePrefix = \"\",   targetDialect = \"sqlite\",   tempEmulationSchema = getOption(\"sqlRenderTempEmulationSchema\"),   cohortDefinitionSchema = resultSchema,   tablePrefixCohortDefinitionTables = tablePrefix,   databaseDefinitionSchema = resultSchema,   tablePrefixDatabaseDefinitionTables = tablePrefix )"},{"path":"/reference/createDatabaseSchemaSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"resultSchema (string) name database schema result tables. tablePrefix (string) string appends PatientLevelPrediction result tables targetDialect (string) database management system used tempEmulationSchema (string) temp schema used database management system oracle cohortDefinitionSchema (string) name database schema cohort definition tables (defaults resultSchema). tablePrefixCohortDefinitionTables (string) string appends cohort definition tables databaseDefinitionSchema (string) name database schema database definition tables (defaults resultSchema). tablePrefixDatabaseDefinitionTables (string) string appends database definition tables","code":""},{"path":"/reference/createDatabaseSchemaSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"Returns list class 'plpDatabaseResultSchema' database settings","code":""},{"path":"/reference/createDatabaseSchemaSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"function can used specify database settings used upload PatientLevelPrediction results database","code":""},{"path":"/reference/createDatabaseSchemaSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the PatientLevelPrediction database result schema settings — createDatabaseSchemaSettings","text":"","code":"createDatabaseSchemaSettings(resultSchema = \"cdm\",                              tablePrefix = \"plp_\") #> $resultSchema #> [1] \"cdm\" #>  #> $tablePrefix #> [1] \"PLP_\" #>  #> $targetDialect #> [1] \"sqlite\" #>  #> $tempEmulationSchema #> NULL #>  #> $cohortDefinitionSchema #> [1] \"cdm\" #>  #> $tablePrefixCohortDefinitionTables #> [1] \"PLP_\" #>  #> $databaseDefinitionSchema #> [1] \"cdm\" #>  #> $tablePrefixDatabaseDefinitionTables #> [1] \"PLP_\" #>  #> attr(,\"class\") #> [1] \"plpDatabaseResultSchema\""},{"path":"/reference/createDefaultExecuteSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","title":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","text":"Creates default list settings specifying parts runPlp execute","code":""},{"path":"/reference/createDefaultExecuteSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","text":"","code":"createDefaultExecuteSettings()"},{"path":"/reference/createDefaultExecuteSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","text":"list TRUE split, preprocess, model development covariate summary","code":""},{"path":"/reference/createDefaultExecuteSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","text":"runs split, preprocess, model development covariate summary","code":""},{"path":"/reference/createDefaultExecuteSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates default list of settings specifying what parts of runPlp to execute — createDefaultExecuteSettings","text":"","code":"createDefaultExecuteSettings() #> $runSplitData #> [1] TRUE #>  #> $runSampleData #> [1] FALSE #>  #> $runFeatureEngineering #> [1] FALSE #>  #> $runPreprocessData #> [1] TRUE #>  #> $runModelDevelopment #> [1] TRUE #>  #> $runCovariateSummary #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"executeSettings\""},{"path":"/reference/createDefaultSplitSetting.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"Create settings defining plpData split test/validation/train sets using default splitting functions (either random stratified outcome, time subject splitting)","code":""},{"path":"/reference/createDefaultSplitSetting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"","code":"createDefaultSplitSetting(   testFraction = 0.25,   trainFraction = 0.75,   splitSeed = sample(1e+05, 1),   nfold = 3,   type = \"stratified\" )"},{"path":"/reference/createDefaultSplitSetting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"testFraction (numeric) real number 0 1 indicating test set fraction data trainFraction (numeric) real number 0 1 indicating train set fraction data. set train equal 1 - test splitSeed (numeric) seed use splitting data reproducibility (set random number generated) nfold (numeric) integer > 1 specifying number folds used cross validation type (character) Choice : 'stratified' data point randomly assigned test train fold set done stratified outcome rate consistent partition 'time' Older data assigned training set newer data assigned test set 'subject' Data partitioned subject, subject data , data points subject assigned either test data train data ().","code":""},{"path":"/reference/createDefaultSplitSetting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"object class splitSettings","code":""},{"path":"/reference/createDefaultSplitSetting.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"Returns object class splitSettings specifies splitting function called settings","code":""},{"path":"/reference/createDefaultSplitSetting.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for defining how the plpData are split into test/validation/train sets using default splitting functions (either random stratified by outcome, time or subject splitting) — createDefaultSplitSetting","text":"","code":"createDefaultSplitSetting(testFraction=0.25, trainFraction=0.75, nfold=3,                           splitSeed=42) #> $test #> [1] 0.25 #>  #> $train #> [1] 0.75 #>  #> $seed #> [1] 42 #>  #> $nfold #> [1] 3 #>  #> attr(,\"fun\") #> [1] \"randomSplitter\" #> attr(,\"class\") #> [1] \"splitSettings\""},{"path":"/reference/createExecuteSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"Creates list settings specifying parts runPlp execute","code":""},{"path":"/reference/createExecuteSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"","code":"createExecuteSettings(   runSplitData = FALSE,   runSampleData = FALSE,   runFeatureEngineering = FALSE,   runPreprocessData = FALSE,   runModelDevelopment = FALSE,   runCovariateSummary = FALSE )"},{"path":"/reference/createExecuteSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"runSplitData TRUE FALSE whether split data train/test runSampleData TRUE FALSE whether sample runFeatureEngineering TRUE FALSE whether feature engineering runPreprocessData TRUE FALSE whether preprocessing runModelDevelopment TRUE FALSE whether develop model runCovariateSummary TRUE FALSE whether create covariate summary","code":""},{"path":"/reference/createExecuteSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"list TRUE/FALSE part runPlp","code":""},{"path":"/reference/createExecuteSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"define parts runPlp execute","code":""},{"path":"/reference/createExecuteSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates list of settings specifying what parts of runPlp to execute — createExecuteSettings","text":"","code":"# create settings with only split and model development createExecuteSettings(runSplitData = TRUE, runModelDevelopment = TRUE)  #> $runSplitData #> [1] TRUE #>  #> $runSampleData #> [1] FALSE #>  #> $runFeatureEngineering #> [1] FALSE #>  #> $runPreprocessData #> [1] FALSE #>  #> $runModelDevelopment #> [1] TRUE #>  #> $runCovariateSummary #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"executeSettings\""},{"path":"/reference/createExistingSplitSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","title":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","text":"Create settings defining plpData split test/validation/train sets using existing split - good use reproducing results different run","code":""},{"path":"/reference/createExistingSplitSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","text":"","code":"createExistingSplitSettings(splitIds)"},{"path":"/reference/createExistingSplitSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","text":"splitIds (data.frame) data frame rowId index columns type integer/numeric. Index -1 test set, positive integer train set folds","code":""},{"path":"/reference/createExistingSplitSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","text":"object class splitSettings","code":""},{"path":"/reference/createExistingSplitSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for defining how the plpData are split into test/validation/train sets using an existing split - good to use for reproducing results from a different run — createExistingSplitSettings","text":"","code":"# rowId 1 is in fold 1, rowId 2 is in fold 2, rowId 3 is in the test set # rowId 4 is in fold 1, rowId 5 is in fold 2 createExistingSplitSettings(splitIds = data.frame(rowId = c(1, 2, 3, 4, 5),                                                   index = c(1, 2, -1, 1, 2))) #> $splitIds #>   rowId index #> 1     1     1 #> 2     2     2 #> 3     3    -1 #> 4     4     1 #> 5     5     2 #>  #> attr(,\"fun\") #> [1] \"existingSplitter\" #> attr(,\"class\") #> [1] \"splitSettings\""},{"path":"/reference/createFeatureEngineeringSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"Create settings defining feature engineering done","code":""},{"path":"/reference/createFeatureEngineeringSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"","code":"createFeatureEngineeringSettings(type = \"none\")"},{"path":"/reference/createFeatureEngineeringSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"type (character) Choice : 'none' feature engineering - default","code":""},{"path":"/reference/createFeatureEngineeringSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"object class featureEngineeringSettings","code":""},{"path":"/reference/createFeatureEngineeringSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"Returns object class featureEngineeringSettings specifies sampling function called settings","code":""},{"path":"/reference/createFeatureEngineeringSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for defining any feature engineering that will be done — createFeatureEngineeringSettings","text":"","code":"createFeatureEngineeringSettings(type = \"none\") #> list() #> attr(,\"fun\") #> [1] \"sameData\" #> attr(,\"class\") #> [1] \"featureEngineeringSettings\""},{"path":"/reference/createGlmModel.html","id":null,"dir":"Reference","previous_headings":"","what":"createGlmModel — createGlmModel","title":"createGlmModel — createGlmModel","text":"Create generalized linear model can used PatientLevelPrediction package.","code":""},{"path":"/reference/createGlmModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createGlmModel — createGlmModel","text":"","code":"createGlmModel(   coefficients,   intercept = 0,   mapping = \"logistic\",   targetId = NULL,   outcomeId = NULL,   populationSettings = createStudyPopulationSettings(),   restrictPlpDataSettings = createRestrictPlpDataSettings(),   covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),   featureEngineering = NULL,   tidyCovariates = NULL,   requireDenseMatrix = FALSE )"},{"path":"/reference/createGlmModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createGlmModel — createGlmModel","text":"coefficients dataframe containing two columns, coefficients covariateId, type numeric. covariateId column must contain valid covariateIds match used FeatureExtraction package. intercept numeric value representing intercept model. mapping string representing mapping linear predictors outcome probabilities. generalized linear models inverse link function. Supported values \"logistic\" logistic regression model moment. targetId Add development targetId outcomeId Add development outcomeId populationSettings Add development population settings (includes time--risk settings). restrictPlpDataSettings Add development restriction settings covariateSettings Add covariate settings specify model covariates created OMOP CDM featureEngineering Add feature engineering (e.g., need modify covariates applying model) list lists containing string named funct specifying engineering function call settings inputs function. funct must take input trainData (plpData object) settings (list). tidyCovariates Add tidyCovariates mappings (e.g., need normalize covariates) requireDenseMatrix Specify whether model needs dense matrix (TRUE FALSE)","code":""},{"path":"/reference/createGlmModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createGlmModel — createGlmModel","text":"model object containing model (Coefficients intercept) prediction function.","code":""},{"path":"/reference/createGlmModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createGlmModel — createGlmModel","text":"","code":"coefficients <- data.frame(   covariateId = c(1002),   coefficient = c(0.05)) model <- createGlmModel(coefficients, intercept = -2.5) data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=50) #> Generating covariates #> Generating cohorts #> Generating outcomes prediction <- predictPlp(model, plpData, plpData$cohorts) #> did FE #> did tidy #> predict risk probabilities using predictGlm #> Prediction took 0.0907 secs #> Prediction done in: 0.0927 secs # see the predicted risk values prediction$value #>  [1] 0.3775407 0.3318122 0.3775407 0.3543437 0.3658644 0.2994329 0.3429895 #>  [8] 0.3893608 0.3893608 0.3775407 0.4013123 0.3775407 0.3893608 0.3893608 #> [15] 0.3100255 0.3543437 0.4255575 0.3429895 0.3893608 0.3208213 0.4255575 #> [22] 0.3208213 0.3429895 0.3543437 0.3893608 0.3543437 0.3429895 0.3543437 #> [29] 0.3775407 0.3775407 0.3893608 0.3893608 0.3775407 0.3658644 0.3775407 #> [36] 0.3775407 0.3429895 0.3543437 0.3658644 0.3893608 0.3318122 0.3318122 #> [43] 0.3775407 0.3318122 0.3318122 0.3543437 0.3100255 0.3318122 0.3429895 #> [50] 0.3775407"},{"path":"/reference/createIterativeImputer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Iterative Imputer settings — createIterativeImputer","title":"Create Iterative Imputer settings — createIterativeImputer","text":"function creates settings iterative imputer first removes features missingThreshold missing values imputes missing values iteratively using chained equations","code":""},{"path":"/reference/createIterativeImputer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Iterative Imputer settings — createIterativeImputer","text":"","code":"createIterativeImputer(   missingThreshold = 0.3,   method = \"pmm\",   methodSettings = list(pmm = list(k = 5, iterations = 5)) )"},{"path":"/reference/createIterativeImputer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Iterative Imputer settings — createIterativeImputer","text":"missingThreshold threshold missing values remove feature method method use imputation, currently \"pmm\" supported methodSettings list settings imputation method use. Currently \"pmm\" supported following settings: k: number donors use matching iterations: number iterations use imputation","code":""},{"path":"/reference/createIterativeImputer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Iterative Imputer settings — createIterativeImputer","text":"settings iterative imputer class featureEngineeringSettings","code":""},{"path":"/reference/createIterativeImputer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Iterative Imputer settings — createIterativeImputer","text":"","code":"# create imputer to impute values with missingness less than 30% using  # predictive mean matching in 5 iterations with 5 donors createIterativeImputer(missingThreshold = 0.3, method = \"pmm\",                        methodSettings = list(pmm = list(k = 5, iterations = 5))) #> $missingThreshold #> [1] 0.3 #>  #> $method #> [1] \"pmm\" #>  #> $methodSettings #> $methodSettings$k #> [1] 5 #>  #> $methodSettings$iterations #> [1] 5 #>  #>  #> attr(,\"fun\") #> [1] \"iterativeImpute\" #> attr(,\"class\") #> [1] \"featureEngineeringSettings\""},{"path":"/reference/createLearningCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"createLearningCurve — createLearningCurve","title":"createLearningCurve — createLearningCurve","text":"Creates learning curve object, can plotted using plotLearningCurve() function.","code":""},{"path":"/reference/createLearningCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createLearningCurve — createLearningCurve","text":"","code":"createLearningCurve(   plpData,   outcomeId,   parallel = TRUE,   cores = 4,   modelSettings,   saveDirectory = NULL,   analysisId = \"learningCurve\",   populationSettings = createStudyPopulationSettings(),   splitSettings = createDefaultSplitSetting(),   trainFractions = c(0.25, 0.5, 0.75),   trainEvents = NULL,   sampleSettings = createSampleSettings(),   featureEngineeringSettings = createFeatureEngineeringSettings(),   preprocessSettings = createPreprocessSettings(minFraction = 0.001, normalize = TRUE),   logSettings = createLogSettings(),   executeSettings = createExecuteSettings(runSplitData = TRUE, runSampleData = FALSE,     runFeatureEngineering = FALSE, runPreprocessData = TRUE, runModelDevelopment = TRUE,     runCovariateSummary = FALSE) )"},{"path":"/reference/createLearningCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createLearningCurve — createLearningCurve","text":"plpData object type plpData - patient level prediction data extracted CDM. outcomeId (integer) ID outcome. parallel Whether run code parallel cores number computer cores use running parallel modelSettings object class modelSettings created using one function: setLassoLogisticRegression() lasso logistic regression model setGradientBoostingMachine() gradient boosting machine setAdaBoost() ada boost model setRandomForest() random forest model setDecisionTree() decision tree model setKNN() KNN model saveDirectory path directory results saved (NULL uses working directory) analysisId (integer) Identifier analysis. used create, e.g., result folder. Default timestamp. populationSettings object type populationSettings created using createStudyPopulationSettings specifies data class labels defined addition exclusions apply plpData cohort splitSettings object type splitSettings specifies split data train/validation/test. default settings can created using createDefaultSplitSetting. trainFractions list training fractions create models . Note, providing trainEvents override input trainFractions. trainEvents Events shown determinant model performance. Therefore, recommended provide trainEvents rather trainFractions. Note, providing trainEvents override input trainFractions. format follows: c(500, 1000, 1500)  - list training events sampleSettings object type sampleSettings specifies /sampling done. default none. featureEngineeringSettings object featureEngineeringSettings specifying feature engineering learned (using train data) preprocessSettings object preprocessSettings. setting specifies minimum fraction target population must covariate included model training whether normalise covariates training logSettings object logSettings created using createLogSettings specifying logging done executeSettings object executeSettings specifying parts analysis run","code":""},{"path":"/reference/createLearningCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createLearningCurve — createLearningCurve","text":"learning curve object containing various performance measures obtained model training set fraction. can plotted using plotLearningCurve.","code":""},{"path":[]},{"path":"/reference/createLogSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for logging the progression of the analysis — createLogSettings","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"Create settings logging progression analysis","code":""},{"path":"/reference/createLogSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"","code":"createLogSettings(   verbosity = \"DEBUG\",   timeStamp = TRUE,   logName = \"runPlp Log\" )"},{"path":"/reference/createLogSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"verbosity Sets level verbosity. log level higher priority logger threshold, message print. levels : DEBUG Highest verbosity showing debug statements TRACE Showing information start end steps INFO Show informative information (Default) WARN Show warning messages ERROR Show error messages FATAL silent except fatal errors timeStamp TRUE timestamp added logging statement. Automatically switched TRACE level. logName string reference logger","code":""},{"path":"/reference/createLogSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"object class logSettings containing settings logger","code":""},{"path":"/reference/createLogSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"Returns object class logSettings specifies logger settings","code":""},{"path":"/reference/createLogSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for logging the progression of the analysis — createLogSettings","text":"","code":"# create a log settings object with DENUG verbosity, timestamp and log name  # \"runPlp Log\". This needs to be passed to `runPlp`. createLogSettings(verbosity = \"DEBUG\", timeStamp = TRUE, logName = \"runPlp Log\") #> Use timeStamp: TRUE #> $verbosity #> [1] \"DEBUG\" #>  #> $timeStamp #> [1] TRUE #>  #> $logName #> [1] \"runPlp Log\" #>  #> attr(,\"class\") #> [1] \"logSettings\""},{"path":"/reference/createModelDesign.html","id":null,"dir":"Reference","previous_headings":"","what":"Specify settings for developing a single model — createModelDesign","title":"Specify settings for developing a single model — createModelDesign","text":"Specify settings developing single model","code":""},{"path":"/reference/createModelDesign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Specify settings for developing a single model — createModelDesign","text":"","code":"createModelDesign(   targetId = NULL,   outcomeId = NULL,   restrictPlpDataSettings = createRestrictPlpDataSettings(),   populationSettings = createStudyPopulationSettings(),   covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),   featureEngineeringSettings = NULL,   sampleSettings = NULL,   preprocessSettings = NULL,   modelSettings = NULL,   splitSettings = createDefaultSplitSetting(),   runCovariateSummary = TRUE )"},{"path":"/reference/createModelDesign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Specify settings for developing a single model — createModelDesign","text":"targetId id target cohort used data extraction (e.g., ATLAS id) outcomeId id outcome used data extraction (e.g., ATLAS id) restrictPlpDataSettings settings specifying extra restriction settings extracting data created using createRestrictPlpDataSettings(). populationSettings population settings specified createStudyPopulationSettings() covariateSettings covariate settings, can list single 'covariateSetting' object. featureEngineeringSettings Either NULL object class featureEngineeringSettings specifying feature engineering used model development sampleSettings Either NULL object class sampleSettings /sampling settings used model development preprocessSettings Either NULL object class preprocessSettings created using createPreprocessingSettings() modelSettings model settings setLassoLogisticRegression() splitSettings train/validation/test splitting used analyses created using createDefaultSplitSetting() runCovariateSummary Whether run covariateSummary","code":""},{"path":"/reference/createModelDesign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Specify settings for developing a single model — createModelDesign","text":"list analysis settings used develop single prediction model","code":""},{"path":"/reference/createModelDesign.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Specify settings for developing a single model — createModelDesign","text":"specifies single analysis developing single model","code":""},{"path":"/reference/createModelDesign.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Specify settings for developing a single model — createModelDesign","text":"","code":"# L1 logistic regression model to predict the outcomeId 2 using the targetId 2 # with with default population, restrictPlp, split, and covariate settings createModelDesign(   targetId = 1,   outcomeId = 2,   modelSettings = setLassoLogisticRegression(seed=42),   populationSettings = createStudyPopulationSettings(),   restrictPlpDataSettings = createRestrictPlpDataSettings(),   covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),   splitSettings = createDefaultSplitSetting(splitSeed = 42),   runCovariateSummary = TRUE ) #> $targetId #> [1] 1 #>  #> $outcomeId #> [1] 2 #>  #> $restrictPlpDataSettings #> $studyStartDate #> [1] \"\" #>  #> $studyEndDate #> [1] \"\" #>  #> $firstExposureOnly #> [1] FALSE #>  #> $washoutPeriod #> [1] 0 #>  #> $sampleSize #> NULL #>  #> attr(,\"class\") #> [1] \"restrictPlpDataSettings\" #>  #> $covariateSettings #> $temporal #> [1] FALSE #>  #> $temporalSequence #> [1] FALSE #>  #> $DemographicsGender #> [1] TRUE #>  #> $DemographicsAgeGroup #> [1] TRUE #>  #> $DemographicsRace #> [1] TRUE #>  #> $DemographicsEthnicity #> [1] TRUE #>  #> $DemographicsIndexYear #> [1] TRUE #>  #> $DemographicsIndexMonth #> [1] TRUE #>  #> $ConditionGroupEraLongTerm #> [1] TRUE #>  #> $ConditionGroupEraShortTerm #> [1] TRUE #>  #> $DrugGroupEraLongTerm #> [1] TRUE #>  #> $DrugGroupEraShortTerm #> [1] TRUE #>  #> $DrugGroupEraOverlapping #> [1] TRUE #>  #> $ProcedureOccurrenceLongTerm #> [1] TRUE #>  #> $ProcedureOccurrenceShortTerm #> [1] TRUE #>  #> $DeviceExposureLongTerm #> [1] TRUE #>  #> $DeviceExposureShortTerm #> [1] TRUE #>  #> $MeasurementLongTerm #> [1] TRUE #>  #> $MeasurementShortTerm #> [1] TRUE #>  #> $MeasurementRangeGroupLongTerm #> [1] TRUE #>  #> $MeasurementRangeGroupShortTerm #> [1] TRUE #>  #> $MeasurementValueAsConceptLongTerm #> [1] TRUE #>  #> $MeasurementValueAsConceptShortTerm #> [1] TRUE #>  #> $ObservationLongTerm #> [1] TRUE #>  #> $ObservationShortTerm #> [1] TRUE #>  #> $ObservationValueAsConceptLongTerm #> [1] TRUE #>  #> $ObservationValueAsConceptShortTerm #> [1] TRUE #>  #> $CharlsonIndex #> [1] TRUE #>  #> $Dcsi #> [1] TRUE #>  #> $Chads2 #> [1] TRUE #>  #> $Chads2Vasc #> [1] TRUE #>  #> $includedCovariateConceptIds #> logical(0) #>  #> $includedCovariateIds #> logical(0) #>  #> $addDescendantsToInclude #> [1] FALSE #>  #> $excludedCovariateConceptIds #> logical(0) #>  #> $addDescendantsToExclude #> [1] FALSE #>  #> $shortTermStartDays #> [1] -30 #>  #> $mediumTermStartDays #> [1] -180 #>  #> $endDays #> [1] 0 #>  #> $longTermStartDays #> [1] -365 #>  #> attr(,\"fun\") #> [1] \"getDbDefaultCovariateData\" #> attr(,\"class\") #> [1] \"covariateSettings\" #>  #> $populationSettings #> $binary #> [1] TRUE #>  #> $includeAllOutcomes #> [1] TRUE #>  #> $firstExposureOnly #> [1] FALSE #>  #> $washoutPeriod #> [1] 0 #>  #> $removeSubjectsWithPriorOutcome #> [1] TRUE #>  #> $priorOutcomeLookback #> [1] 99999 #>  #> $requireTimeAtRisk #> [1] TRUE #>  #> $minTimeAtRisk #> [1] 364 #>  #> $riskWindowStart #> [1] 1 #>  #> $startAnchor #> [1] \"cohort start\" #>  #> $riskWindowEnd #> [1] 365 #>  #> $endAnchor #> [1] \"cohort start\" #>  #> $restrictTarToCohortEnd #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"populationSettings\" #>  #> $sampleSettings #> $sampleSettings[[1]] #> $numberOutcomestoNonOutcomes #> [1] 1 #>  #> $sampleSeed #> [1] 1 #>  #> attr(,\"fun\") #> [1] \"sameData\" #> attr(,\"class\") #> [1] \"sampleSettings\" #>  #>  #> $featureEngineeringSettings #> $featureEngineeringSettings[[1]] #> list() #> attr(,\"fun\") #> [1] \"sameData\" #> attr(,\"class\") #> [1] \"featureEngineeringSettings\" #>  #>  #> $preprocessSettings #> $minFraction #> [1] 0.001 #>  #> $normalize #> [1] TRUE #>  #> $removeRedundancy #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"preprocessSettings\" #>  #> $modelSettings #> $fitFunction #> [1] \"fitCyclopsModel\" #>  #> $param #> $param$priorParams #> $param$priorParams$priorType #> [1] \"laplace\" #>  #> $param$priorParams$forceIntercept #> [1] FALSE #>  #> $param$priorParams$variance #> [1] 0.01 #>  #> $param$priorParams$exclude #> [1] 0 #>  #>  #> $param$includeCovariateIds #> NULL #>  #> $param$upperLimit #> [1] 20 #>  #> $param$lowerLimit #> [1] 0.01 #>  #> $param$priorCoefs #> NULL #>  #> attr(,\"settings\") #> attr(,\"settings\")$priorfunction #> [1] \"Cyclops::createPrior\" #>  #> attr(,\"settings\")$selectorType #> [1] \"byPid\" #>  #> attr(,\"settings\")$crossValidationInPrior #> [1] TRUE #>  #> attr(,\"settings\")$modelType #> [1] \"logistic\" #>  #> attr(,\"settings\")$addIntercept #> [1] TRUE #>  #> attr(,\"settings\")$useControl #> [1] TRUE #>  #> attr(,\"settings\")$seed #> [1] 42 #>  #> attr(,\"settings\")$name #> [1] \"Lasso Logistic Regression\" #>  #> attr(,\"settings\")$threads #> [1] -1 #>  #> attr(,\"settings\")$tolerance #> [1] 2e-06 #>  #> attr(,\"settings\")$cvRepetitions #> [1] 1 #>  #> attr(,\"settings\")$maxIterations #> [1] 3000 #>  #> attr(,\"modelType\") #> [1] \"binary\" #> attr(,\"saveType\") #> [1] \"RtoJson\" #>  #> attr(,\"class\") #> [1] \"modelSettings\" #>  #> $splitSettings #> $test #> [1] 0.25 #>  #> $train #> [1] 0.75 #>  #> $seed #> [1] 42 #>  #> $nfold #> [1] 3 #>  #> attr(,\"fun\") #> [1] \"randomSplitter\" #> attr(,\"class\") #> [1] \"splitSettings\" #>  #> $executeSettings #> $runSplitData #> [1] TRUE #>  #> $runSampleData #> [1] FALSE #>  #> $runFeatureEngineering #> [1] FALSE #>  #> $runPreprocessData #> [1] FALSE #>  #> $runModelDevelopment #> [1] TRUE #>  #> $runCovariateSummary #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"executeSettings\" #>  #> attr(,\"class\") #> [1] \"modelDesign\""},{"path":"/reference/createNormalizer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for normalizing the data @param type The type of normalization to use, either ","title":"Create the settings for normalizing the data @param type The type of normalization to use, either ","text":"Create settings normalizing data @param type type normalization use, either \"minmax\" \"robust\"","code":""},{"path":"/reference/createNormalizer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for normalizing the data @param type The type of normalization to use, either ","text":"","code":"createNormalizer(type = \"minmax\", settings = list())"},{"path":"/reference/createNormalizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for normalizing the data @param type The type of normalization to use, either ","text":"type type normalization use, either \"minmax\" \"robust\" settings list settings normalization. robust normalization, settings list can contain boolean value clip, clips values -3 3 normalization. See https://arxiv.org/abs/2407.04491","code":""},{"path":"/reference/createNormalizer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for normalizing the data @param type The type of normalization to use, either ","text":"object class featureEngineeringSettings object class featureEngineeringSettings'","code":""},{"path":"/reference/createNormalizer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for normalizing the data @param type The type of normalization to use, either ","text":"","code":"# create a minmax normalizer that normalizes the data between 0 and 1 normalizer <- createNormalizer(type = \"minmax\") # create a robust normalizer that normalizes the data by the interquartile range # and squeezes the values to be between -3 and 3 normalizer <- createNormalizer(type = \"robust\", settings = list(clip = TRUE))"},{"path":"/reference/createPlpResultTables.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"function executes large set SQL statements create tables can store models results","code":""},{"path":"/reference/createPlpResultTables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"","code":"createPlpResultTables(   connectionDetails,   targetDialect = \"postgresql\",   resultSchema,   deleteTables = TRUE,   createTables = TRUE,   tablePrefix = \"\",   tempEmulationSchema = getOption(\"sqlRenderTempEmulationSchema\"),   testFile = NULL )"},{"path":"/reference/createPlpResultTables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"connectionDetails database connection details targetDialect database management system used resultSchema name database schema result tables created. deleteTables true existing tables matching PatientLevelPrediction result tables names deleted createTables true PatientLevelPrediction result tables created tablePrefix string appends PatientLevelPrediction result tables tempEmulationSchema temp schema used database management system oracle testFile (used testing) location sql file table creation code","code":""},{"path":"/reference/createPlpResultTables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"Returns NULL creates deletes required tables specified database schema(s).","code":""},{"path":"/reference/createPlpResultTables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"function can used create (delete) PatientLevelPrediction result tables","code":""},{"path":"/reference/createPlpResultTables.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the results tables to store PatientLevelPrediction models and results into a database — createPlpResultTables","text":"","code":"# create a sqlite database with the PatientLevelPrediction result tables connectionDetails <- DatabaseConnector::createConnectionDetails(   dbms = \"sqlite\",   server = file.path(tempdir(), \"test.sqlite\")) createPlpResultTables(connectionDetails = connectionDetails,                       targetDialect = \"sqlite\",                       resultSchema = \"main\",                       tablePrefix = \"plp_\") #> Connecting using SQLite driver #> All or some PLP result tables do not exist, tables being recreated #> Deleting existing tables #> Creating PLP results tables #>    |                                                                               |                                                                      |   0%   |                                                                               |==                                                                    |   3%   |                                                                               |=====                                                                 |   7%   |                                                                               |=======                                                               |  10%   |                                                                               |=========                                                             |  13%   |                                                                               |============                                                          |  17%   |                                                                               |==============                                                        |  20%   |                                                                               |================                                                      |  23%   |                                                                               |===================                                                   |  27%   |                                                                               |=====================                                                 |  30%   |                                                                               |=======================                                               |  33%   |                                                                               |==========================                                            |  37%   |                                                                               |============================                                          |  40%   |                                                                               |==============================                                        |  43%   |                                                                               |=================================                                     |  47%   |                                                                               |===================================                                   |  50%   |                                                                               |=====================================                                 |  53%   |                                                                               |========================================                              |  57%   |                                                                               |==========================================                            |  60%   |                                                                               |============================================                          |  63%   |                                                                               |===============================================                       |  67%   |                                                                               |=================================================                     |  70%   |                                                                               |===================================================                   |  73%   |                                                                               |======================================================                |  77%   |                                                                               |========================================================              |  80%   |                                                                               |==========================================================            |  83%   |                                                                               |=============================================================         |  87%   |                                                                               |===============================================================       |  90%   |                                                                               |=================================================================     |  93%   |                                                                               |====================================================================  |  97%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.0291 secs #> PLP result migration being applied #> Migrating data set #> Migrator using SQL files in PatientLevelPrediction #> Connecting using SQLite driver #> Creating migrations table #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00389 secs #> Migrations table created #> Executing migration: Migration_1-store_version.sql #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00379 secs #> Saving migration: Migration_1-store_version.sql #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00448 secs #> Migration complete Migration_1-store_version.sql #> Closing database connection #> Updating version number #> Connecting using SQLite driver #>    |                                                                               |                                                                      |   0%   |                                                                               |===================================                                   |  50%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00536 secs # delete the tables createPlpResultTables(connectionDetails = connectionDetails,                       targetDialect = \"sqlite\",                       resultSchema = \"main\",                       deleteTables = TRUE,                       createTables = FALSE,                       tablePrefix = \"plp_\") #> Connecting using SQLite driver #> All or some PLP result tables do not exist, tables being recreated #> Deleting existing tables #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00459 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00428 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00462 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00323 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00396 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00499 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00265 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00279 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00272 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00315 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00267 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00279 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00271 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00274 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00339 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00356 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00344 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00344 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00339 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00359 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00363 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00356 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00362 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00379 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00363 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.0037 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00355 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00366 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00393 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00374 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00385 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00312 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.0029 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00289 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00303 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00302 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00303 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00282 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00284 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00288 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00265 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00281 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00262 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00281 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00265 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00271 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.0027 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00269 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00266 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00268 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00259 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00266 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00269 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00263 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00258 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00268 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00257 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00264 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.0026 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00259 secs #> Deleting PLP migration tables #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00265 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00275 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00259 secs #>    |                                                                               |                                                                      |   0%   |                                                                               |======================================================================| 100% #> Executing SQL took 0.00264 secs # clean up the database file unlink(file.path(tempdir(), \"test.sqlite\"))"},{"path":"/reference/createPreprocessSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for preprocessing the trainData. — createPreprocessSettings","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"Create settings preprocessing trainData.","code":""},{"path":"/reference/createPreprocessSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"","code":"createPreprocessSettings(   minFraction = 0.001,   normalize = TRUE,   removeRedundancy = TRUE )"},{"path":"/reference/createPreprocessSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"minFraction minimum fraction target population must covariate included model training normalize Whether normalise covariates training (Default: TRUE) removeRedundancy Whether remove redundant features (Default: TRUE) Redundant features features within analysisId together cover observations. example ageGroups, ageGroup 0-18 18-100 patients one groups, one groups redundant.","code":""},{"path":"/reference/createPreprocessSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"object class preprocessingSettings","code":""},{"path":"/reference/createPreprocessSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"Returns object class preprocessingSettings specifies preprocess training data","code":""},{"path":"/reference/createPreprocessSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for preprocessing the trainData. — createPreprocessSettings","text":"","code":"# Create the settings for preprocessing, remove no features, normalise the data createPreprocessSettings(minFraction = 0.0, normalize = TRUE, removeRedundancy = FALSE) #> $minFraction #> [1] 0 #>  #> $normalize #> [1] TRUE #>  #> $removeRedundancy #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"preprocessSettings\""},{"path":"/reference/createRandomForestFeatureSelection.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","title":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","text":"Create settings random foreat based feature selection","code":""},{"path":"/reference/createRandomForestFeatureSelection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","text":"","code":"createRandomForestFeatureSelection(ntrees = 2000, maxDepth = 17)"},{"path":"/reference/createRandomForestFeatureSelection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","text":"ntrees number tree forest maxDepth MAx depth tree","code":""},{"path":"/reference/createRandomForestFeatureSelection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","text":"object class featureEngineeringSettings","code":""},{"path":"/reference/createRandomForestFeatureSelection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for random foreat based feature selection — createRandomForestFeatureSelection","text":"Returns object class featureEngineeringSettings specifies sampling function called settings","code":""},{"path":[]},{"path":"/reference/createRareFeatureRemover.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for removing rare features — createRareFeatureRemover","title":"Create the settings for removing rare features — createRareFeatureRemover","text":"Create settings removing rare features","code":""},{"path":"/reference/createRareFeatureRemover.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for removing rare features — createRareFeatureRemover","text":"","code":"createRareFeatureRemover(threshold = 0.001)"},{"path":"/reference/createRareFeatureRemover.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for removing rare features — createRareFeatureRemover","text":"threshold minimum fraction training data must feature included","code":""},{"path":"/reference/createRareFeatureRemover.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for removing rare features — createRareFeatureRemover","text":"object class featureEngineeringSettings","code":""},{"path":[]},{"path":"/reference/createRestrictPlpDataSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"function creates settings used restrict target cohort calling getPlpData","code":""},{"path":"/reference/createRestrictPlpDataSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"","code":"createRestrictPlpDataSettings(   studyStartDate = \"\",   studyEndDate = \"\",   firstExposureOnly = FALSE,   washoutPeriod = 0,   sampleSize = NULL )"},{"path":"/reference/createRestrictPlpDataSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"studyStartDate calendar date specifying minimum date cohort index date can appear. Date format 'yyyymmdd'. studyEndDate calendar date specifying maximum date cohort index date can appear. Date format 'yyyymmdd'. Important: study end data also used truncate risk windows, meaning outcomes beyond study end date considered. firstExposureOnly first exposure per subject included? Note typically done createStudyPopulation function, can already done efficiency reasons. washoutPeriod mininum required continuous observation time prior index date person included risk cohort. Note typically done createStudyPopulation function, can already done efficiency reasons. sampleSize NULL, number people sample target cohort","code":""},{"path":"/reference/createRestrictPlpDataSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"setting object class restrictPlpDataSettings containing list settings: studyStartDate: calendar date specifying minimum date cohort index date can appear studyEndDate: calendar date specifying maximum date cohort index date can appear firstExposureOnly: first exposure per subject included washoutPeriod: mininum required continuous observation time prior index date person included risk cohort sampleSize: NULL, number people sample target cohort","code":""},{"path":"/reference/createRestrictPlpDataSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"Users need specify extra restrictions apply downloading target cohort","code":""},{"path":"/reference/createRestrictPlpDataSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createRestrictPlpDataSettings define extra restriction settings when calling getPlpData — createRestrictPlpDataSettings","text":"","code":"# restrict to 2010, first exposure only, require washout period of 365 day # and sample 1000 people createRestrictPlpDataSettings(studyStartDate = \"20100101\", studyEndDate = \"20101231\",  firstExposureOnly = TRUE, washoutPeriod = 365, sampleSize = 1000) #> $studyStartDate #> [1] \"20100101\" #>  #> $studyEndDate #> [1] \"20101231\" #>  #> $firstExposureOnly #> [1] TRUE #>  #> $washoutPeriod #> [1] 365 #>  #> $sampleSize #> [1] 1000 #>  #> attr(,\"class\") #> [1] \"restrictPlpDataSettings\""},{"path":"/reference/createSampleSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"Create settings defining trainData splitData sampled using default sample functions.","code":""},{"path":"/reference/createSampleSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"","code":"createSampleSettings(   type = \"none\",   numberOutcomestoNonOutcomes = 1,   sampleSeed = sample(10000, 1) )"},{"path":"/reference/createSampleSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"type (character) Choice : 'none' sampling applied - default 'underSample' Undersample non-outcome class make data balanced 'overSample' Oversample outcome class adding outcome multiple times numberOutcomestoNonOutcomes (numeric) numeric specifying required number outcomes per non-outcomes sampleSeed (numeric) seed use splitting data reproducibility (set random number generated)","code":""},{"path":"/reference/createSampleSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"object class sampleSettings","code":""},{"path":"/reference/createSampleSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"Returns object class sampleSettings specifies sampling function called settings","code":""},{"path":"/reference/createSampleSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for defining how the trainData from splitData are sampled using default sample functions. — createSampleSettings","text":"","code":"# \\donttest{ # sample even rate of outcomes to non-outcomes sampleSetting <- createSampleSettings(   type = \"underSample\",   numberOutcomestoNonOutcomes = 1,   sampleSeed = 42 ) # }"},{"path":"/reference/createSimpleImputer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Simple Imputer settings — createSimpleImputer","title":"Create Simple Imputer settings — createSimpleImputer","text":"function creates settings simple imputer imputes missing values mean median","code":""},{"path":"/reference/createSimpleImputer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Simple Imputer settings — createSimpleImputer","text":"","code":"createSimpleImputer(method = \"mean\", missingThreshold = 0.3)"},{"path":"/reference/createSimpleImputer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Simple Imputer settings — createSimpleImputer","text":"method method use imputation, either \"mean\" \"median\" missingThreshold threshold missing values imputed vs removed","code":""},{"path":"/reference/createSimpleImputer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Simple Imputer settings — createSimpleImputer","text":"settings single imputer class featureEngineeringSettings","code":""},{"path":"/reference/createSimpleImputer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Simple Imputer settings — createSimpleImputer","text":"","code":"# create imputer to impute values with missingness less than 10% using the median # of observed values createSimpleImputer(method = \"median\", missingThreshold = 0.10) #> $method #> [1] \"median\" #>  #> $missingThreshold #> [1] 0.1 #>  #> attr(,\"fun\") #> [1] \"simpleImpute\" #> attr(,\"class\") #> [1] \"featureEngineeringSettings\""},{"path":"/reference/createSklearnModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","title":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","text":"Plug existing scikit learn python model PLP framework","code":""},{"path":"/reference/createSklearnModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","text":"","code":"createSklearnModel(   modelLocation = \"/model\",   covariateMap = data.frame(columnId = 1:2, covariateId = c(1, 2), ),   isPickle = TRUE,   targetId = NULL,   outcomeId = NULL,   populationSettings = createStudyPopulationSettings(),   restrictPlpDataSettings = createRestrictPlpDataSettings(),   covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),   featureEngineering = NULL,   tidyCovariates = NULL,   requireDenseMatrix = FALSE )"},{"path":"/reference/createSklearnModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","text":"modelLocation location folder contains model model.pkl covariateMap data.frame columns: columnId covariateId. covariateId FeatureExtraction standard OHDSI covariateId. columnId column location model expects covariate . example, column called 'age' model 3rd column fitting model, values columnId 3, covariateId 1002 (covariateId age years) isPickle model saved pickle set TRUE saved json set FALSE. targetId Add development targetId outcomeId Add development outcomeId populationSettings Add development population settings (includes time--risk settings). restrictPlpDataSettings Add development restriction settings covariateSettings Add covariate settings specify model covariates created OMOP CDM featureEngineering Add feature engineering (e.g., need modify covariates applying model) list lists containing string named funct specifying engineering function call settings inputs function. funct must take input trainData (plpData object) settings (list). tidyCovariates Add tidyCovariates mappings (e.g., need normalize covariates) requireDenseMatrix Specify whether model needs dense matrix (TRUE FALSE)","code":""},{"path":"/reference/createSklearnModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","text":"object class plpModel, list contains: model (location model.pkl), preprocessing (settings mapping covariateIds model column mames), modelDesign (specification model design), trainDetails (information model fitting) covariateImportance. can use output input PatientLevelPrediction::predictPlp apply model calculate risk patients.","code":""},{"path":"/reference/createSklearnModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plug an existing scikit learn python model into the PLP framework — createSklearnModel","text":"function lets users add existing scikit learn model saved model.pkl PLP format.  covariateMap mapping standard covariateIds model columns. user also needs specify covariate settings population settings used determine standard PLP model design.","code":""},{"path":"/reference/createSplineSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for adding a spline for continuous variables — createSplineSettings","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"Create settings adding spline continuous variables","code":""},{"path":"/reference/createSplineSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"","code":"createSplineSettings(continousCovariateId, knots, analysisId = 683)"},{"path":"/reference/createSplineSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"continousCovariateId covariateId apply splines knots Either number knots vector split values analysisId analysisId use spline covariates","code":""},{"path":"/reference/createSplineSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"object class featureEngineeringSettings","code":""},{"path":"/reference/createSplineSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"Returns object class featureEngineeringSettings specifies sampling function called settings","code":""},{"path":"/reference/createSplineSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for adding a spline for continuous variables — createSplineSettings","text":"","code":"# create splines for age (1002) with 5 knots createSplineSettings(continousCovariateId = 1002, knots = 5, analysisId = 683) #> $continousCovariateId #> [1] 1002 #>  #> $knots #> [1] 5 #>  #> $analysisId #> [1] 683 #>  #> attr(,\"fun\") #> [1] \"splineCovariates\" #> attr(,\"class\") #> [1] \"featureEngineeringSettings\""},{"path":"/reference/createStratifiedImputationSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"Create settings using stratified imputation.","code":""},{"path":"/reference/createStratifiedImputationSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"","code":"createStratifiedImputationSettings(covariateId, ageSplits = NULL)"},{"path":"/reference/createStratifiedImputationSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"covariateId covariateId needs imputed values ageSplits vector age splits years create age groups","code":""},{"path":"/reference/createStratifiedImputationSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"object class featureEngineeringSettings","code":""},{"path":"/reference/createStratifiedImputationSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"Returns object class featureEngineeringSettings specifies stratified imputation. function splits covariate age groups fits splines covariate within age group. spline values used impute missing values.","code":""},{"path":"/reference/createStratifiedImputationSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create the settings for using stratified imputation. — createStratifiedImputationSettings","text":"","code":"# create a stratified imputation settings for covariate 1050 with age splits  # at 50 and 70 stratifiedImputationSettings <-    createStratifiedImputationSettings(covariateId = 1050, ageSplits = c(50, 70))"},{"path":"/reference/createStudyPopulation.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a study population — createStudyPopulation","title":"Create a study population — createStudyPopulation","text":"Create study population","code":""},{"path":"/reference/createStudyPopulation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a study population — createStudyPopulation","text":"","code":"createStudyPopulation(   plpData,   outcomeId = plpData$metaData$databaseDetails$outcomeIds[1],   populationSettings = createStudyPopulationSettings(),   population = NULL )"},{"path":"/reference/createStudyPopulation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a study population — createStudyPopulation","text":"plpData object type plpData generated using getplpData. outcomeId  ID outcome. populationSettings object class populationSettings created using createPopulationSettings population specified, population used starting point instead cohorts plpData object.","code":""},{"path":"/reference/createStudyPopulation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a study population — createStudyPopulation","text":"data frame specifying study population. data frame following columns: rowId unique identifier exposure subjectId person ID subject cohortStartdate index date outcomeCount number outcomes observed risk window timeAtRisk number days risk window survivalTime number days either outcome end risk window","code":""},{"path":"/reference/createStudyPopulation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a study population — createStudyPopulation","text":"Create study population enforcing certain inclusion exclusion criteria, defining risk window, determining outcomes fall inside risk window.","code":""},{"path":[]},{"path":"/reference/createStudyPopulationSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"create the study population settings — createStudyPopulationSettings","title":"create the study population settings — createStudyPopulationSettings","text":"create study population settings","code":""},{"path":"/reference/createStudyPopulationSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"create the study population settings — createStudyPopulationSettings","text":"","code":"createStudyPopulationSettings(   binary = TRUE,   includeAllOutcomes = TRUE,   firstExposureOnly = FALSE,   washoutPeriod = 0,   removeSubjectsWithPriorOutcome = TRUE,   priorOutcomeLookback = 99999,   requireTimeAtRisk = TRUE,   minTimeAtRisk = 364,   riskWindowStart = 1,   startAnchor = \"cohort start\",   riskWindowEnd = 365,   endAnchor = \"cohort start\",   restrictTarToCohortEnd = FALSE )"},{"path":"/reference/createStudyPopulationSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"create the study population settings — createStudyPopulationSettings","text":"binary Forces outcomeCount 0 1 (use binary prediction problems) includeAllOutcomes (binary) indicating whether include people outcomes observed whole risk period firstExposureOnly first exposure per subject included? Note typically done createStudyPopulation function, washoutPeriod mininum required continuous observation time prior index date person included cohort. removeSubjectsWithPriorOutcome Remove subjects outcome prior risk window start? priorOutcomeLookback many days look back identifying prior outcomes? requireTimeAtRisk subject without time risk removed? minTimeAtRisk minimum number days risk required included riskWindowStart start risk window (days) relative index date (+ days exposure addExposureDaysToStart parameter specified). startAnchor anchor point start risk window. Can \"cohort start\" \"cohort end\". riskWindowEnd end risk window (days) relative index data (+ days exposure addExposureDaysToEnd parameter specified). endAnchor anchor point end risk window. Can \"cohort start\" \"cohort end\". restrictTarToCohortEnd using survival model want time--risk end cohort end date set T","code":""},{"path":"/reference/createStudyPopulationSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"create the study population settings — createStudyPopulationSettings","text":"object type populationSettings containing settings required creating study population","code":""},{"path":"/reference/createStudyPopulationSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"create the study population settings — createStudyPopulationSettings","text":"","code":"# Create study population settings with a washout period of 30 days and a  # risk window of 1 to 90 days populationSettings <- createStudyPopulationSettings(washoutPeriod = 30,                                                      riskWindowStart = 1,                                                     riskWindowEnd = 90) #> Warning: issue: minTimeAtRisk is greater than max possible time-at-risk"},{"path":"/reference/createTempModelLoc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a temporary model location — createTempModelLoc","title":"Create a temporary model location — createTempModelLoc","text":"Create temporary model location","code":""},{"path":"/reference/createTempModelLoc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a temporary model location — createTempModelLoc","text":"","code":"createTempModelLoc()"},{"path":"/reference/createTempModelLoc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a temporary model location — createTempModelLoc","text":"string location temporary model location","code":""},{"path":"/reference/createTempModelLoc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a temporary model location — createTempModelLoc","text":"","code":"modelLoc <- createTempModelLoc() dir.exists(modelLoc) #> [1] FALSE # clean up unlink(modelLoc, recursive = TRUE)"},{"path":"/reference/createUnivariateFeatureSelection.html","id":null,"dir":"Reference","previous_headings":"","what":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","title":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","text":"Create settings defining feature selection done","code":""},{"path":"/reference/createUnivariateFeatureSelection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","text":"","code":"createUnivariateFeatureSelection(k = 100)"},{"path":"/reference/createUnivariateFeatureSelection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","text":"k function returns K features associated (univariately) outcome","code":""},{"path":"/reference/createUnivariateFeatureSelection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","text":"object class featureEngineeringSettings","code":""},{"path":"/reference/createUnivariateFeatureSelection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create the settings for defining any feature selection that will be done — createUnivariateFeatureSelection","text":"Returns object class featureEngineeringSettings specifies function called settings. Uses scikit-learn SelectKBest function chi2 univariate feature selection.","code":""},{"path":[]},{"path":"/reference/createValidationDesign.html","id":null,"dir":"Reference","previous_headings":"","what":"createValidationDesign - Define the validation design for external validation — createValidationDesign","title":"createValidationDesign - Define the validation design for external validation — createValidationDesign","text":"createValidationDesign - Define validation design external validation","code":""},{"path":"/reference/createValidationDesign.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createValidationDesign - Define the validation design for external validation — createValidationDesign","text":"","code":"createValidationDesign(   targetId,   outcomeId,   populationSettings = NULL,   restrictPlpDataSettings = NULL,   plpModelList,   recalibrate = NULL,   runCovariateSummary = TRUE )"},{"path":"/reference/createValidationDesign.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createValidationDesign - Define the validation design for external validation — createValidationDesign","text":"targetId targetId target cohort validate outcomeId outcomeId outcome cohort validate populationSettings list population restriction settings created createPopulationSettings. Default NULL taken model restrictPlpDataSettings list plpData restriction settings created createRestrictPlpDataSettings. Default NULL taken model. plpModelList list plpModels objects created runPlp path objects recalibrate vector characters specifying recalibration method apply, runCovariateSummary whether run covariate summary validation data","code":""},{"path":"/reference/createValidationDesign.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createValidationDesign - Define the validation design for external validation — createValidationDesign","text":"validation design object class validationDesign list objects","code":""},{"path":"/reference/createValidationDesign.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createValidationDesign - Define the validation design for external validation — createValidationDesign","text":"","code":"# create a validation design for targetId 1 and outcomeId 2 one l1 model and  # one gradient boosting model createValidationDesign(1, 2, plpModelList = list( \"pathToL1model\", \"PathToGBMModel\")) #> $targetId #> [1] 1 #>  #> $outcomeId #> [1] 2 #>  #> $populationSettings #> NULL #>  #> $plpModelList #> $plpModelList[[1]] #> [1] \"pathToL1model\" #>  #> $plpModelList[[2]] #> [1] \"PathToGBMModel\" #>  #>  #> $restrictPlpDataSettings #> NULL #>  #> $recalibrate #> NULL #>  #> $runCovariateSummary #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"validationDesign\""},{"path":"/reference/createValidationSettings.html","id":null,"dir":"Reference","previous_headings":"","what":"createValidationSettings define optional settings for performing external validation — createValidationSettings","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"function creates settings required externalValidatePlp","code":""},{"path":"/reference/createValidationSettings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"","code":"createValidationSettings(recalibrate = NULL, runCovariateSummary = TRUE)"},{"path":"/reference/createValidationSettings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"recalibrate vector characters specifying recalibration method apply runCovariateSummary Whether run covariate summary validation data","code":""},{"path":"/reference/createValidationSettings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"setting object class validationSettings containing list settings externalValidatePlp","code":""},{"path":"/reference/createValidationSettings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"Users need specify whether want sample recalibate performing external validation","code":""},{"path":"/reference/createValidationSettings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createValidationSettings define optional settings for performing external validation — createValidationSettings","text":"","code":"# do weak recalibration and don't run covariate summary createValidationSettings(recalibrate = \"weakRecalibration\",                           runCovariateSummary = FALSE) #> $recalibrate #> [1] \"weakRecalibration\" #>  #> $runCovariateSummary #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"validationSettings\""},{"path":"/reference/diagnoseMultiplePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a list of predictions diagnoses — diagnoseMultiplePlp","title":"Run a list of predictions diagnoses — diagnoseMultiplePlp","text":"Run list predictions diagnoses","code":""},{"path":"/reference/diagnoseMultiplePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a list of predictions diagnoses — diagnoseMultiplePlp","text":"","code":"diagnoseMultiplePlp(   databaseDetails = createDatabaseDetails(),   modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =     setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,     modelSettings = setLassoLogisticRegression())),   cohortDefinitions = NULL,   logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = TRUE, logName =     \"diagnosePlp Log\"),   saveDirectory = NULL )"},{"path":"/reference/diagnoseMultiplePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a list of predictions diagnoses — diagnoseMultiplePlp","text":"databaseDetails database settings created using createDatabaseDetails() modelDesignList list model designs created using createModelDesign() cohortDefinitions list cohort definitions target outcome cohorts logSettings setting spexcifying logging analyses created using createLogSettings() saveDirectory Name folder outputs written .","code":""},{"path":"/reference/diagnoseMultiplePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a list of predictions diagnoses — diagnoseMultiplePlp","text":"data frame following columns:","code":""},{"path":"/reference/diagnoseMultiplePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a list of predictions diagnoses — diagnoseMultiplePlp","text":"function run specified prediction design diagnoses.","code":""},{"path":"/reference/diagnosePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","title":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","text":"function runs set prediction diagnoses help pick suitable T, O, TAR determine whether prediction problem worth executing.","code":""},{"path":"/reference/diagnosePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","text":"","code":"diagnosePlp(   plpData = NULL,   outcomeId,   analysisId,   populationSettings,   splitSettings = createDefaultSplitSetting(),   sampleSettings = createSampleSettings(),   saveDirectory = NULL,   featureEngineeringSettings = createFeatureEngineeringSettings(),   modelSettings = setLassoLogisticRegression(),   logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = TRUE, logName =     \"diagnosePlp Log\"),   preprocessSettings = createPreprocessSettings() )"},{"path":"/reference/diagnosePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","text":"plpData object type plpData - patient level prediction data extracted CDM.  Can also include initial population plpData$popualtion. outcomeId (integer) ID outcome. analysisId (integer) Identifier analysis. used create, e.g., result folder. Default timestamp. populationSettings object type populationSettings created using createStudyPopulationSettings specifies data class labels defined addition exclusions apply plpData cohort splitSettings object type splitSettings specifies split data train/validation/test. default settings can created using createDefaultSplitSetting. sampleSettings object type sampleSettings specifies /sampling done. default none. saveDirectory path directory results saved (NULL uses working directory) featureEngineeringSettings object featureEngineeringSettings specifying feature engineering learned (using train data) modelSettings object class modelSettings created using one function: setLassoLogisticRegression() lasso logistic regression model setGradientBoostingMachine() gradient boosting machine setAdaBoost() ada boost model setRandomForest() random forest model setDecisionTree() decision tree model logSettings object logSettings created using createLogSettings specifying logging done preprocessSettings object preprocessSettings. setting specifies minimum fraction target population must covariate included model training whether normalise covariates training","code":""},{"path":"/reference/diagnosePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","text":"object containing model location model saved, data selection settings, preprocessing training settings well various performance measures obtained model. distribution: List O data.frame containing: ) Time observation end distribution, ii) Time observation start distribution, iii) Time event distribution iv) Time last prior event index distribution (patients T O index) incident: List O incidence O T TAR characterization: List O Characterization T, TnO, Tn~O","code":""},{"path":"/reference/diagnosePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"diagnostic - Investigates the prediction problem settings - use before training a model — diagnosePlp","text":"Users can define set Ts, Os, databases population settings.  list data.frames containing details follow-time distribution, time--event information, characteriszation details, time last prior event, observation time distribution.","code":""},{"path":[]},{"path":"/reference/evaluatePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"evaluatePlp — evaluatePlp","title":"evaluatePlp — evaluatePlp","text":"Evaluates performance patient level prediction model","code":""},{"path":"/reference/evaluatePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"evaluatePlp — evaluatePlp","text":"","code":"evaluatePlp(prediction, typeColumn = \"evaluationType\")"},{"path":"/reference/evaluatePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"evaluatePlp — evaluatePlp","text":"prediction patient level prediction model's prediction typeColumn column name prediction object used stratify evaluation","code":""},{"path":"/reference/evaluatePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"evaluatePlp — evaluatePlp","text":"object class plpEvaluation containing following components evaluationStatistics: data frame containing evaluation statistics' thresholdSummary: data frame containing threshold summary' demographicSummary: data frame containing demographic summary' calibrationSummary: data frame containing calibration summary' predictionDistribution: data frame containing prediction distribution'","code":""},{"path":"/reference/evaluatePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"evaluatePlp — evaluatePlp","text":"function calculates various metrics measure performance model","code":""},{"path":[]},{"path":"/reference/externalValidateDbPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","title":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","text":"function extracts data using user specified connection cdm_schema, applied model calcualtes performance","code":""},{"path":"/reference/externalValidateDbPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","text":"","code":"externalValidateDbPlp(   plpModel,   validationDatabaseDetails = createDatabaseDetails(),   validationRestrictPlpDataSettings = createRestrictPlpDataSettings(),   settings = createValidationSettings(recalibrate = \"weakRecalibration\"),   logSettings = createLogSettings(verbosity = \"INFO\", logName = \"validatePLP\"),   outputFolder = NULL )"},{"path":"/reference/externalValidateDbPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","text":"plpModel model object returned runPlp() containing trained model validationDatabaseDetails list objects class databaseDetails created using createDatabaseDetails validationRestrictPlpDataSettings list population restriction settings created createRestrictPlpDataSettings() settings settings object class validationSettings created using createValidationSettings logSettings object logSettings created using createLogSettings specifying logging done outputFolder directory save validation results (subfolders created per database validationDatabaseDetails)","code":""},{"path":"/reference/externalValidateDbPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","text":"externalValidatePlp object containing following components model: model object executionSummary: list execution details prediction: dataframe containing predictions performanceEvaluation: dataframe containing performance metrics covariateSummary: dataframe containing covariate summary","code":""},{"path":"/reference/externalValidateDbPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"externalValidateDbPlp - Validate a model on new databases — externalValidateDbPlp","text":"Users need input trained model (output runPlp()) new database connections. function return list length equal number cdm_schemas input performance new data","code":""},{"path":[]},{"path":"/reference/extractDatabaseToCsv.html","id":null,"dir":"Reference","previous_headings":"","what":"Exports all the results from a database into csv files — extractDatabaseToCsv","title":"Exports all the results from a database into csv files — extractDatabaseToCsv","text":"Exports results database csv files","code":""},{"path":"/reference/extractDatabaseToCsv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exports all the results from a database into csv files — extractDatabaseToCsv","text":"","code":"extractDatabaseToCsv(   conn = NULL,   connectionDetails,   databaseSchemaSettings = createDatabaseSchemaSettings(resultSchema = \"main\"),   csvFolder,   minCellCount = 5,   sensitiveColumns = getPlpSensitiveColumns(),   fileAppend = NULL )"},{"path":"/reference/extractDatabaseToCsv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exports all the results from a database into csv files — extractDatabaseToCsv","text":"conn connection database results connectionDetails connectionDetails result database databaseSchemaSettings result database schema settings csvFolder Location save csv files minCellCount min value show cells sensitive (values less value replaced -1) sensitiveColumns named list (name table columns belong ) list columns apply minCellCount . fileAppend set string appended start csv file names","code":""},{"path":"/reference/extractDatabaseToCsv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exports all the results from a database into csv files — extractDatabaseToCsv","text":"directory path results saved","code":""},{"path":"/reference/extractDatabaseToCsv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Exports all the results from a database into csv files — extractDatabaseToCsv","text":"Extracts results database set csv files","code":""},{"path":[]},{"path":"/reference/fitPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"fitPlp — fitPlp","title":"fitPlp — fitPlp","text":"Train various models using default parameter grid search user specified parameters","code":""},{"path":"/reference/fitPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fitPlp — fitPlp","text":"","code":"fitPlp(trainData, modelSettings, search = \"grid\", analysisId, analysisPath)"},{"path":"/reference/fitPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fitPlp — fitPlp","text":"trainData object type trainData created using splitData data extracted CDM. modelSettings object class modelSettings created using one createModelSettings functions search search strategy hyper-parameter selection (currently used) analysisId id analysis analysisPath path analysis","code":""},{"path":"/reference/fitPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fitPlp — fitPlp","text":"object class plpModel containing: model trained prediction model preprocessing preprocessing required applying model prediction cohort data.frame predicted risk column added modelDesign list specifiying modelDesign settings used fit model trainDetails model meta data covariateImportance covariate importance model","code":""},{"path":"/reference/fitPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"fitPlp — fitPlp","text":"user can define machine learning model train","code":""},{"path":[]},{"path":"/reference/getCalibrationSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a sparse summary of the calibration — getCalibrationSummary","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"Get sparse summary calibration","code":""},{"path":"/reference/getCalibrationSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"","code":"getCalibrationSummary(   prediction,   predictionType,   typeColumn = \"evaluation\",   numberOfStrata = 10,   truncateFraction = 0.05 )"},{"path":"/reference/getCalibrationSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"prediction prediction object generated using predict functions. predictionType type prediction (binary survival) typeColumn column used stratify results numberOfStrata number strata plot. truncateFraction fraction probability values ignored plotting, avoid x-axis scale dominated outliers.","code":""},{"path":"/reference/getCalibrationSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"dataframe calibration summary","code":""},{"path":"/reference/getCalibrationSummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"Generates sparse summary showing predicted probabilities observed fractions. Predictions stratified equally sized bins predicted probabilities.","code":""},{"path":"/reference/getCalibrationSummary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get a sparse summary of the calibration — getCalibrationSummary","text":"","code":"# simulate data data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=500) #> Generating covariates #> Generating cohorts #> Generating outcomes # create study population, split into train/test and preprocess with default settings population <- createStudyPopulation(plpData, outcomeId = 3) #> outcomeId: 3 #> binary: TRUE #> includeAllOutcomes: TRUE #> firstExposureOnly: FALSE #> washoutPeriod: 0 #> removeSubjectsWithPriorOutcome: TRUE #> priorOutcomeLookback: 99999 #> requireTimeAtRisk: TRUE #> minTimeAtRisk: 364 #> restrictTarToCohortEnd: FALSE #> riskWindowStart: 1 #> startAnchor: cohort start #> riskWindowEnd: 365 #> endAnchor: cohort start #> restrictTarToCohortEnd: FALSE #> Removing subjects with prior outcomes (if any) #> Removing non outcome subjects with insufficient time at risk (if any) #> Outcome is 0 or 1 #> Population created with: 481 observations, 481 unique subjects and 231 outcomes #> Population created in 0.0382 secs data <- splitData(plpData, population, createDefaultSplitSetting()) #> test: 0.25 #> train: 0.75 #> nfold: 3 #> seed: 2020 #> Creating a 25% test and 75% train (into 3 folds) random stratified split by class #> Data split into 119 test cases and 362 train cases (121, 121, 120) #> Starting to limit covariate data to population... #> Finished limiting covariate data to population... #> Starting to limit covariate data to population... #> Finished limiting covariate data to population... #> Data split in 0.239 secs data$Train$covariateData <- preprocessData(data$Train$covariateData) #> minFraction: 0.001 #> normalize: TRUE #> removeRedundancy: TRUE #> Removing 1 redundant covariates #> Removing 0 infrequent covariates #> Normalizing covariates #> Tidying covariates took 0.855 secs saveLoc <- file.path(tempdir(), \"calibrationSummary\") # fit a lasso logistic regression model using the training data plpModel <- fitPlp(data$Train, modelSettings=setLassoLogisticRegression(seed=42),                    analysisId=1, analysisPath=saveLoc) #> Running Cyclops #> Done. #> GLM fit status:  OK #> Returned from fitting to LassoLogisticRegression #> Getting variable importance #> Creating variable importance data frame #> Getting predictions on train set #> predictProbabilities - predictAndromeda start #> Prediction took 0.1 secs #> Returned from classifier function #> Time to fit model: 0.217 secs calibrationSummary <- getCalibrationSummary(plpModel$prediction,                                              \"binary\",                                              numberOfStrata = 10,                                             typeColumn = \"evaluationType\") calibrationSummary #>    predictionThreshold PersonCountAtRisk PersonCountWithOutcome #> 1            0.0000000                72                     28 #> 2            0.3967779                 4                      0 #> 3            0.4182838                85                     34 #> 4            0.4311129                70                     31 #> 5            0.4746809                71                     40 #> 6            0.5100570                31                     19 #> 7            0.6209443                29                     22 #> 8            0.0000000                51                     24 #> 9            0.3953152                24                     14 #> 10           0.4136041                36                     14 #> 11           0.4389705                37                     17 #> 12           0.4443968                38                     22 #> 13           0.4698104                33                     10 #> 14           0.4816173                51                     24 #> 15           0.5028033                20                      9 #> 16           0.5371844                36                     16 #> 17           0.5973235                36                     24 #>    averagePredictedProbability StDevPredictedProbability #> 1                    0.3935215               0.015228158 #> 2                    0.4182838               0.000000000 #> 3                    0.4311129               0.000000000 #> 4                    0.4746809               0.000000000 #> 5                    0.5098516               0.001730388 #> 6                    0.5965674               0.024012417 #> 7                    0.6699312               0.014078745 #> 8                    0.3874111               0.008500192 #> 9                    0.4064082               0.003716602 #> 10                   0.4346486               0.006989355 #> 11                   0.4419036               0.002741549 #> 12                   0.4662758               0.007653728 #> 13                   0.4788592               0.004643898 #> 14                   0.4968033               0.005687440 #> 15                   0.5261732               0.007376023 #> 16                   0.5670503               0.022460997 #> 17                   0.6449116               0.025450689 #>    MinPredictedProbability P25PredictedProbability MedianPredictedProbability #> 1                0.3123910               0.3967779                  0.3967779 #> 2                0.4182838               0.4182838                  0.4182838 #> 3                0.4311129               0.4311129                  0.4311129 #> 4                0.4746809               0.4746809                  0.4746809 #> 5                0.4954765               0.5100570                  0.5100570 #> 6                0.5308364               0.5870932                  0.5870932 #> 7                0.6613927               0.6613927                  0.6613927 #> 8                0.3546579               0.3837830                  0.3843002 #> 9                0.3968635               0.4053957                  0.4053957 #> 10               0.4219420               0.4357438                  0.4389705 #> 11               0.4389705               0.4389705                  0.4443968 #> 12               0.4458033               0.4698104                  0.4698104 #> 13               0.4698104               0.4728699                  0.4816173 #> 14               0.4875726               0.4923061                  0.4955607 #> 15               0.5140176               0.5227849                  0.5263849 #> 16               0.5434985               0.5434985                  0.5754406 #> 17               0.6104719               0.6227571                  0.6507883 #>    P75PredictedProbability MaxPredictedProbability observedIncidence evaluation #> 1                0.3967779               0.3967779         0.3888889      Train #> 2                0.4182838               0.4182838         0.0000000      Train #> 3                0.4311129               0.4311129         0.4000000      Train #> 4                0.4746809               0.4746809         0.4428571      Train #> 5                0.5100570               0.5100570         0.5633803      Train #> 6                0.6209443               0.6209443         0.6129032      Train #> 7                0.6923446               0.6923446         0.7586207      Train #> 8                0.3953152               0.3953152         0.4705882         CV #> 9                0.4053957               0.4136041         0.5833333         CV #> 10               0.4389705               0.4389705         0.3888889         CV #> 11               0.4443968               0.4443968         0.4594595         CV #> 12               0.4698104               0.4698104         0.5789474         CV #> 13               0.4816173               0.4816173         0.3030303         CV #> 14               0.5028033               0.5028033         0.4705882         CV #> 15               0.5347731               0.5371844         0.4500000         CV #> 16               0.5852895               0.5973235         0.4444444         CV #> 17               0.6552410               0.7147491         0.6666667         CV # clean up unlink(saveLoc, recursive = TRUE)"},{"path":"/reference/getCohortCovariateData.html","id":null,"dir":"Reference","previous_headings":"","what":"Extracts covariates based on cohorts — getCohortCovariateData","title":"Extracts covariates based on cohorts — getCohortCovariateData","text":"Extracts covariates based cohorts","code":""},{"path":"/reference/getCohortCovariateData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extracts covariates based on cohorts — getCohortCovariateData","text":"","code":"getCohortCovariateData(   connection,   tempEmulationSchema = NULL,   oracleTempSchema = NULL,   cdmDatabaseSchema,   cdmVersion = \"5\",   cohortTable = \"#cohort_person\",   rowIdField = \"row_id\",   aggregated,   cohortIds,   covariateSettings,   ... )"},{"path":"/reference/getCohortCovariateData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extracts covariates based on cohorts — getCohortCovariateData","text":"connection database connection tempEmulationSchema schema use temp tables oracleTempSchema DEPRECATED temp schema using oracle cdmDatabaseSchema schema OMOP CDM data cdmVersion version OMOP CDM data cohortTable table name contains target population cohort rowIdField string representing unique identifier target population cohort aggregated whether covariate aggregated cohortIds cohort id target cohort covariateSettings settings covariate cohorts time periods ... additional arguments FeatureExtraction","code":""},{"path":"/reference/getCohortCovariateData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extracts covariates based on cohorts — getCohortCovariateData","text":"CovariateData object covariates, covariateRef, analysisRef tables","code":""},{"path":"/reference/getCohortCovariateData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extracts covariates based on cohorts — getCohortCovariateData","text":"user specifies cohort time period covariate constructed whether cohort time periods relative target population cohort index","code":""},{"path":[]},{"path":"/reference/getDemographicSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a demographic summary — getDemographicSummary","title":"Get a demographic summary — getDemographicSummary","text":"Get demographic summary","code":""},{"path":"/reference/getDemographicSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a demographic summary — getDemographicSummary","text":"","code":"getDemographicSummary(prediction, predictionType, typeColumn = \"evaluation\")"},{"path":"/reference/getDemographicSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a demographic summary — getDemographicSummary","text":"prediction prediction object predictionType type prediction (binary survival) typeColumn column used stratify results","code":""},{"path":"/reference/getDemographicSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get a demographic summary — getDemographicSummary","text":"dataframe demographic summary","code":""},{"path":"/reference/getDemographicSummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get a demographic summary — getDemographicSummary","text":"Generates data.frame prediction summary per 5 year age group gender group","code":""},{"path":[]},{"path":"/reference/getEunomiaPlpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a plpData object from the Eunomia database' — getEunomiaPlpData","title":"Create a plpData object from the Eunomia database' — getEunomiaPlpData","text":"function creates plpData object Eunomia database. gets connection details, creates cohorts, extracts data. cohort predicting GIbleed new users celecoxib.","code":""},{"path":"/reference/getEunomiaPlpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a plpData object from the Eunomia database' — getEunomiaPlpData","text":"","code":"getEunomiaPlpData(covariateSettings = NULL)"},{"path":"/reference/getEunomiaPlpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a plpData object from the Eunomia database' — getEunomiaPlpData","text":"covariateSettings list covariateSettings objects created using createCovariateSettings function FeatureExtraction package. nothing specified covariates age, gender, conditions drug era used.","code":""},{"path":"/reference/getEunomiaPlpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a plpData object from the Eunomia database' — getEunomiaPlpData","text":"object type plpData, containing information cohorts, outcomes, baseline covariates. Information multiple outcomes can captured efficiency reasons. object list following components: outcomes data frame listing outcomes per person, including time event, outcome id cohorts data frame listing persons cohort, listing exposure status well time end observation period time end cohort covariateData Andromeda object created FeatureExtraction package. object contains following items: covariates Andromeda table listing covariates per person two cohorts. done using sparse representation: covariates value 0 omitted save space. Usually three columns, rowId, covariateId covariateValue'. covariateRef Andromeda table describing covariates extracted. AnalysisRef Andromeda table information analysisIds 'FeatureExtraction' used.","code":""},{"path":[]},{"path":"/reference/getPlpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the patient level prediction data from the server — getPlpData","title":"Extract the patient level prediction data from the server — getPlpData","text":"function executes large set SQL statements database OMOP CDM format extract data needed perform analysis.","code":""},{"path":"/reference/getPlpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the patient level prediction data from the server — getPlpData","text":"","code":"getPlpData(databaseDetails, covariateSettings, restrictPlpDataSettings = NULL)"},{"path":"/reference/getPlpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the patient level prediction data from the server — getPlpData","text":"databaseDetails cdm database details created using createDatabaseDetails() covariateSettings object type covariateSettings list objects created using createCovariateSettings function FeatureExtraction package. restrictPlpDataSettings Extra settings apply target population extracting data. Created using createRestrictPlpDataSettings(). optional.","code":""},{"path":"/reference/getPlpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the patient level prediction data from the server — getPlpData","text":"'r plpDataObjectDoc()`","code":""},{"path":"/reference/getPlpData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract the patient level prediction data from the server — getPlpData","text":"Based arguments, risk cohort data retrieved, well outcomes occurring subjects. risk cohort identified  user-defined cohorts cohort table either inside CDM instance separate schema. Similarly, outcomes identified user-defined cohorts cohort table either inside CDM instance separate schema. Covariates automatically extracted appropriate tables within CDM. wish exclude concepts covariates need manually add concept_ids descendants excludedCovariateConceptIds covariateSettings argument.","code":""},{"path":[]},{"path":"/reference/getPredictionDistribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the prediction distribution — getPredictionDistribution","title":"Calculates the prediction distribution — getPredictionDistribution","text":"Calculates prediction distribution","code":""},{"path":"/reference/getPredictionDistribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the prediction distribution — getPredictionDistribution","text":"","code":"getPredictionDistribution(   prediction,   predictionType = \"binary\",   typeColumn = \"evaluation\" )"},{"path":"/reference/getPredictionDistribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the prediction distribution — getPredictionDistribution","text":"prediction prediction object predictionType type prediction (binary survival) typeColumn column used stratify results","code":""},{"path":"/reference/getPredictionDistribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates the prediction distribution — getPredictionDistribution","text":"0.00, 0.1, 0.25, 0.5, 0.75, 0.9, 1.00 quantile pf prediction, mean standard deviation per class","code":""},{"path":"/reference/getPredictionDistribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates the prediction distribution — getPredictionDistribution","text":"Calculates quantiles predition object","code":""},{"path":"/reference/getPredictionDistribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculates the prediction distribution — getPredictionDistribution","text":"","code":"prediction <- data.frame(rowId = 1:100,                           outcomeCount = stats::rbinom(1:100, 1, prob=0.5),                           value = runif(100),                           evaluation = rep(\"Train\", 100)) getPredictionDistribution(prediction) #>   class PersonCount averagePredictedProbability StDevPredictedProbability #> 1     0          59                   0.4573650                 0.3173900 #> 2     1          41                   0.4679611                 0.2767069 #>   MinPredictedProbability P05PredictedProbability P25PredictedProbability #> 1            0.0038786188              0.05016270               0.1871836 #> 2            0.0008937623              0.02819794               0.3078777 #>   MedianPredictedProbability P75PredictedProbability P95PredictedProbability #> 1                  0.4287375               0.7820303               0.9386516 #> 2                  0.4382716               0.6981910               0.9258297 #>   MaxPredictedProbability evaluation #> 1               0.9891444      Train #> 2               0.9465100      Train"},{"path":"/reference/getPredictionDistribution_binary.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculates the prediction distribution — getPredictionDistribution_binary","title":"Calculates the prediction distribution — getPredictionDistribution_binary","text":"Calculates prediction distribution","code":""},{"path":"/reference/getPredictionDistribution_binary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculates the prediction distribution — getPredictionDistribution_binary","text":"","code":"getPredictionDistribution_binary(prediction, evalColumn, ...)"},{"path":"/reference/getPredictionDistribution_binary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculates the prediction distribution — getPredictionDistribution_binary","text":"prediction prediction object evalColumn column used stratify results ... inputs","code":""},{"path":"/reference/getPredictionDistribution_binary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculates the prediction distribution — getPredictionDistribution_binary","text":"0.00, 0.1, 0.25, 0.5, 0.75, 0.9, 1.00 quantile pf prediction, mean standard deviation per class","code":""},{"path":"/reference/getPredictionDistribution_binary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculates the prediction distribution — getPredictionDistribution_binary","text":"Calculates quantiles predition object","code":""},{"path":"/reference/getThresholdSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate all measures for sparse ROC — getThresholdSummary","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"Calculate measures sparse ROC","code":""},{"path":"/reference/getThresholdSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"","code":"getThresholdSummary(   prediction,   predictionType = \"binary\",   typeColumn = \"evaluation\" )"},{"path":"/reference/getThresholdSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"prediction prediction object predictionType type prediction (binary survival) typeColumn column used stratify results","code":""},{"path":"/reference/getThresholdSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"data.frame TP, FP, TN, FN, TPR, FPR, accuracy, PPF, Fmeasure","code":""},{"path":"/reference/getThresholdSummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"Calculates TP, FP, TN, FN, TPR, FPR, accuracy, PPF, Fmeasure prediction object","code":""},{"path":"/reference/getThresholdSummary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate all measures for sparse ROC — getThresholdSummary","text":"","code":"prediction <- data.frame(rowId = 1:100,                           outcomeCount = stats::rbinom(1:100, 1, prob=0.5),                          value = runif(100),                           evaluation = rep(\"Train\", 100)) summary <- getThresholdSummary(prediction) str(summary) #> 'data.frame':\t100 obs. of  24 variables: #>  $ evaluation             : chr  \"Train\" \"Train\" \"Train\" \"Train\" ... #>  $ predictionThreshold    : num  0.993 0.993 0.986 0.982 0.973 ... #>  $ preferenceThreshold    : num  0.994 0.993 0.988 0.984 0.976 ... #>  $ positiveCount          : num  1 2 3 4 5 6 7 8 9 10 ... #>  $ negativeCount          : num  99 98 97 96 95 94 93 92 91 90 ... #>  $ trueCount              : num  47 47 47 47 47 47 47 47 47 47 ... #>  $ falseCount             : num  53 53 53 53 53 53 53 53 53 53 ... #>  $ truePositiveCount      : num  0 1 1 1 2 3 3 4 4 5 ... #>  $ trueNegativeCount      : num  52 52 51 50 50 50 49 49 48 48 ... #>  $ falsePositiveCount     : num  1 1 2 3 3 3 4 4 5 5 ... #>  $ falseNegativeCount     : num  47 46 46 46 45 44 44 43 43 42 ... #>  $ f1Score                : num  NaN 0.0408 0.04 0.0392 0.0769 ... #>  $ accuracy               : num  0.52 0.53 0.52 0.51 0.52 0.53 0.52 0.53 0.52 0.53 ... #>  $ sensitivity            : num  0 0.0213 0.0213 0.0213 0.0426 ... #>  $ falseNegativeRate      : num  1 0.979 0.979 0.979 0.957 ... #>  $ falsePositiveRate      : num  0.0189 0.0189 0.0377 0.0566 0.0566 ... #>  $ specificity            : num  0.981 0.981 0.962 0.943 0.943 ... #>  $ positivePredictiveValue: num  0 0.5 0.333 0.25 0.4 ... #>  $ falseDiscoveryRate     : num  1 0.5 0.667 0.75 0.6 ... #>  $ negativePredictiveValue: num  0.525 0.531 0.526 0.521 0.526 ... #>  $ falseOmissionRate      : num  0.475 0.469 0.474 0.479 0.474 ... #>  $ positiveLikelihoodRatio: num  0 1.128 0.564 0.376 0.752 ... #>  $ negativeLikelihoodRatio: num  1.019 0.998 1.017 1.037 1.015 ... #>  $ diagnosticOddsRatio    : num  0 1.13 0.554 0.362 0.741 ..."},{"path":"/reference/ici.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"Calculate Integrated Calibration Index Austin Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281","code":""},{"path":"/reference/ici.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"","code":"ici(prediction)"},{"path":"/reference/ici.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"prediction prediction object found plpResult object","code":""},{"path":"/reference/ici.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"Integrated Calibration Index value NULL calculation fails","code":""},{"path":"/reference/ici.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"Calculate Integrated Calibration Index","code":""},{"path":"/reference/ici.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Integrated Calibration Index from Austin and Steyerberg https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281 — ici","text":"","code":"prediction <- data.frame(rowId = 1:100,                          outcomeCount = stats::rbinom(1:100, 1, prob=0.5),                         value = runif(100),                          evaluation = rep(\"Train\", 100)) ici(prediction) #> [1] 0.2786735"},{"path":"/reference/insertCsvToDatabase.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to insert results into a database from csvs — insertCsvToDatabase","title":"Function to insert results into a database from csvs — insertCsvToDatabase","text":"function converts folder csv results plp objects loads plp result database","code":""},{"path":"/reference/insertCsvToDatabase.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to insert results into a database from csvs — insertCsvToDatabase","text":"","code":"insertCsvToDatabase(   csvFolder,   connectionDetails,   databaseSchemaSettings,   modelSaveLocation,   csvTableAppend = \"\" )"},{"path":"/reference/insertCsvToDatabase.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to insert results into a database from csvs — insertCsvToDatabase","text":"csvFolder location csv folder plp results connectionDetails connection details plp results database csv results inserted databaseSchemaSettings object created createDatabaseSchemaSettings settings specifying result tables insert csv results modelSaveLocation location save models csv folder - location picked inserting models database csvTableAppend string appends csv file names","code":""},{"path":"/reference/insertCsvToDatabase.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to insert results into a database from csvs — insertCsvToDatabase","text":"Returns data.frame indicating whether results inported database","code":""},{"path":"/reference/insertCsvToDatabase.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Function to insert results into a database from csvs — insertCsvToDatabase","text":"user needs plp csv results single folder existing plp result database","code":""},{"path":[]},{"path":"/reference/insertResultsToSqlite.html","id":null,"dir":"Reference","previous_headings":"","what":"Create sqlite database with the results — insertResultsToSqlite","title":"Create sqlite database with the results — insertResultsToSqlite","text":"function create sqlite database PLP result schema inserts results","code":""},{"path":"/reference/insertResultsToSqlite.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create sqlite database with the results — insertResultsToSqlite","text":"","code":"insertResultsToSqlite(   resultLocation,   cohortDefinitions = NULL,   databaseList = NULL,   sqliteLocation = file.path(resultLocation, \"sqlite\") )"},{"path":"/reference/insertResultsToSqlite.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create sqlite database with the results — insertResultsToSqlite","text":"resultLocation (string) location directory main package results saved cohortDefinitions set one cohorts extracted using ROhdsiWebApi::exportCohortDefinitionSet() databaseList list created createDatabaseList specify databases sqliteLocation (string) location directory sqlite database saved","code":""},{"path":"/reference/insertResultsToSqlite.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create sqlite database with the results — insertResultsToSqlite","text":"Returns location sqlite database file","code":""},{"path":"/reference/insertResultsToSqlite.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create sqlite database with the results — insertResultsToSqlite","text":"function can used upload PatientLevelPrediction results sqlite database","code":""},{"path":[]},{"path":"/reference/iterativeImpute.html","id":null,"dir":"Reference","previous_headings":"","what":"Imputation — iterativeImpute","title":"Imputation — iterativeImpute","text":"function single imputation predictive mean matchin","code":""},{"path":"/reference/iterativeImpute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Imputation — iterativeImpute","text":"","code":"iterativeImpute(trainData, featureEngineeringSettings, done = FALSE)"},{"path":"/reference/iterativeImpute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Imputation — iterativeImpute","text":"trainData data imputed featureEngineeringSettings settings imputation done Whether imputation already done (bool)","code":""},{"path":"/reference/iterativeImpute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Imputation — iterativeImpute","text":"imputed data","code":""},{"path":"/reference/listAppend.html","id":null,"dir":"Reference","previous_headings":"","what":"join two lists — listAppend","title":"join two lists — listAppend","text":"join two lists","code":""},{"path":"/reference/listAppend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"join two lists — listAppend","text":"","code":"listAppend(a, b)"},{"path":"/reference/listAppend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"join two lists — listAppend","text":"list b Another list","code":""},{"path":"/reference/listAppend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"join two lists — listAppend","text":"joined list","code":""},{"path":"/reference/listAppend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"join two lists — listAppend","text":"function joins two lists","code":""},{"path":"/reference/listAppend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"join two lists — listAppend","text":"","code":"a <- list(a = 1, b = 2) b <- list(c = 3, d = 4) listAppend(a, b) #> $a #> [1] 1 #>  #> $b #> [1] 2 #>  #> $c #> [1] 3 #>  #> $d #> [1] 4 #>"},{"path":"/reference/listCartesian.html","id":null,"dir":"Reference","previous_headings":"","what":"Cartesian product — listCartesian","title":"Cartesian product — listCartesian","text":"Computes Cartesian product combinations elements list","code":""},{"path":"/reference/listCartesian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cartesian product — listCartesian","text":"","code":"listCartesian(allList)"},{"path":"/reference/listCartesian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cartesian product — listCartesian","text":"allList list lists","code":""},{"path":"/reference/listCartesian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cartesian product — listCartesian","text":"list possible combinations input list lists","code":""},{"path":"/reference/listCartesian.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cartesian product — listCartesian","text":"","code":"listCartesian(list(list(1, 2), list(3, 4))) #> [[1]] #> [[1]]$Var1 #> [1] 1 #>  #> [[1]]$Var2 #> [1] 3 #>  #>  #> [[2]] #> [[2]]$Var1 #> [1] 2 #>  #> [[2]]$Var2 #> [1] 3 #>  #>  #> [[3]] #> [[3]]$Var1 #> [1] 1 #>  #> [[3]]$Var2 #> [1] 4 #>  #>  #> [[4]] #> [[4]]$Var1 #> [1] 2 #>  #> [[4]]$Var2 #> [1] 4 #>  #>"},{"path":"/reference/loadPlpAnalysesJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"Load multiple prediction json settings file","code":""},{"path":"/reference/loadPlpAnalysesJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"","code":"loadPlpAnalysesJson(jsonFileLocation)"},{"path":"/reference/loadPlpAnalysesJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"jsonFileLocation location file 'predictionAnalysisList.json' modelDesignList","code":""},{"path":"/reference/loadPlpAnalysesJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"list modelDesignList cohortDefinitions","code":""},{"path":"/reference/loadPlpAnalysesJson.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"function interprets json multiple prediction settings creates list can combined connection settings run multiple prediction study","code":""},{"path":"/reference/loadPlpAnalysesJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load the multiple prediction json settings from a file — loadPlpAnalysesJson","text":"","code":"modelDesign <- createModelDesign(targetId = 1, outcomeId = 2,                                   modelSettings = setLassoLogisticRegression()) saveLoc <- file.path(tempdir(), \"loadPlpAnalysesJson\") savePlpAnalysesJson(modelDesignList = modelDesign, saveDirectory = saveLoc) #> [1] \"/tmp/RtmpXFoGyv/loadPlpAnalysesJson/predictionAnalysisList.json\" loadPlpAnalysesJson(file.path(saveLoc, \"predictionAnalysisList.json\")) #> $plpVersion #> [1] \"6.4.1\" #>  #> $analyses #> $analyses[[1]] #> $targetId #> [1] 1 #>  #> $outcomeId #> [1] 2 #>  #> $restrictPlpDataSettings #> $studyStartDate #> [1] \"\" #>  #> $studyEndDate #> [1] \"\" #>  #> $firstExposureOnly #> [1] FALSE #>  #> $washoutPeriod #> [1] 0 #>  #> $sampleSize #> NULL #>  #> attr(,\"class\") #> [1] \"restrictPlpDataSettings\" #>  #> $covariateSettings #> $temporal #> [1] FALSE #>  #> $temporalSequence #> [1] FALSE #>  #> $DemographicsGender #> [1] TRUE #>  #> $DemographicsAgeGroup #> [1] TRUE #>  #> $DemographicsRace #> [1] TRUE #>  #> $DemographicsEthnicity #> [1] TRUE #>  #> $DemographicsIndexYear #> [1] TRUE #>  #> $DemographicsIndexMonth #> [1] TRUE #>  #> $ConditionGroupEraLongTerm #> [1] TRUE #>  #> $ConditionGroupEraShortTerm #> [1] TRUE #>  #> $DrugGroupEraLongTerm #> [1] TRUE #>  #> $DrugGroupEraShortTerm #> [1] TRUE #>  #> $DrugGroupEraOverlapping #> [1] TRUE #>  #> $ProcedureOccurrenceLongTerm #> [1] TRUE #>  #> $ProcedureOccurrenceShortTerm #> [1] TRUE #>  #> $DeviceExposureLongTerm #> [1] TRUE #>  #> $DeviceExposureShortTerm #> [1] TRUE #>  #> $MeasurementLongTerm #> [1] TRUE #>  #> $MeasurementShortTerm #> [1] TRUE #>  #> $MeasurementRangeGroupLongTerm #> [1] TRUE #>  #> $MeasurementRangeGroupShortTerm #> [1] TRUE #>  #> $MeasurementValueAsConceptLongTerm #> [1] TRUE #>  #> $MeasurementValueAsConceptShortTerm #> [1] TRUE #>  #> $ObservationLongTerm #> [1] TRUE #>  #> $ObservationShortTerm #> [1] TRUE #>  #> $ObservationValueAsConceptLongTerm #> [1] TRUE #>  #> $ObservationValueAsConceptShortTerm #> [1] TRUE #>  #> $CharlsonIndex #> [1] TRUE #>  #> $Dcsi #> [1] TRUE #>  #> $Chads2 #> [1] TRUE #>  #> $Chads2Vasc #> [1] TRUE #>  #> $includedCovariateConceptIds #> list() #>  #> $includedCovariateIds #> list() #>  #> $addDescendantsToInclude #> [1] FALSE #>  #> $excludedCovariateConceptIds #> list() #>  #> $addDescendantsToExclude #> [1] FALSE #>  #> $shortTermStartDays #> [1] -30 #>  #> $mediumTermStartDays #> [1] -180 #>  #> $endDays #> [1] 0 #>  #> $longTermStartDays #> [1] -365 #>  #> attr(,\"class\") #> [1] \"covariateSettings\" #> attr(,\"fun\") #> [1] \"getDbDefaultCovariateData\" #>  #> $populationSettings #> $binary #> [1] TRUE #>  #> $includeAllOutcomes #> [1] TRUE #>  #> $firstExposureOnly #> [1] FALSE #>  #> $washoutPeriod #> [1] 0 #>  #> $removeSubjectsWithPriorOutcome #> [1] TRUE #>  #> $priorOutcomeLookback #> [1] 99999 #>  #> $requireTimeAtRisk #> [1] TRUE #>  #> $minTimeAtRisk #> [1] 364 #>  #> $riskWindowStart #> [1] 1 #>  #> $startAnchor #> [1] \"cohort start\" #>  #> $riskWindowEnd #> [1] 365 #>  #> $endAnchor #> [1] \"cohort start\" #>  #> $restrictTarToCohortEnd #> [1] FALSE #>  #> attr(,\"class\") #> [1] \"populationSettings\" #>  #> $sampleSettings #> $sampleSettings[[1]] #> $numberOutcomestoNonOutcomes #> [1] 1 #>  #> $sampleSeed #> [1] 1 #>  #> attr(,\"class\") #> [1] \"sampleSettings\" #> attr(,\"fun\") #> [1] \"sameData\" #>  #>  #> $featureEngineeringSettings #> $featureEngineeringSettings[[1]] #> named list() #> attr(,\"class\") #> [1] \"featureEngineeringSettings\" #> attr(,\"fun\") #> [1] \"sameData\" #>  #>  #> $preprocessSettings #> $minFraction #> [1] 0.001 #>  #> $normalize #> [1] TRUE #>  #> $removeRedundancy #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"preprocessSettings\" #>  #> $modelSettings #> $fitFunction #> [1] \"fitCyclopsModel\" #>  #> $param #> $param$priorParams #> $param$priorParams$priorType #> [1] \"laplace\" #>  #> $param$priorParams$forceIntercept #> [1] FALSE #>  #> $param$priorParams$variance #> [1] 0.01 #>  #> $param$priorParams$exclude #> [1] 0 #>  #>  #> $param$includeCovariateIds #> NULL #>  #> $param$upperLimit #> [1] 20 #>  #> $param$lowerLimit #> [1] 0.01 #>  #> $param$priorCoefs #> NULL #>  #> attr(,\"settings\") #> attr(,\"settings\")$priorfunction #> [1] \"Cyclops::createPrior\" #>  #> attr(,\"settings\")$selectorType #> [1] \"byPid\" #>  #> attr(,\"settings\")$crossValidationInPrior #> [1] TRUE #>  #> attr(,\"settings\")$modelType #> [1] \"logistic\" #>  #> attr(,\"settings\")$addIntercept #> [1] TRUE #>  #> attr(,\"settings\")$useControl #> [1] TRUE #>  #> attr(,\"settings\")$seed #> [1] 31725918 #>  #> attr(,\"settings\")$name #> [1] \"Lasso Logistic Regression\" #>  #> attr(,\"settings\")$threads #> [1] -1 #>  #> attr(,\"settings\")$tolerance #> [1] 2e-06 #>  #> attr(,\"settings\")$cvRepetitions #> [1] 1 #>  #> attr(,\"settings\")$maxIterations #> [1] 3000 #>  #> attr(,\"modelType\") #> [1] \"binary\" #> attr(,\"saveType\") #> [1] \"RtoJson\" #>  #> attr(,\"class\") #> [1] \"modelSettings\" #>  #> $splitSettings #> $test #> [1] 0.25 #>  #> $train #> [1] 0.75 #>  #> $seed #> [1] 75317 #>  #> $nfold #> [1] 3 #>  #> attr(,\"class\") #> [1] \"splitSettings\" #> attr(,\"fun\") #> [1] \"randomSplitter\" #>  #> $executeSettings #> $runSplitData #> [1] TRUE #>  #> $runSampleData #> [1] FALSE #>  #> $runFeatureEngineering #> [1] FALSE #>  #> $runPreprocessData #> [1] FALSE #>  #> $runModelDevelopment #> [1] TRUE #>  #> $runCovariateSummary #> [1] TRUE #>  #> attr(,\"class\") #> [1] \"executeSettings\" #>  #> attr(,\"class\") #> [1] \"modelDesign\" #>  #>  #> $cohortDefinitions #> NULL #>  # clean use unlink(saveLoc, recursive = TRUE)"},{"path":"/reference/loadPlpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Load the plpData from a folder — loadPlpData","title":"Load the plpData from a folder — loadPlpData","text":"loadPlpData loads object type plpData folder file system.","code":""},{"path":"/reference/loadPlpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load the plpData from a folder — loadPlpData","text":"","code":"loadPlpData(file, readOnly = TRUE)"},{"path":"/reference/loadPlpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load the plpData from a folder — loadPlpData","text":"file name folder containing data. readOnly true, data opened read .","code":""},{"path":"/reference/loadPlpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load the plpData from a folder — loadPlpData","text":"object class plpData.","code":""},{"path":"/reference/loadPlpData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load the plpData from a folder — loadPlpData","text":"data written set files folder specified user.","code":""},{"path":"/reference/loadPlpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load the plpData from a folder — loadPlpData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n = 500) #> Generating covariates #> Generating cohorts #> Generating outcomes saveLoc <- file.path(tempdir(), \"loadPlpData\") savePlpData(plpData, saveLoc) dir(saveLoc) #> [1] \"cohorts.rds\"  \"covariates\"   \"metaData.rds\" \"outcomes.rds\" \"timeRef.rds\"  # clean up unlink(saveLoc, recursive = TRUE)"},{"path":"/reference/loadPlpModel.html","id":null,"dir":"Reference","previous_headings":"","what":"loads the plp model — loadPlpModel","title":"loads the plp model — loadPlpModel","text":"loads plp model","code":""},{"path":"/reference/loadPlpModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"loads the plp model — loadPlpModel","text":"","code":"loadPlpModel(dirPath)"},{"path":"/reference/loadPlpModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"loads the plp model — loadPlpModel","text":"dirPath location model","code":""},{"path":"/reference/loadPlpModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"loads the plp model — loadPlpModel","text":"","code":"The plpModel object"},{"path":"/reference/loadPlpModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"loads the plp model — loadPlpModel","text":"Loads plp model saved using savePlpModel()","code":""},{"path":[]},{"path":"/reference/loadPlpResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads the evalaution dataframe — loadPlpResult","title":"Loads the evalaution dataframe — loadPlpResult","text":"Loads evalaution dataframe","code":""},{"path":"/reference/loadPlpResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads the evalaution dataframe — loadPlpResult","text":"","code":"loadPlpResult(dirPath)"},{"path":"/reference/loadPlpResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads the evalaution dataframe — loadPlpResult","text":"dirPath directory evaluation saved","code":""},{"path":"/reference/loadPlpResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads the evalaution dataframe — loadPlpResult","text":"","code":"The runPlp object"},{"path":"/reference/loadPlpResult.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Loads the evalaution dataframe — loadPlpResult","text":"Loads evaluation","code":""},{"path":[]},{"path":"/reference/loadPlpShareable.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","title":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","text":"Loads plp result saved json/csv files transparent sharing","code":""},{"path":"/reference/loadPlpShareable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","text":"","code":"loadPlpShareable(loadDirectory)"},{"path":"/reference/loadPlpShareable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","text":"loadDirectory directory results json/csv files","code":""},{"path":"/reference/loadPlpShareable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","text":"","code":"The runPlp object"},{"path":"/reference/loadPlpShareable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Loads the plp result saved as json/csv files for transparent sharing — loadPlpShareable","text":"Load main results json/csv files runPlp object","code":""},{"path":[]},{"path":"/reference/loadPrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads the prediction dataframe to json — loadPrediction","title":"Loads the prediction dataframe to json — loadPrediction","text":"Loads prediction dataframe json","code":""},{"path":"/reference/loadPrediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads the prediction dataframe to json — loadPrediction","text":"","code":"loadPrediction(fileLocation)"},{"path":"/reference/loadPrediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads the prediction dataframe to json — loadPrediction","text":"fileLocation location saved prediction","code":""},{"path":"/reference/loadPrediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads the prediction dataframe to json — loadPrediction","text":"","code":"The prediction data.frame"},{"path":"/reference/loadPrediction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Loads the prediction dataframe to json — loadPrediction","text":"Loads prediciton json file","code":""},{"path":[]},{"path":"/reference/migrateDataModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Migrate Data model — migrateDataModel","title":"Migrate Data model — migrateDataModel","text":"Migrate data current state next state strongly advised backup data (either sqlite files, backup database (case using postgres backend) kept csv/zip files data generation.","code":""},{"path":"/reference/migrateDataModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Migrate Data model — migrateDataModel","text":"","code":"migrateDataModel(connectionDetails, databaseSchema, tablePrefix = \"\")"},{"path":"/reference/migrateDataModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Migrate Data model — migrateDataModel","text":"connectionDetails DatabaseConnector connection details object databaseSchema String schema database schema lives tablePrefix (Optional) Use table prefix used table names (e.g. \"cd_\")","code":""},{"path":"/reference/migrateDataModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Migrate Data model — migrateDataModel","text":"Nothing. called side effects migrating data model database","code":""},{"path":"/reference/minMaxNormalize.html","id":null,"dir":"Reference","previous_headings":"","what":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","title":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","text":"function normalizes continous features values 0 1","code":""},{"path":"/reference/minMaxNormalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","text":"","code":"minMaxNormalize(trainData, featureEngineeringSettings, done = FALSE)"},{"path":"/reference/minMaxNormalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","text":"trainData training data normalized featureEngineeringSettings settings normalization done Whether data already normalized (bool)","code":""},{"path":"/reference/minMaxNormalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","text":"normalized data","code":""},{"path":"/reference/minMaxNormalize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A function that normalizes continous features to have values between 0 and 1 — minMaxNormalize","text":"uses value - min / (max - min) normalize data","code":""},{"path":"/reference/modelBasedConcordance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"Calculate model-based concordance, calculation expected discrimination performance model assumption model predicts \"TRUE\" outcome detailed van Klaveren et al. https://pubmed.ncbi.nlm.nih.gov/27251001/","code":""},{"path":"/reference/modelBasedConcordance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"","code":"modelBasedConcordance(prediction)"},{"path":"/reference/modelBasedConcordance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"prediction prediction object found plpResult object","code":""},{"path":"/reference/modelBasedConcordance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"model-based concordance value","code":""},{"path":"/reference/modelBasedConcordance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"Calculate model-based concordance","code":""},{"path":"/reference/modelBasedConcordance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the model-based concordance, which is a calculation of the expected discrimination performance of a model under the assumption the model predicts the ","text":"","code":"prediction <- data.frame(value = runif(100)) modelBasedConcordance(prediction) #> [1] 0.8331227"},{"path":"/reference/outcomeSurvivalPlot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the outcome incidence over time — outcomeSurvivalPlot","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"Plot outcome incidence time","code":""},{"path":"/reference/outcomeSurvivalPlot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"","code":"outcomeSurvivalPlot(   plpData,   outcomeId,   populationSettings = createStudyPopulationSettings(binary = TRUE, includeAllOutcomes =     TRUE, firstExposureOnly = FALSE, washoutPeriod = 0, removeSubjectsWithPriorOutcome =     TRUE, priorOutcomeLookback = 99999, requireTimeAtRisk = FALSE, riskWindowStart = 1,     startAnchor = \"cohort start\", riskWindowEnd = 3650, endAnchor = \"cohort start\"),   riskTable = TRUE,   confInt = TRUE,   yLabel = \"Fraction of those who are outcome free in target population\" )"},{"path":"/reference/outcomeSurvivalPlot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"plpData plpData object returned running getPlpData() outcomeId cohort id corresponding outcome populationSettings population settings created using createStudyPopulationSettings riskTable (binary) Whether include table bottom  plot showing number people risk time confInt (binary) Whether include confidence interval yLabel (string) label y-axis","code":""},{"path":"/reference/outcomeSurvivalPlot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"ggsurvplot object","code":""},{"path":"/reference/outcomeSurvivalPlot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"creates survival plot can used pick suitable time--risk period","code":""},{"path":"/reference/outcomeSurvivalPlot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the outcome incidence over time — outcomeSurvivalPlot","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=1000) #> Generating covariates #> Generating cohorts #> Generating outcomes plotObject <- outcomeSurvivalPlot(plpData, outcomeId = 3) #> outcomeId: 3 #> binary: TRUE #> includeAllOutcomes: TRUE #> firstExposureOnly: FALSE #> washoutPeriod: 0 #> removeSubjectsWithPriorOutcome: TRUE #> priorOutcomeLookback: 99999 #> requireTimeAtRisk: FALSE #> minTimeAtRisk: 364 #> restrictTarToCohortEnd: FALSE #> riskWindowStart: 1 #> startAnchor: cohort start #> riskWindowEnd: 3650 #> endAnchor: cohort start #> restrictTarToCohortEnd: FALSE #> Removing subjects with prior outcomes (if any) #> Removing subjects with no time at risk (if any) #> Outcome is 0 or 1 #> Population created with: 1000 observations, 1000 unique subjects and 455 outcomes #> Population created in 0.0535 secs print(plotObject)"},{"path":"/reference/pfi.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Feature Importance — pfi","title":"Permutation Feature Importance — pfi","text":"Calculate permutation feature importance (pfi) PLP model.","code":""},{"path":"/reference/pfi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — pfi","text":"","code":"pfi(   plpResult,   population,   plpData,   repeats = 1,   covariates = NULL,   cores = NULL,   log = NULL,   logthreshold = \"INFO\" )"},{"path":"/reference/pfi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — pfi","text":"plpResult object type runPlp population population created using createStudyPopulation() risks predicted plpData object type plpData - patient level prediction data extracted CDM. repeats number times permute covariate covariates vector covariates calculate pfi .  NULL uses covariates included model. cores Number cores use running (runs parallel) log location save log running pfi logthreshold log threshold (e.g., INFO, TRACE, ...)","code":""},{"path":"/reference/pfi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Permutation Feature Importance — pfi","text":"dataframe covariateIds pfi (change AUC caused permuting covariate) value","code":""},{"path":"/reference/pfi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Permutation Feature Importance — pfi","text":"function permutes covariate/features repeats times calculates mean AUC change caused permutation.","code":""},{"path":[]},{"path":"/reference/plotDemographicSummary.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"Plot Observed vs. expected incidence, age gender","code":""},{"path":"/reference/plotDemographicSummary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"","code":"plotDemographicSummary(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotDemographicSummary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotDemographicSummary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotDemographicSummary.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"Create plot showing Observed vs. expected incidence, age gender #'","code":""},{"path":"/reference/plotDemographicSummary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the Observed vs. expected incidence, by age and gender — plotDemographicSummary","text":"","code":"# \\donttest{ data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=1000) #> Generating covariates #> Generating cohorts #> Generating outcomes saveLoc <- file.path(tempdir(), \"plotDemographicSummary\") plpResult <- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc) #> Use timeStamp: TRUE #> Creating save directory at: /tmp/RtmpXFoGyv/plotDemographicSummary/2025-04-14-3 #> Currently in a tryCatch or withCallingHandlers block, so unable to add global calling handlers. ParallelLogger will not capture R messages, errors, and warnings, only explicit calls to ParallelLogger. (This message will not be shown again this R session) #> Patient-Level Prediction Package version 6.4.1 #> Study started at: 2025-04-14 09:15:55.930513 #> AnalysisID:         2025-04-14-3 #> AnalysisName:       Study details #> TargetID:           1 #> OutcomeID:          3 #> Cohort size:        1000 #> Covariates:         98 #> Creating population #> Outcome is 0 or 1 #> Population created with: 970 observations, 970 unique subjects and 452 outcomes #> Population created in 0.0576 secs #> seed: 123 #> Creating a 25% test and 75% train (into 3 folds) random stratified split by class #> Data split into 242 test cases and 728 train cases (243, 243, 242) #> Data split in 0.259 secs #> Train Set: #> Fold 1 243 patients with 113 outcomes - Fold 2 243 patients with 113 outcomes - Fold 3 242 patients with 113 outcomes #> 63 covariates in train data #> Test Set: #> 242 patients with 113 outcomes #> Removing 0 redundant covariates #> Removing 0 infrequent covariates #> Normalizing covariates #> Tidying covariates took 0.701 secs #> Train Set: #> Fold 1 243 patients with 113 outcomes - Fold 2 243 patients with 113 outcomes - Fold 3 242 patients with 113 outcomes #> 63 covariates in train data #> Test Set: #> 242 patients with 113 outcomes #>  #> Running Cyclops #> Done. #> GLM fit status:  OK #> Creating variable importance data frame #> Prediction took 0.109 secs #> Time to fit model: 0.229 secs #> Removing infrequent and redundant covariates and normalizing #> Removing infrequent and redundant covariates covariates and normalizing took 0.153 secs #> Prediction took 0.106 secs #> Prediction done in: 0.343 secs #> Calculating Performance for Test #> ============= #> AUC                 54.85 #> 95% lower AUC:      47.51 #> 95% upper AUC:      62.19 #> AUPRC:              54.72 #> Brier:              0.25 #> Eavg:               0.05 #> Calibration in large- Mean predicted risk 0.4737 : observed risk 0.4669 #> Calibration in large- Intercept -0.0804 #> Weak calibration intercept: -0.0804 - gradient:0.5476 #> Hosmer-Lemeshow calibration gradient: 0.66 intercept:         0.17 #> Average Precision:  0.56 #> Calculating Performance for Train #> ============= #> AUC                 62.86 #> 95% lower AUC:      58.85 #> 95% upper AUC:      66.88 #> AUPRC:              63.57 #> Brier:              0.23 #> Eavg:               0.02 #> Calibration in large- Mean predicted risk 0.4657 : observed risk 0.4657 #> Calibration in large- Intercept 0.0234 #> Weak calibration intercept: 0.0234 - gradient:1.1381 #> Hosmer-Lemeshow calibration gradient: 1.14 intercept:         -0.04 #> Average Precision:  0.64 #> Calculating Performance for CV #> ============= #> AUC                 58.26 #> 95% lower AUC:      54.07 #> 95% upper AUC:      62.46 #> AUPRC:              58.54 #> Brier:              0.23 #> Eavg:               0.04 #> Calibration in large- Mean predicted risk 0.4657 : observed risk 0.4657 #> Calibration in large- Intercept 0.0075 #> Weak calibration intercept: 0.0075 - gradient:1.0481 #> Hosmer-Lemeshow calibration gradient: 1.10 intercept:         -0.07 #> Average Precision:  0.59 #> Time to calculate evaluation metrics: 0.214 secs #> Calculating covariate summary @ 2025-04-14 09:15:57.963456 #> This can take a while... #> Creating binary labels #> Joining with strata #> calculating subset of strata 1 #> calculating subset of strata 2 #> calculating subset of strata 3 #> calculating subset of strata 4 #> Restricting to subgroup #> Calculating summary for subgroup TestWithOutcome #> Restricting to subgroup #> Calculating summary for subgroup TrainWithNoOutcome #> Restricting to subgroup #> Calculating summary for subgroup TrainWithOutcome #> Restricting to subgroup #> Calculating summary for subgroup TestWithNoOutcome #> Aggregating with labels and strata #> Finished covariate summary @ 2025-04-14 09:15:58.873493 #> Time to calculate covariate summary: 0.91 secs #> Run finished successfully. #> Saving PlpResult #> Creating directory to save model #> plpResult saved to ..\\/tmp/RtmpXFoGyv/plotDemographicSummary/2025-04-14-3\\plpResult #> runPlp time taken: 3.03 secs plotDemographicSummary(plpResult)  # clean up  unlink(saveLoc, recursive = TRUE) # }"},{"path":"/reference/plotF1Measure.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","title":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","text":"Plot F1 measure efficiency frontier using sparse thresholdSummary data frame","code":""},{"path":"/reference/plotF1Measure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","text":"","code":"plotF1Measure(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotF1Measure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotF1Measure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotF1Measure.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame — plotF1Measure","text":"Create plot showing F1 measure efficiency frontier using sparse thresholdSummary data frame","code":""},{"path":[]},{"path":"/reference/plotGeneralizability.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the train/test generalizability diagnostic — plotGeneralizability","title":"Plot the train/test generalizability diagnostic — plotGeneralizability","text":"Plot train/test generalizability diagnostic","code":""},{"path":"/reference/plotGeneralizability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the train/test generalizability diagnostic — plotGeneralizability","text":"","code":"plotGeneralizability(   covariateSummary,   saveLocation = NULL,   fileName = \"Generalizability.png\" )"},{"path":"/reference/plotGeneralizability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the train/test generalizability diagnostic — plotGeneralizability","text":"covariateSummary prediction object generated using runPlp function. saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotGeneralizability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the train/test generalizability diagnostic — plotGeneralizability","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotGeneralizability.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the train/test generalizability diagnostic — plotGeneralizability","text":"Create plot showing train/test generalizability diagnostic #'","code":""},{"path":[]},{"path":"/reference/plotLearningCurve.html","id":null,"dir":"Reference","previous_headings":"","what":"plotLearningCurve — plotLearningCurve","title":"plotLearningCurve — plotLearningCurve","text":"Create plot learning curve using object returned createLearningCurve.","code":""},{"path":"/reference/plotLearningCurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plotLearningCurve — plotLearningCurve","text":"","code":"plotLearningCurve(   learningCurve,   metric = \"AUROC\",   abscissa = \"events\",   plotTitle = \"Learning Curve\",   plotSubtitle = NULL,   fileName = NULL )"},{"path":"/reference/plotLearningCurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plotLearningCurve — plotLearningCurve","text":"learningCurve object returned createLearningCurve function. metric Specifies metric plotted: 'AUROC' - use area Receiver Operating Characteristic curve 'AUPRC' - use area Precision-Recall curve 'sBrier' - use scaled Brier score abscissa Specify abscissa metric plotted: 'events' - use number events 'observations' - use number observations plotTitle Title learning curve plot. plotSubtitle Subtitle learning curve plot. fileName Filename plot saved, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotLearningCurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plotLearningCurve — plotLearningCurve","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":[]},{"path":"/reference/plotNetBenefit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the net benefit — plotNetBenefit","title":"Plot the net benefit — plotNetBenefit","text":"Plot net benefit","code":""},{"path":"/reference/plotNetBenefit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the net benefit — plotNetBenefit","text":"","code":"plotNetBenefit(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"netBenefit.png\",   evalType = NULL,   ylim = NULL,   xlim = NULL )"},{"path":"/reference/plotNetBenefit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the net benefit — plotNetBenefit","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats. evalType evaluation type plot . example Test, Train. NULL everything plotted ylim y limits plot, NULL limits calculated data xlim x limits plot, NULL limits calculated data","code":""},{"path":"/reference/plotNetBenefit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the net benefit — plotNetBenefit","text":"list ggplot objects single ggplot object one evaluation type plotted","code":""},{"path":[]},{"path":"/reference/plotPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot all the PatientLevelPrediction plots — plotPlp","title":"Plot all the PatientLevelPrediction plots — plotPlp","text":"Plot PatientLevelPrediction plots","code":""},{"path":"/reference/plotPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot all the PatientLevelPrediction plots — plotPlp","text":"","code":"plotPlp(plpResult, saveLocation = NULL, typeColumn = \"evaluation\")"},{"path":"/reference/plotPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot all the PatientLevelPrediction plots — plotPlp","text":"plpResult Object returned runPlp() function saveLocation Name directory plots saved (NULL means saving) typeColumn name column specifying evaluation type (stratify plots)","code":""},{"path":"/reference/plotPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot all the PatientLevelPrediction plots — plotPlp","text":"TRUE ran, plots saved specified directory","code":""},{"path":"/reference/plotPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot all the PatientLevelPrediction plots — plotPlp","text":"Create directory plots","code":""},{"path":[]},{"path":"/reference/plotPrecisionRecall.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","title":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","text":"Plot precision-recall curve using sparse thresholdSummary data frame","code":""},{"path":"/reference/plotPrecisionRecall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","text":"","code":"plotPrecisionRecall(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotPrecisionRecall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotPrecisionRecall.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotPrecisionRecall.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the precision-recall curve using the sparse thresholdSummary data frame — plotPrecisionRecall","text":"Create plot showing precision-recall curve using sparse thresholdSummary data frame","code":""},{"path":[]},{"path":"/reference/plotPredictedPDF.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","title":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","text":"Plot Predicted probability density function, showing prediction overlap true false cases","code":""},{"path":"/reference/plotPredictedPDF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","text":"","code":"plotPredictedPDF(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"PredictedPDF.png\" )"},{"path":"/reference/plotPredictedPDF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotPredictedPDF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotPredictedPDF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the Predicted probability density function, showing prediction overlap between true and false cases — plotPredictedPDF","text":"Create plot showing predicted probability density function, showing prediction overlap true false cases","code":""},{"path":[]},{"path":"/reference/plotPredictionDistribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","title":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","text":"Plot side--side boxplots prediction distribution, class","code":""},{"path":"/reference/plotPredictionDistribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","text":"","code":"plotPredictionDistribution(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"PredictionDistribution.png\" )"},{"path":"/reference/plotPredictionDistribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotPredictionDistribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotPredictionDistribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the side-by-side boxplots of prediction distribution, by class — plotPredictionDistribution","text":"Create plot showing side--side boxplots prediction distribution, class #'","code":""},{"path":[]},{"path":"/reference/plotPreferencePDF.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","title":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","text":"Plot preference score probability density function, showing prediction overlap true false cases #'","code":""},{"path":"/reference/plotPreferencePDF.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","text":"","code":"plotPreferencePDF(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"plotPreferencePDF.png\" )"},{"path":"/reference/plotPreferencePDF.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotPreferencePDF.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotPreferencePDF.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the preference score probability density function, showing prediction overlap between true and false cases #' — plotPreferencePDF","text":"Create plot showing preference score probability density function, showing prediction overlap true false cases #'","code":""},{"path":[]},{"path":"/reference/plotSmoothCalibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the smooth calibration as detailed in Calster et al. ","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"Plot smooth calibration detailed Calster et al. \"calibration heirarchy risk models defined: utopia empirical data\" (2016)","code":""},{"path":"/reference/plotSmoothCalibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"","code":"plotSmoothCalibration(   plpResult,   smooth = \"loess\",   span = 0.75,   nKnots = 5,   scatter = FALSE,   bins = 20,   sample = TRUE,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"smoothCalibration.pdf\" )"},{"path":"/reference/plotSmoothCalibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"plpResult result running runPlp function. object containing model location model save, data selection settings, preprocessing training settings well various performance measures obtained model. smooth options: 'loess' 'rcs' span specifies width span used loess. allow faster computing lower memory usage. nKnots number knots used rcs evaluation. Default 5 scatter plot decile calibrations points graph. Default False bins number bins histogram. Default 20. sample using loess default 20,000 patients sampled save time typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotSmoothCalibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"ggplot object.","code":""},{"path":"/reference/plotSmoothCalibration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"Create plot showing smoothed calibration","code":""},{"path":"/reference/plotSmoothCalibration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot the smooth calibration as detailed in Calster et al. ","text":"","code":"# generate prediction dataaframe with 1000 patients predictedRisk <- stats::runif(1000) # overconfident for high risk patients actualRisk <- ifelse(predictedRisk < 0.5, predictedRisk, 0.5 + 0.5 * (predictedRisk - 0.5)) outcomeCount <- stats::rbinom(1000, 1, actualRisk) # mock data frame prediction <- data.frame(rowId = 1:1000,                          value = predictedRisk,                           outcomeCount = outcomeCount,                          evaluationType = \"Test\") attr(prediction, \"modelType\") <- \"binary\" calibrationSummary <- getCalibrationSummary(prediction, \"binary\",                                             numberOfStrata = 10,                                             typeColumn = \"evaluationType\") plpResults <- list() plpResults$performanceEvaluation$calibrationSummary <- calibrationSummary plpResults$prediction <- prediction plotSmoothCalibration(plpResults) #> Smooth calibration plot for Test #> $test #> $test$smoothPlot  #>  #> $test$histPlot  #>  #>"},{"path":"/reference/plotSparseCalibration.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the calibration — plotSparseCalibration","title":"Plot the calibration — plotSparseCalibration","text":"Plot calibration","code":""},{"path":"/reference/plotSparseCalibration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the calibration — plotSparseCalibration","text":"","code":"plotSparseCalibration(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotSparseCalibration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the calibration — plotSparseCalibration","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotSparseCalibration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the calibration — plotSparseCalibration","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotSparseCalibration.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the calibration — plotSparseCalibration","text":"Create plot showing calibration #'","code":""},{"path":[]},{"path":"/reference/plotSparseCalibration2.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the conventional calibration — plotSparseCalibration2","title":"Plot the conventional calibration — plotSparseCalibration2","text":"Plot conventional calibration","code":""},{"path":"/reference/plotSparseCalibration2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the conventional calibration — plotSparseCalibration2","text":"","code":"plotSparseCalibration2(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotSparseCalibration2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the conventional calibration — plotSparseCalibration2","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotSparseCalibration2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the conventional calibration — plotSparseCalibration2","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotSparseCalibration2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the conventional calibration — plotSparseCalibration2","text":"Create plot showing calibration #'","code":""},{"path":[]},{"path":"/reference/plotSparseRoc.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","title":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","text":"Plot ROC curve using sparse thresholdSummary data frame","code":""},{"path":"/reference/plotSparseRoc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","text":"","code":"plotSparseRoc(   plpResult,   typeColumn = \"evaluation\",   saveLocation = NULL,   fileName = \"roc.png\" )"},{"path":"/reference/plotSparseRoc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","text":"plpResult plp result object generated using runPlp function. typeColumn name column specifying evaluation type saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotSparseRoc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotSparseRoc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the ROC curve using the sparse thresholdSummary data frame — plotSparseRoc","text":"Create plot showing Receiver Operator Characteristics (ROC) curve.","code":""},{"path":[]},{"path":"/reference/plotVariableScatterplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot the variable importance scatterplot — plotVariableScatterplot","title":"Plot the variable importance scatterplot — plotVariableScatterplot","text":"Plot variable importance scatterplot","code":""},{"path":"/reference/plotVariableScatterplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot the variable importance scatterplot — plotVariableScatterplot","text":"","code":"plotVariableScatterplot(   covariateSummary,   saveLocation = NULL,   fileName = \"VariableScatterplot.png\" )"},{"path":"/reference/plotVariableScatterplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot the variable importance scatterplot — plotVariableScatterplot","text":"covariateSummary prediction object generated using runPlp function. saveLocation Directory save plot (NULL plot saved) fileName Name file save plot, example 'plot.png'. See function ggsave ggplot2 package supported file formats.","code":""},{"path":"/reference/plotVariableScatterplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot the variable importance scatterplot — plotVariableScatterplot","text":"ggplot object. Use ggsave function save file different format.","code":""},{"path":"/reference/plotVariableScatterplot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot the variable importance scatterplot — plotVariableScatterplot","text":"Create plot showing variable importance scatterplot #'","code":""},{"path":[]},{"path":"/reference/pmmFit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictive mean matching using lasso — pmmFit","title":"Predictive mean matching using lasso — pmmFit","text":"Predictive mean matching using lasso","code":""},{"path":"/reference/pmmFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictive mean matching using lasso — pmmFit","text":"","code":"pmmFit(data, k = 5)"},{"path":"/reference/pmmFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictive mean matching using lasso — pmmFit","text":"data andromeda object following fields: xObs: covariates table observed data xMiss: covariates table missing data yObs: outcome variable want impute k number donors use matching (default 5)","code":""},{"path":"/reference/predictCyclops.html","id":null,"dir":"Reference","previous_headings":"","what":"Create predictive probabilities — predictCyclops","title":"Create predictive probabilities — predictCyclops","text":"Create predictive probabilities","code":""},{"path":"/reference/predictCyclops.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create predictive probabilities — predictCyclops","text":"","code":"predictCyclops(plpModel, data, cohort)"},{"path":"/reference/predictCyclops.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create predictive probabilities — predictCyclops","text":"plpModel object type predictiveModel generated using fitPlp. data new plpData containing covariateData new population cohort cohort calculate prediction ","code":""},{"path":"/reference/predictCyclops.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create predictive probabilities — predictCyclops","text":"value column result data.frame : logistic: probabilities outcome, poisson: Poisson rate (per day) outome, survival: hazard rate (per day) outcome.","code":""},{"path":"/reference/predictCyclops.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create predictive probabilities — predictCyclops","text":"Generates predictions population specified plpData given model.","code":""},{"path":[]},{"path":"/reference/predictGlm.html","id":null,"dir":"Reference","previous_headings":"","what":"predict using a logistic regression model — predictGlm","title":"predict using a logistic regression model — predictGlm","text":"Predict risk given plpModel containing generalized linear model.","code":""},{"path":"/reference/predictGlm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predict using a logistic regression model — predictGlm","text":"","code":"predictGlm(plpModel, data, cohort)"},{"path":"/reference/predictGlm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predict using a logistic regression model — predictGlm","text":"plpModel object type plpModel - patient level prediction model data object type plpData - patient level prediction data extracted CDM. cohort population dataframe created using createStudyPopulation risks predicted cohort without outcome known","code":""},{"path":"/reference/predictGlm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predict using a logistic regression model — predictGlm","text":"dataframe containing prediction person population","code":""},{"path":"/reference/predictGlm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"predict using a logistic regression model — predictGlm","text":"","code":"coefficients <- data.frame(   covariateId = c(1002),   coefficient = c(0.05)) model <- createGlmModel(coefficients, intercept = -2.5) data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=50) #> Generating covariates #> Generating cohorts #> Generating outcomes prediction <- predictGlm(model, plpData, plpData$cohorts) #> predict risk probabilities using predictGlm #> Prediction took 0.108 secs # see the predicted risk values head(prediction) #>   rowId subjectId targetId cohortStartDate daysFromObsStart daysToCohortEnd #> 1     1     2e+10        1      2007-12-31              541             320 #> 2     2     2e+10        1      2007-07-14              769               1 #> 3     3     2e+10        1      2010-05-16              195             544 #> 4     4     2e+10        1      2011-10-07              966             842 #> 5     5     2e+10        1      2007-05-08              760             492 #> 6     6     2e+10        1      2010-10-19              732             870 #>   daysToObsEnd ageYear gender     value #> 1          569      39   8507 0.3658644 #> 2          882      40   8532 0.3775407 #> 3          859      38   8507 0.3543437 #> 4         1478      36   8532 0.3318122 #> 5          535      36   8507 0.3318122 #> 6          897      40   8507 0.3775407"},{"path":"/reference/predictPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"predictPlp — predictPlp","title":"predictPlp — predictPlp","text":"Predict risk outcome using input plpModel input plpData","code":""},{"path":"/reference/predictPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"predictPlp — predictPlp","text":"","code":"predictPlp(plpModel, plpData, population, timepoint)"},{"path":"/reference/predictPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"predictPlp — predictPlp","text":"plpModel object type plpModel - patient level prediction model plpData object type plpData - patient level prediction data extracted CDM. population population created using createStudyPopulation() risks predicted cohort without outcome known timepoint timepoint predict risk (survival models )","code":""},{"path":"/reference/predictPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"predictPlp — predictPlp","text":"data frame containing predicted risk values","code":""},{"path":"/reference/predictPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"predictPlp — predictPlp","text":"function applied trained model plpData make predictions","code":""},{"path":"/reference/predictPlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"predictPlp — predictPlp","text":"","code":"coefficients <- data.frame(   covariateId = c(1002),   coefficient = c(0.05) ) model <- createGlmModel(coefficients, intercept = -2.5) data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n = 50) #> Generating covariates #> Generating cohorts #> Generating outcomes prediction <- predictPlp(model, plpData, plpData$cohorts) #> predict risk probabilities using predictGlm #> Prediction took 0.1 secs #> Prediction done in: 0.103 secs # see the predicted risk values head(prediction) #>   rowId subjectId targetId cohortStartDate daysFromObsStart daysToCohortEnd #> 1     1     2e+10        1      2011-08-07              533             751 #> 2     2     2e+10        1      2010-07-31              516             652 #> 3     3     2e+10        1      2009-10-07              559             979 #> 4     4     2e+10        1      2008-12-05              629             460 #> 5     5     2e+10        1      2009-04-08              351             815 #> 6     6     2e+10        1      2011-07-10              914             633 #>   daysToObsEnd ageYear gender     value #> 1         1615      35   8507 0.3208213 #> 2         1042      38   8532 0.3543437 #> 3         1174      40   8507 0.3775407 #> 4          462      39   8507 0.3658644 #> 5         1719      39   8507 0.3658644 #> 6         1304      38   8532 0.3543437"},{"path":"/reference/preprocessData.html","id":null,"dir":"Reference","previous_headings":"","what":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"function wraps around FeatureExtraction::tidyCovariateData normalise data remove rare redundant features","code":""},{"path":"/reference/preprocessData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"","code":"preprocessData(covariateData, preprocessSettings = createPreprocessSettings())"},{"path":"/reference/preprocessData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"covariateData covariate part training data created splitData sampled required feature engineering preprocessSettings settings preprocessing created createPreprocessSettings data processed","code":""},{"path":"/reference/preprocessData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"covariateData object processed covariates","code":""},{"path":"/reference/preprocessData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"Returns object class covariateData processed. includes normalising data removing rare redundant features. Redundant features features within analysisId together cover obervations.","code":""},{"path":"/reference/preprocessData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A function that wraps around FeatureExtraction::tidyCovariateData to normalise the data and remove rare or redundant features — preprocessData","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=1000) #> Generating covariates #> Generating cohorts #> Generating outcomes preProcessedData <- preprocessData(plpData$covariateData, createPreprocessSettings()) #> Removing 2 redundant covariates #> Removing 0 infrequent covariates #> Normalizing covariates #> Tidying covariates took 0.659 secs # check age is normalized by max value preProcessedData$covariates %>% dplyr::filter(.data$covariateId == 1002) #> # Source:   SQL [?? x 3] #> # Database: sqlite 3.47.1 [/tmp/RtmpXFoGyv/file23582750b548.sqlite] #>    rowId covariateId covariateValue #>    <int>       <dbl>          <dbl> #>  1     1        1002          0.787 #>  2     2        1002          0.851 #>  3     3        1002          0.830 #>  4     4        1002          0.809 #>  5     5        1002          0.809 #>  6     6        1002          0.787 #>  7     7        1002          0.766 #>  8     8        1002          0.766 #>  9     9        1002          0.766 #> 10    10        1002          0.787 #> # ℹ more rows"},{"path":"/reference/print.plpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a plpData object — print.plpData","title":"Print a plpData object — print.plpData","text":"Print plpData object","code":""},{"path":"/reference/print.plpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a plpData object — print.plpData","text":"","code":"# S3 method for class 'plpData' print(x, ...)"},{"path":"/reference/print.plpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a plpData object — print.plpData","text":"x plpData object print ... Additional arguments","code":""},{"path":"/reference/print.plpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a plpData object — print.plpData","text":"message describing object","code":""},{"path":"/reference/print.plpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a plpData object — print.plpData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=10) #> Generating covariates #> Generating cohorts #> Generating outcomes print(plpData) #> plpData object #>  #> At risk concept ID: 1 #> Outcome concept ID(s): 3"},{"path":"/reference/print.summary.plpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a summary.plpData object — print.summary.plpData","title":"Print a summary.plpData object — print.summary.plpData","text":"Print summary.plpData object","code":""},{"path":"/reference/print.summary.plpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a summary.plpData object — print.summary.plpData","text":"","code":"# S3 method for class 'summary.plpData' print(x, ...)"},{"path":"/reference/print.summary.plpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a summary.plpData object — print.summary.plpData","text":"x summary.plpData object print ... Additional arguments","code":""},{"path":"/reference/print.summary.plpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a summary.plpData object — print.summary.plpData","text":"message describing object","code":""},{"path":"/reference/print.summary.plpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a summary.plpData object — print.summary.plpData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=10) #> Generating covariates #> Generating cohorts #> Generating outcomes summary <- summary(plpData) print(summary) #> plpData object summary #>  #> At risk cohort concept ID: 1 #> Outcome concept ID(s): 3 #>  #> People: 10 #>  #> Outcome counts: #>   Event count Person count #> 3           4            4 #>  #> Covariates: #> Number of covariates: 98 #> Number of non-zero covariate values: 47"},{"path":"/reference/recalibratePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"recalibratePlp — recalibratePlp","title":"recalibratePlp — recalibratePlp","text":"Recalibrating model using recalibrationInTheLarge weakRecalibration methods","code":""},{"path":"/reference/recalibratePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"recalibratePlp — recalibratePlp","text":"","code":"recalibratePlp(   prediction,   analysisId,   typeColumn = \"evaluationType\",   method = c(\"recalibrationInTheLarge\", \"weakRecalibration\") )"},{"path":"/reference/recalibratePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"recalibratePlp — recalibratePlp","text":"prediction prediction dataframe analysisId model analysisId typeColumn column name strata types specified method Method used recalibrate ('recalibrationInTheLarge' 'weakRecalibration' )","code":""},{"path":"/reference/recalibratePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"recalibratePlp — recalibratePlp","text":"prediction dataframe recalibrated predictions added","code":""},{"path":"/reference/recalibratePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"recalibratePlp — recalibratePlp","text":"'recalibrationInTheLarge' calculates single correction factor average predicted risks match average observed risks. 'weakRecalibration' fits glm model logit predicted risks, also known Platt scaling/logistic recalibration.","code":""},{"path":"/reference/recalibratePlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"recalibratePlp — recalibratePlp","text":"","code":"prediction <- data.frame(rowId = 1:100,                          value = runif(100),                          outcomeCount = stats::rbinom(100, 1, 0.1),                          evaluationType = rep(\"validation\", 100)) attr(prediction, \"metaData\") <- list(modelType = \"binary\") # since value is unformally distributed but outcomeCount is not (prob <- 0.1) # the predictions are mis-calibrated outcomeRate <- mean(prediction$outcomeCount) observedRisk <- mean(prediction$value) message(\"outcome rate is: \", outcomeRate) #> outcome rate is: 0.08 message(\"observed risk is: \", observedRisk) #> observed risk is: 0.515802227428649 # lets recalibrate the predictions prediction <- recalibratePlp(prediction,                               analysisId = \"recalibration\",                               method = \"recalibrationInTheLarge\") recalibratedRisk <- mean(prediction$value) message(\"recalibrated risk with recalibration in the large is: \", recalibratedRisk) #> recalibrated risk with recalibration in the large is: 0.346187758745589 prediction <- recalibratePlp(prediction,                               analysisId = \"recalibration\",                               method = \"weakRecalibration\") recalibratedRisk <- mean(prediction$value) message(\"recalibrated risk with weak recalibration is: \", recalibratedRisk) #> recalibrated risk with weak recalibration is: 0.213093879383669"},{"path":"/reference/recalibratePlpRefit.html","id":null,"dir":"Reference","previous_headings":"","what":"recalibratePlpRefit — recalibratePlpRefit","title":"recalibratePlpRefit — recalibratePlpRefit","text":"Recalibrating model refitting ","code":""},{"path":"/reference/recalibratePlpRefit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"recalibratePlpRefit — recalibratePlpRefit","text":"","code":"recalibratePlpRefit(plpModel, newPopulation, newData, returnModel = FALSE)"},{"path":"/reference/recalibratePlpRefit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"recalibratePlpRefit — recalibratePlpRefit","text":"plpModel trained plpModel (runPlp$model) newPopulation population created using createStudyPopulation() risks predicted newData object type plpData - patient level prediction data extracted CDM. returnModel Logical: return refitted model","code":""},{"path":"/reference/recalibratePlpRefit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"recalibratePlpRefit — recalibratePlpRefit","text":"prediction dataframe predictions recalibrated model added","code":""},{"path":[]},{"path":"/reference/removeRareFeatures.html","id":null,"dir":"Reference","previous_headings":"","what":"A function that removes rare features from the data — removeRareFeatures","title":"A function that removes rare features from the data — removeRareFeatures","text":"function removes rare features data","code":""},{"path":"/reference/removeRareFeatures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function that removes rare features from the data — removeRareFeatures","text":"","code":"removeRareFeatures(trainData, featureEngineeringSettings, done = FALSE)"},{"path":"/reference/removeRareFeatures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function that removes rare features from the data — removeRareFeatures","text":"trainData data normalized featureEngineeringSettings settings normalization done Whether find remove rare features remove (bool)","code":""},{"path":"/reference/removeRareFeatures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function that removes rare features from the data — removeRareFeatures","text":"data rare features removed","code":""},{"path":"/reference/removeRareFeatures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A function that removes rare features from the data — removeRareFeatures","text":"removes features present less certain fraction population","code":""},{"path":"/reference/robustNormalize.html","id":null,"dir":"Reference","previous_headings":"","what":"A function that normalizes continous by the interquartile range and optionally forces the resulting values to be between -3 and 3 with f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr to normalize the data and then can applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values. This forces the values to be between -3 and 3 while preserving the relative ordering of the values. based on https://arxiv.org/abs/2407.04491 for more details — robustNormalize","title":"A function that normalizes continous by the interquartile range and optionally forces the resulting values to be between -3 and 3 with f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr to normalize the data and then can applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values. This forces the values to be between -3 and 3 while preserving the relative ordering of the values. based on https://arxiv.org/abs/2407.04491 for more details — robustNormalize","text":"function normalizes continous interquartile range optionally forces resulting values -3 3 f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr normalize data can applies function f(x) = x / sqrt(1 + (x/3)^2) normalized values. forces values -3 3 preserving relative ordering values. based https://arxiv.org/abs/2407.04491 details","code":""},{"path":"/reference/robustNormalize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A function that normalizes continous by the interquartile range and optionally forces the resulting values to be between -3 and 3 with f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr to normalize the data and then can applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values. This forces the values to be between -3 and 3 while preserving the relative ordering of the values. based on https://arxiv.org/abs/2407.04491 for more details — robustNormalize","text":"","code":"robustNormalize(trainData, featureEngineeringSettings, done = FALSE)"},{"path":"/reference/robustNormalize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A function that normalizes continous by the interquartile range and optionally forces the resulting values to be between -3 and 3 with f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr to normalize the data and then can applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values. This forces the values to be between -3 and 3 while preserving the relative ordering of the values. based on https://arxiv.org/abs/2407.04491 for more details — robustNormalize","text":"trainData training data normalized featureEngineeringSettings settings normalization done Whether data already normalized (bool)","code":""},{"path":"/reference/robustNormalize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A function that normalizes continous by the interquartile range and optionally forces the resulting values to be between -3 and 3 with f(x) = x / sqrt(1 + (x/3)^2) '@details uses (value - median) / iqr to normalize the data and then can applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values. This forces the values to be between -3 and 3 while preserving the relative ordering of the values. based on https://arxiv.org/abs/2407.04491 for more details — robustNormalize","text":"trainData object normalized data","code":""},{"path":"/reference/runMultiplePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a list of predictions analyses — runMultiplePlp","title":"Run a list of predictions analyses — runMultiplePlp","text":"Run list predictions analyses","code":""},{"path":"/reference/runMultiplePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a list of predictions analyses — runMultiplePlp","text":"","code":"runMultiplePlp(   databaseDetails = createDatabaseDetails(),   modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =     setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,     modelSettings = setLassoLogisticRegression())),   onlyFetchData = FALSE,   cohortDefinitions = NULL,   logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = TRUE, logName =     \"runPlp Log\"),   saveDirectory = NULL,   sqliteLocation = file.path(saveDirectory, \"sqlite\") )"},{"path":"/reference/runMultiplePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a list of predictions analyses — runMultiplePlp","text":"databaseDetails database settings created using createDatabaseDetails() modelDesignList list model designs created using createModelDesign() onlyFetchData fetches saves data object output folder without running analysis. cohortDefinitions list cohort definitions target outcome cohorts logSettings setting specifying logging analyses created using createLogSettings() saveDirectory Name folder outputs written . sqliteLocation (optional) location sqlite database results","code":""},{"path":"/reference/runMultiplePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a list of predictions analyses — runMultiplePlp","text":"data frame following columns:","code":""},{"path":"/reference/runMultiplePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a list of predictions analyses — runMultiplePlp","text":"function run specified predictions defined using .","code":""},{"path":[]},{"path":"/reference/runPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","title":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","text":"provides general framework training patient level prediction models.  user can select various default feature selection methods incorporate ,  user can also select range default classifiers incorporate .  three types evaluations model patient (randomly splits people train/validation sets) year (randomly splits data train/validation sets based index year - older training, newer validation) (year spliting checks overlaps patients within training set validaiton set - overlaps removed validation set)","code":""},{"path":"/reference/runPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","text":"","code":"runPlp(   plpData,   outcomeId = plpData$metaData$databaseDetails$outcomeIds[1],   analysisId = paste(Sys.Date(), outcomeId, sep = \"-\"),   analysisName = \"Study details\",   populationSettings = createStudyPopulationSettings(),   splitSettings = createDefaultSplitSetting(type = \"stratified\", testFraction = 0.25,     trainFraction = 0.75, splitSeed = 123, nfold = 3),   sampleSettings = createSampleSettings(type = \"none\"),   featureEngineeringSettings = createFeatureEngineeringSettings(type = \"none\"),   preprocessSettings = createPreprocessSettings(minFraction = 0.001, normalize = TRUE),   modelSettings = setLassoLogisticRegression(),   logSettings = createLogSettings(verbosity = \"DEBUG\", timeStamp = TRUE, logName =     \"runPlp Log\"),   executeSettings = createDefaultExecuteSettings(),   saveDirectory = NULL )"},{"path":"/reference/runPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","text":"plpData object type plpData - patient level prediction data extracted CDM.  Can also include initial population plpData$popualtion. outcomeId (integer) ID outcome. analysisId (integer) Identifier analysis. used create, e.g., result folder. Default timestamp. analysisName (character) Name analysis populationSettings object type populationSettings created using createStudyPopulationSettings specifies data class labels defined addition exclusions apply plpData cohort splitSettings object type splitSettings specifies split data train/validation/test. default settings can created using createDefaultSplitSetting. sampleSettings object type sampleSettings specifies /sampling done. default none. featureEngineeringSettings object featureEngineeringSettings specifying feature engineering learned (using train data) preprocessSettings object preprocessSettings. setting specifies minimum fraction target population must covariate included model training whether normalise covariates training modelSettings object class modelSettings created using one function: setLassoLogisticRegression() lasso logistic regression model setGradientBoostingMachine() gradient boosting machine setAdaBoost() ada boost model setRandomForest() random forest model setDecisionTree() decision tree model setKNN() KNN model logSettings object logSettings created using createLogSettings specifying logging done executeSettings object executeSettings specifying parts analysis run saveDirectory path directory results saved (NULL uses working directory)","code":""},{"path":"/reference/runPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","text":"plpResults object containing following: model developed model class plpModel executionSummary list containing hardward details, R package details execution time performanceEvaluation Various internal performance metrics sparse format prediction plpData cohort table predicted risks added column (named value) covariateSummary characterization features patients without outcome time risk analysisRef list details analysis","code":""},{"path":"/reference/runPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"runPlp - Develop and internally evaluate a model using specified settings — runPlp","text":"function takes input plpData extracted OMOP CDM database follows specified settings develop internally validate model specified outcomeId.","code":""},{"path":[]},{"path":"/reference/savePlpAnalysesJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the modelDesignList to a json file — savePlpAnalysesJson","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"Save modelDesignList json file","code":""},{"path":"/reference/savePlpAnalysesJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"","code":"savePlpAnalysesJson(   modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =     setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,     modelSettings = setLassoLogisticRegression())),   cohortDefinitions = NULL,   saveDirectory = NULL )"},{"path":"/reference/savePlpAnalysesJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"modelDesignList list modelDesigns created using createModelDesign() cohortDefinitions list cohortDefinitions (generally extracted ATLAS) saveDirectory directory save modelDesignList settings","code":""},{"path":"/reference/savePlpAnalysesJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"json string ModelDesignList","code":""},{"path":"/reference/savePlpAnalysesJson.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"function creates json file modelDesignList saved","code":""},{"path":"/reference/savePlpAnalysesJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save the modelDesignList to a json file — savePlpAnalysesJson","text":"","code":"modelDesign <- createModelDesign(targetId = 1,                                   outcomeId = 2,                                  modelSettings = setLassoLogisticRegression()) saveLoc <- file.path(tempdir(), \"loadPlpAnalysesJson\") jsonFile <- savePlpAnalysesJson(modelDesignList = modelDesign, saveDirectory = saveLoc) # clean up unlink(saveLoc, recursive = TRUE)"},{"path":"/reference/savePlpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the plpData to folder — savePlpData","title":"Save the plpData to folder — savePlpData","text":"savePlpData saves object type plpData folder.","code":""},{"path":"/reference/savePlpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the plpData to folder — savePlpData","text":"","code":"savePlpData(plpData, file, envir = NULL, overwrite = FALSE)"},{"path":"/reference/savePlpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the plpData to folder — savePlpData","text":"plpData object type plpData generated using getPlpData. file name folder data written. folder yet exist. envir environment evaluate variables saving overwrite Whether force overwrite existing file","code":""},{"path":"/reference/savePlpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the plpData to folder — savePlpData","text":"Called side effect, data written set files folder specified user.","code":""},{"path":"/reference/savePlpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save the plpData to folder — savePlpData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n = 500) #> Generating covariates #> Generating cohorts #> Generating outcomes saveLoc <- file.path(tempdir(), \"savePlpData\") savePlpData(plpData, saveLoc) dir(saveLoc, full.names = TRUE) #> [1] \"/tmp/RtmpXFoGyv/savePlpData/cohorts.rds\"  #> [2] \"/tmp/RtmpXFoGyv/savePlpData/covariates\"   #> [3] \"/tmp/RtmpXFoGyv/savePlpData/metaData.rds\" #> [4] \"/tmp/RtmpXFoGyv/savePlpData/outcomes.rds\" #> [5] \"/tmp/RtmpXFoGyv/savePlpData/timeRef.rds\"   # clean up unlink(saveLoc, recursive = TRUE)"},{"path":"/reference/savePlpModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves the plp model — savePlpModel","title":"Saves the plp model — savePlpModel","text":"Saves plp model","code":""},{"path":"/reference/savePlpModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves the plp model — savePlpModel","text":"","code":"savePlpModel(plpModel, dirPath)"},{"path":"/reference/savePlpModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves the plp model — savePlpModel","text":"plpModel trained classifier returned running runPlp()$model dirPath location save model ","code":""},{"path":"/reference/savePlpModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Saves the plp model — savePlpModel","text":"","code":"The directory path where the model was saved"},{"path":"/reference/savePlpModel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Saves the plp model — savePlpModel","text":"Saves plp model user specificed folder","code":""},{"path":[]},{"path":"/reference/savePlpResult.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves the result from runPlp into the location directory — savePlpResult","title":"Saves the result from runPlp into the location directory — savePlpResult","text":"Saves result runPlp location directory","code":""},{"path":"/reference/savePlpResult.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves the result from runPlp into the location directory — savePlpResult","text":"","code":"savePlpResult(result, dirPath)"},{"path":"/reference/savePlpResult.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves the result from runPlp into the location directory — savePlpResult","text":"result result running runPlp() dirPath directory save csv","code":""},{"path":"/reference/savePlpResult.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Saves the result from runPlp into the location directory — savePlpResult","text":"","code":"The directory path where the results were saved"},{"path":"/reference/savePlpResult.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Saves the result from runPlp into the location directory — savePlpResult","text":"Saves result runPlp location directory","code":""},{"path":[]},{"path":"/reference/savePlpShareable.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","title":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","text":"Save plp result json files csv files transparent sharing","code":""},{"path":"/reference/savePlpShareable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","text":"","code":"savePlpShareable(result, saveDirectory, minCellCount = 10)"},{"path":"/reference/savePlpShareable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","text":"result object class runPlp development validation results saveDirectory directory save results csv files minCellCount Minimum cell count covariateSummary certain evaluation results","code":""},{"path":"/reference/savePlpShareable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","text":"","code":"The directory path where the results were saved"},{"path":"/reference/savePlpShareable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save the plp result as json files and csv files for transparent sharing — savePlpShareable","text":"Saves main results json/csv files (files can read shiny app)","code":""},{"path":[]},{"path":"/reference/savePrediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves the prediction dataframe to a json file — savePrediction","title":"Saves the prediction dataframe to a json file — savePrediction","text":"Saves prediction dataframe json file","code":""},{"path":"/reference/savePrediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves the prediction dataframe to a json file — savePrediction","text":"","code":"savePrediction(prediction, dirPath, fileName = \"prediction.json\")"},{"path":"/reference/savePrediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves the prediction dataframe to a json file — savePrediction","text":"prediction prediciton data.frame dirPath directory save prediction json fileName name json file saved","code":""},{"path":"/reference/savePrediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Saves the prediction dataframe to a json file — savePrediction","text":"","code":"The file location where the prediction was saved"},{"path":"/reference/savePrediction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Saves the prediction dataframe to a json file — savePrediction","text":"Saves prediction data frame returned predict.R json file returns fileLocation prediction saved","code":""},{"path":"/reference/savePrediction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Saves the prediction dataframe to a json file — savePrediction","text":"","code":"prediction <- data.frame(   rowIds = c(1, 2, 3),   outcomeCount = c(0, 1, 0),   value = c(0.1, 0.9, 0.2) ) saveLoc <- file.path(tempdir()) savePrediction(prediction, saveLoc) #> [1] \"/tmp/RtmpXFoGyv/prediction.json\" dir(saveLoc) #>  [1] \"bslib-9ee1ff8fb0d376991b79e33da4be30c7\" #>  [2] \"downlit\"                                #>  [3] \"file23582368785e\"                       #>  [4] \"file2358263244af.sqlite\"                #>  [5] \"file23582750b548.sqlite\"                #>  [6] \"file235828000332.sqlite\"                #>  [7] \"file2358282d31b8.sqlite\"                #>  [8] \"file23583003969c\"                       #>  [9] \"file235833b5cb12\"                       #> [10] \"file235844e7f429.sqlite\"                #> [11] \"file23585120a655.sqlite\"                #> [12] \"file2358557894cc.sqlite\"                #> [13] \"file23585f48cb1f.sqlite\"                #> [14] \"file23585fe43f5b.sqlite\"                #> [15] \"file2358612619be.sqlite\"                #> [16] \"file235869b96f1b.sqlite\"                #> [17] \"file23586b1dee45.sqlite\"                #> [18] \"file23587101c482.sqlite\"                #> [19] \"file23587740e7a1\"                       #> [20] \"file23587ae33dc3.sqlite\"                #> [21] \"file2358884ce0d.sqlite\"                 #> [22] \"file2358b4a8d10.sqlite\"                 #> [23] \"prediction.json\"                        #> [24] \"temp_libpath235876cc0fc0\"                # clean up unlink(file.path(saveLoc, \"prediction.json\"))"},{"path":"/reference/setAdaBoost.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for AdaBoost with python DecisionTreeClassifier base estimator — setAdaBoost","title":"Create setting for AdaBoost with python DecisionTreeClassifier base estimator — setAdaBoost","text":"Create setting AdaBoost python DecisionTreeClassifier base estimator","code":""},{"path":"/reference/setAdaBoost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for AdaBoost with python DecisionTreeClassifier base estimator — setAdaBoost","text":"","code":"setAdaBoost(   nEstimators = list(10, 50, 200),   learningRate = list(1, 0.5, 0.1),   algorithm = list(\"SAMME\"),   seed = sample(1e+06, 1) )"},{"path":"/reference/setAdaBoost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for AdaBoost with python DecisionTreeClassifier base estimator — setAdaBoost","text":"nEstimators (list) maximum number estimators boosting terminated. case perfect fit, learning procedure stopped early. learningRate (list) Weight applied classifier boosting iteration. higher learning rate increases contribution classifier. trade-learningRate nEstimators parameters trade-learningRate nEstimators. algorithm ‘SAMME’ can provided. 'algorithm' argument deprecated scikit-learn 1.8. seed seed model","code":""},{"path":"/reference/setAdaBoost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for AdaBoost with python DecisionTreeClassifier base estimator — setAdaBoost","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/setCoxModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for lasso Cox model — setCoxModel","title":"Create setting for lasso Cox model — setCoxModel","text":"Create setting lasso Cox model","code":""},{"path":"/reference/setCoxModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for lasso Cox model — setCoxModel","text":"","code":"setCoxModel(   variance = 0.01,   seed = NULL,   includeCovariateIds = c(),   noShrinkage = c(),   threads = -1,   upperLimit = 20,   lowerLimit = 0.01,   tolerance = 2e-07,   maxIterations = 3000 )"},{"path":"/reference/setCoxModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for lasso Cox model — setCoxModel","text":"variance Numeric: prior distribution starting variance seed option add seed training model includeCovariateIds set covariate IDS limit analysis noShrinkage set covariates whcih forced included final model. default intercept threads option set number threads training model upperLimit Numeric: Upper prior variance limit grid-search lowerLimit Numeric: Lower prior variance limit grid-search tolerance Numeric: maximum relative change convergence criterion successive iterations achieve convergence maxIterations Integer: maximum iterations Cyclops attempt returning failed--converge error","code":""},{"path":"/reference/setCoxModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for lasso Cox model — setCoxModel","text":"modelSettings object","code":""},{"path":"/reference/setCoxModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create setting for lasso Cox model — setCoxModel","text":"","code":"coxL1 <- setCoxModel()"},{"path":"/reference/setDecisionTree.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for the scikit-learn DecisionTree with python — setDecisionTree","title":"Create setting for the scikit-learn DecisionTree with python — setDecisionTree","text":"Create setting scikit-learn DecisionTree python","code":""},{"path":"/reference/setDecisionTree.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for the scikit-learn DecisionTree with python — setDecisionTree","text":"","code":"setDecisionTree(   criterion = list(\"gini\"),   splitter = list(\"best\"),   maxDepth = list(as.integer(4), as.integer(10), NULL),   minSamplesSplit = list(2, 10),   minSamplesLeaf = list(10, 50),   minWeightFractionLeaf = list(0),   maxFeatures = list(100, \"sqrt\", NULL),   maxLeafNodes = list(NULL),   minImpurityDecrease = list(10^-7),   classWeight = list(NULL),   seed = sample(1e+06, 1) )"},{"path":"/reference/setDecisionTree.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for the scikit-learn DecisionTree with python — setDecisionTree","text":"criterion function measure quality split. Supported criteria “gini” Gini impurity “entropy” information gain. splitter strategy used choose split node. Supported strategies “best” choose best split “random” choose best random split. maxDepth (list) maximum depth tree. NULL, nodes expanded leaves pure leaves contain less min_samples_split samples. minSamplesSplit minimum number samples required split internal node minSamplesLeaf minimum number samples required leaf node. split point depth considered leaves least minSamplesLeaf training samples left right branches. may effect smoothing model, especially regression. minWeightFractionLeaf minimum weighted fraction sum total weights (input samples) required leaf node. Samples equal weight sampleWeight provided. maxFeatures (list) number features consider looking best split (int/'sqrt'/NULL) maxLeafNodes (list) Grow tree max_leaf_nodes best-first fashion. Best nodes defined relative reduction impurity. None unlimited number leaf nodes. (int/NULL) minImpurityDecrease Threshold early stopping tree growth. node split impurity threshold, otherwise leaf. classWeight (list) Weights associated classes 'balance' NULL seed random state seed","code":""},{"path":"/reference/setDecisionTree.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for the scikit-learn DecisionTree with python — setDecisionTree","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/setGradientBoostingMachine.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","title":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","text":"Create setting gradient boosting machine model using gbm_xgboost implementation","code":""},{"path":"/reference/setGradientBoostingMachine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","text":"","code":"setGradientBoostingMachine(   ntrees = c(100, 300),   nthread = 20,   earlyStopRound = 25,   maxDepth = c(4, 6, 8),   minChildWeight = 1,   learnRate = c(0.05, 0.1, 0.3),   scalePosWeight = 1,   lambda = 1,   alpha = 0,   seed = sample(1e+07, 1) )"},{"path":"/reference/setGradientBoostingMachine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","text":"ntrees number trees build nthread number computer threads use (many cores ?) earlyStopRound performance increase earlyStopRound number trees training stops (prevents overfitting) maxDepth Maximum depth tree - large value lead slow model training minChildWeight Minimum sum instance weight child node - larger values conservative learnRate boosting learn rate scalePosWeight Controls weight positive class loss - useful imbalanced classes lambda L2 regularization weights - larger conservative alpha L1 regularization weights - larger conservative seed option add seed training final model","code":""},{"path":"/reference/setGradientBoostingMachine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","text":"modelSettings object can used fit model","code":""},{"path":"/reference/setGradientBoostingMachine.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create setting for gradient boosting machine model using gbm_xgboost implementation — setGradientBoostingMachine","text":"","code":"modelGbm <- setGradientBoostingMachine(   ntrees = c(10, 100), nthread = 20,   maxDepth = c(4, 6), learnRate = c(0.1, 0.3) )"},{"path":"/reference/setIterativeHardThresholding.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","title":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","text":"Create setting Iterative Hard Thresholding model","code":""},{"path":"/reference/setIterativeHardThresholding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","text":"","code":"setIterativeHardThresholding(   K = 10,   penalty = \"bic\",   seed = sample(1e+05, 1),   exclude = c(),   forceIntercept = FALSE,   fitBestSubset = FALSE,   initialRidgeVariance = 0.1,   tolerance = 1e-08,   maxIterations = 10000,   threshold = 1e-06,   delta = 0 )"},{"path":"/reference/setIterativeHardThresholding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","text":"K maximum number non-zero predictors penalty Specifies IHT penalty; possible values BIC AIC numeric value seed option add seed training model exclude vector numbers covariateId names exclude prior forceIntercept Logical: Force intercept coefficient regularization fitBestSubset Logical: Fit final subset regularization initialRidgeVariance integer tolerance numeric maxIterations integer threshold numeric delta numeric","code":""},{"path":"/reference/setIterativeHardThresholding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","text":"modelSettings object","code":""},{"path":"/reference/setIterativeHardThresholding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create setting for Iterative Hard Thresholding model — setIterativeHardThresholding","text":"","code":"modelIht <- setIterativeHardThresholding(K = 5, seed = 42)"},{"path":"/reference/setLassoLogisticRegression.html","id":null,"dir":"Reference","previous_headings":"","what":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","title":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","text":"Create modelSettings lasso logistic regression","code":""},{"path":"/reference/setLassoLogisticRegression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","text":"","code":"setLassoLogisticRegression(   variance = 0.01,   seed = NULL,   includeCovariateIds = c(),   noShrinkage = c(0),   threads = -1,   forceIntercept = FALSE,   upperLimit = 20,   lowerLimit = 0.01,   tolerance = 2e-06,   maxIterations = 3000,   priorCoefs = NULL )"},{"path":"/reference/setLassoLogisticRegression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","text":"variance Numeric: prior distribution starting variance seed option add seed training model includeCovariateIds set covariateIds limit analysis noShrinkage set covariates whcih forced included final model. Default intercept threads option set number threads training model. forceIntercept Logical: Force intercept coefficient prior upperLimit Numeric: Upper prior variance limit grid-search lowerLimit Numeric: Lower prior variance limit grid-search tolerance Numeric: maximum relative change convergence criterion successive iterations achieve convergence maxIterations Integer: maximum iterations Cyclops attempt returning failed--converge error priorCoefs Use coefficients previous model starting points model fit (transfer learning)","code":""},{"path":"/reference/setLassoLogisticRegression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","text":"modelSettings object","code":""},{"path":"/reference/setLassoLogisticRegression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create modelSettings for lasso logistic regression — setLassoLogisticRegression","text":"","code":"modelLasso <- setLassoLogisticRegression(seed=42)"},{"path":"/reference/setLightGBM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","title":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","text":"Create setting gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package).","code":""},{"path":"/reference/setLightGBM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","text":"","code":"setLightGBM(   nthread = 20,   earlyStopRound = 25,   numIterations = c(100),   numLeaves = c(31),   maxDepth = c(5, 10),   minDataInLeaf = c(20),   learningRate = c(0.05, 0.1, 0.3),   lambdaL1 = c(0),   lambdaL2 = c(0),   scalePosWeight = 1,   isUnbalance = FALSE,   seed = sample(1e+07, 1) )"},{"path":"/reference/setLightGBM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","text":"nthread number computer threads use (many cores ?) earlyStopRound performance increase earlyStopRound number trees training stops (prevents overfitting) numIterations Number boosting iterations. numLeaves hyperparameter sets maximum number leaves. Increasing parameter can lead higher model complexity potential overfitting. maxDepth hyperparameter sets maximum depth . Increasing parameter can also lead higher model complexity potential overfitting. minDataInLeaf hyperparameter sets minimum number data points must present leaf node. Increasing parameter can help reduce overfitting learningRate hyperparameter controls step size iteration gradient descent algorithm. Lower values can lead slower convergence may result better performance. lambdaL1 hyperparameter controls L1 regularization, can help reduce overfitting encouraging sparse models. lambdaL2 hyperparameter controls L2 regularization, can also help reduce overfitting discouraging large weights model. scalePosWeight Controls weight positive class loss - useful imbalanced classes isUnbalance parameter used time scalePosWeight, choose one . enabling increase overall performance metric model, also result poor estimates individual class probabilities. seed option add seed training final model","code":""},{"path":"/reference/setLightGBM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","text":"list settings can used train model runPlp","code":""},{"path":"/reference/setLightGBM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create setting for gradient boosting machine model using lightGBM (https://github.com/microsoft/LightGBM/tree/master/R-package). — setLightGBM","text":"","code":"modelLightGbm <- setLightGBM(   numLeaves = c(20, 31, 50), maxDepth = c(-1, 5, 10),   minDataInLeaf = c(10, 20, 30), learningRate = c(0.05, 0.1, 0.3) )"},{"path":"/reference/setMLP.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for neural network model with python's scikit-learn. For bigger models, consider using DeepPatientLevelPrediction package. — setMLP","title":"Create setting for neural network model with python's scikit-learn. For bigger models, consider using DeepPatientLevelPrediction package. — setMLP","text":"Create setting neural network model python's scikit-learn. bigger models, consider using DeepPatientLevelPrediction package.","code":""},{"path":"/reference/setMLP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for neural network model with python's scikit-learn. For bigger models, consider using DeepPatientLevelPrediction package. — setMLP","text":"","code":"setMLP(   hiddenLayerSizes = list(c(100), c(20)),   activation = list(\"relu\"),   solver = list(\"adam\"),   alpha = list(0.3, 0.01, 1e-04, 1e-06),   batchSize = list(\"auto\"),   learningRate = list(\"constant\"),   learningRateInit = list(0.001),   powerT = list(0.5),   maxIter = list(200, 100),   shuffle = list(TRUE),   tol = list(1e-04),   warmStart = list(TRUE),   momentum = list(0.9),   nesterovsMomentum = list(TRUE),   earlyStopping = list(FALSE),   validationFraction = list(0.1),   beta1 = list(0.9),   beta2 = list(0.999),   epsilon = list(1e-08),   nIterNoChange = list(10),   seed = sample(1e+05, 1) )"},{"path":"/reference/setMLP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for neural network model with python's scikit-learn. For bigger models, consider using DeepPatientLevelPrediction package. — setMLP","text":"hiddenLayerSizes (list vectors) ith element represents number neurons ith hidden layer. activation (list) Activation function hidden layer. \"identity\": -op activation, useful implement linear bottleneck, returns f(x) = x \"logistic\": logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). \"tanh\": hyperbolic tan function, returns f(x) = tanh(x). \"relu\": rectified linear unit function, returns f(x) = max(0, x) solver (list) solver weight optimization. (‘lbfgs’, ‘sgd’, ‘adam’) alpha (list) L2 penalty (regularization term) parameter. batchSize (list) Size minibatches stochastic optimizers. solver ‘lbfgs’, classifier use minibatch. set “auto”, batchSize=min(200, n_samples). learningRate (list) used solver='sgd' Learning rate schedule weight updates. ‘constant’, ‘invscaling’, ‘adaptive’, default=’constant’ learningRateInit (list) used solver=’sgd’ ‘adam’. initial learning rate used. controls step-size updating weights. powerT (list) used solver=’sgd’.  exponent inverse scaling learning rate. used updating effective learning rate learning_rate set ‘invscaling’. maxIter (list)  Maximum number iterations. solver iterates convergence (determined ‘tol’) number iterations. stochastic solvers (‘sgd’, ‘adam’), note determines number epochs (many times data point used), number gradient steps. shuffle (list) boolean: Whether shuffle samples iteration. used solver=’sgd’ ‘adam’. tol (list) Tolerance optimization. loss score improving least tol nIterNoChange consecutive iterations, unless learning_rate set ‘adaptive’, convergence considered reached training stops. warmStart (list) set True, reuse solution previous call fit initialization, otherwise, just erase previous solution. momentum (list) Momentum gradient descent update. 0 1. used solver=’sgd’. nesterovsMomentum (list) Whether use Nesterov’s momentum. used solver=’sgd’ momentum > 0. earlyStopping (list) boolean Whether use early stopping terminate training validation score improving. set true, automatically set aside 10 percent training data validation terminate training validation score improving least tol n_iter_no_change consecutive epochs. validationFraction (list) proportion training data set aside validation set early stopping. Must 0 1. used earlyStopping True. beta1 (list) Exponential decay rate estimates first moment vector adam, 0 1. beta2 (list) Exponential decay rate estimates second moment vector adam, 0 1. epsilon (list) Value numerical stability adam. nIterNoChange (list) Maximum number epochs meet tol improvement. effective solver=’sgd’ ‘adam’. seed seed model","code":""},{"path":"/reference/setMLP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for neural network model with python's scikit-learn. For bigger models, consider using DeepPatientLevelPrediction package. — setMLP","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/setNaiveBayes.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for naive bayes model with python — setNaiveBayes","title":"Create setting for naive bayes model with python — setNaiveBayes","text":"Create setting naive bayes model python","code":""},{"path":"/reference/setNaiveBayes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for naive bayes model with python — setNaiveBayes","text":"","code":"setNaiveBayes()"},{"path":"/reference/setNaiveBayes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for naive bayes model with python — setNaiveBayes","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/setPythonEnvironment.html","id":null,"dir":"Reference","previous_headings":"","what":"Use the python environment created using configurePython() — setPythonEnvironment","title":"Use the python environment created using configurePython() — setPythonEnvironment","text":"Use python environment created using configurePython()","code":""},{"path":"/reference/setPythonEnvironment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use the python environment created using configurePython() — setPythonEnvironment","text":"","code":"setPythonEnvironment(envname = \"PLP\", envtype = NULL)"},{"path":"/reference/setPythonEnvironment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use the python environment created using configurePython() — setPythonEnvironment","text":"envname string name virtual environment (default 'PLP') envtype option specifying environment 'conda' 'python'.  NULL default 'conda' windows users 'python' non-windows users","code":""},{"path":"/reference/setPythonEnvironment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use the python environment created using configurePython() — setPythonEnvironment","text":"string indicating python environment used","code":""},{"path":"/reference/setPythonEnvironment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use the python environment created using configurePython() — setPythonEnvironment","text":"function sets PatientLevelPrediction use python environment","code":""},{"path":[]},{"path":"/reference/setRandomForest.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for random forest model using sklearn — setRandomForest","title":"Create setting for random forest model using sklearn — setRandomForest","text":"Create setting random forest model using sklearn","code":""},{"path":"/reference/setRandomForest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for random forest model using sklearn — setRandomForest","text":"","code":"setRandomForest(   ntrees = list(100, 500),   criterion = list(\"gini\"),   maxDepth = list(4, 10, 17),   minSamplesSplit = list(2, 5),   minSamplesLeaf = list(1, 10),   minWeightFractionLeaf = list(0),   mtries = list(\"sqrt\", \"log2\"),   maxLeafNodes = list(NULL),   minImpurityDecrease = list(0),   bootstrap = list(TRUE),   maxSamples = list(NULL, 0.9),   oobScore = list(FALSE),   nJobs = list(NULL),   classWeight = list(NULL),   seed = sample(1e+05, 1) )"},{"path":"/reference/setRandomForest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for random forest model using sklearn — setRandomForest","text":"ntrees (list) number trees build criterion (list) function measure quality split. Supported criteria “gini” Gini impurity “entropy” information gain. Note: parameter tree-specific. maxDepth (list) maximum depth tree. NULL, nodes expanded leaves pure leaves contain less minSamplesSplit samples. minSamplesSplit (list) minimum number samples required split internal node minSamplesLeaf (list) minimum number samples required leaf node. split point depth considered leaves least minSamplesLeaf training samples left right branches. may effect smoothing model, especially regression. minWeightFractionLeaf (list) minimum weighted fraction sum total weights (input samples) required leaf node. Samples equal weight sampleWeight provided. mtries (list) number features consider looking best split: int consider max_features features split. float max_features fraction round(max_features * n_features) features considered split 'sqrt' max_features=sqrt(n_features) 'log2' max_features=log2(n_features) NULL max_features=n_features maxLeafNodes (list) Grow trees max_leaf_nodes best-first fashion. Best nodes defined relative reduction impurity. None unlimited number leaf nodes. minImpurityDecrease (list) node split split induces decrease impurity greater equal value. bootstrap (list) Whether bootstrap samples used building trees. False, whole dataset used build tree. maxSamples (list) bootstrap True, number samples draw X train base estimator. oobScore (list) Whether use --bag samples estimate generalization score. available bootstrap=True. nJobs number jobs run parallel. classWeight (list) Weights associated classes. given, classes supposed weight one. NULL, “balanced”, “balanced_subsample” seed seed training final model","code":""},{"path":"/reference/setRandomForest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for random forest model using sklearn — setRandomForest","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/setSVM.html","id":null,"dir":"Reference","previous_headings":"","what":"Create setting for the python sklearn SVM (SVC function) — setSVM","title":"Create setting for the python sklearn SVM (SVC function) — setSVM","text":"Create setting python sklearn SVM (SVC function)","code":""},{"path":"/reference/setSVM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create setting for the python sklearn SVM (SVC function) — setSVM","text":"","code":"setSVM(   C = list(1, 0.9, 2, 0.1),   kernel = list(\"rbf\"),   degree = list(1, 3, 5),   gamma = list(\"scale\", 1e-04, 3e-05, 0.001, 0.01, 0.25),   coef0 = list(0),   shrinking = list(TRUE),   tol = list(0.001),   classWeight = list(NULL),   cacheSize = 500,   seed = sample(1e+05, 1) )"},{"path":"/reference/setSVM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create setting for the python sklearn SVM (SVC function) — setSVM","text":"C (list) Regularization parameter. strength regularization inversely proportional C. Must strictly positive. penalty squared l2 penalty. kernel (list) Specifies kernel type used algorithm. one ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’. none given ‘rbf’ used. degree (list) degree kernel function significant poly, rbf, sigmoid gamma (list) kernel coefficient rbf poly, default 1/n_features taken. ‘scale’, ‘auto’ float, default=’scale’ coef0 (list) independent term kernel function. significant poly/sigmoid. shrinking (list) whether use shrinking heuristic. tol (list) Tolerance stopping criterion. classWeight (list) Class weight based imbalance either 'balanced' NULL cacheSize Specify size kernel cache (MB). seed seed model","code":""},{"path":"/reference/setSVM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create setting for the python sklearn SVM (SVC function) — setSVM","text":"modelSettings object","code":""},{"path":[]},{"path":"/reference/simpleImpute.html","id":null,"dir":"Reference","previous_headings":"","what":"Simple Imputation — simpleImpute","title":"Simple Imputation — simpleImpute","text":"function single imputation mean median","code":""},{"path":"/reference/simpleImpute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simple Imputation — simpleImpute","text":"","code":"simpleImpute(trainData, featureEngineeringSettings, done = FALSE)"},{"path":"/reference/simpleImpute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simple Imputation — simpleImpute","text":"trainData data imputed featureEngineeringSettings settings imputation done Whether imputation already done (bool)","code":""},{"path":"/reference/simpleImpute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simple Imputation — simpleImpute","text":"imputed data","code":""},{"path":"/reference/simulatePlpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate simulated data — simulatePlpData","title":"Generate simulated data — simulatePlpData","text":"simulateplpData creates plpData object simulated data.","code":""},{"path":"/reference/simulatePlpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate simulated data — simulatePlpData","text":"","code":"simulatePlpData(plpDataSimulationProfile, n = 10000)"},{"path":"/reference/simulatePlpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate simulated data — simulatePlpData","text":"plpDataSimulationProfile object type plpDataSimulationProfile generated using createplpDataSimulationProfile function. n size population generated.","code":""},{"path":"/reference/simulatePlpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate simulated data — simulatePlpData","text":"object type plpData.","code":""},{"path":"/reference/simulatePlpData.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate simulated data — simulatePlpData","text":"function generates simulated data many ways similar original data simulation profile based.","code":""},{"path":"/reference/simulatePlpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate simulated data — simulatePlpData","text":"","code":"# first load the simulation profile to use data(\"simulationProfile\") # then generate the simulated data plpData <- simulatePlpData(simulationProfile, n = 100) #> Generating covariates #> Generating cohorts #> Generating outcomes nrow(plpData$cohorts) #> [1] 100"},{"path":"/reference/simulationProfile.html","id":null,"dir":"Reference","previous_headings":"","what":"A simulation profile for generating synthetic patient level prediction data — simulationProfile","title":"A simulation profile for generating synthetic patient level prediction data — simulationProfile","text":"simulation profile generating synthetic patient level prediction data","code":""},{"path":"/reference/simulationProfile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simulation profile for generating synthetic patient level prediction data — simulationProfile","text":"","code":"data(simulationProfile)"},{"path":"/reference/simulationProfile.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A simulation profile for generating synthetic patient level prediction data — simulationProfile","text":"data frame containing following elements: covariatePrevalence prevalence covariates outcomeModels regression model parameters simulate outcomes metaData settings used simulate profile covariateRef covariateIds covariateNames timePrevalence time window exclusionPrevalence prevalence exclusion covariates","code":""},{"path":"/reference/sklearnFromJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Loads sklearn python model from json — sklearnFromJson","title":"Loads sklearn python model from json — sklearnFromJson","text":"Loads sklearn python model json","code":""},{"path":"/reference/sklearnFromJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loads sklearn python model from json — sklearnFromJson","text":"","code":"sklearnFromJson(path)"},{"path":"/reference/sklearnFromJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loads sklearn python model from json — sklearnFromJson","text":"path path model json file","code":""},{"path":"/reference/sklearnFromJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loads sklearn python model from json — sklearnFromJson","text":"sklearn python model object","code":""},{"path":[]},{"path":"/reference/sklearnToJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Saves sklearn python model object to json in path — sklearnToJson","title":"Saves sklearn python model object to json in path — sklearnToJson","text":"Saves sklearn python model object json path","code":""},{"path":"/reference/sklearnToJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Saves sklearn python model object to json in path — sklearnToJson","text":"","code":"sklearnToJson(model, path)"},{"path":"/reference/sklearnToJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Saves sklearn python model object to json in path — sklearnToJson","text":"model fitted sklearn python model object path path saved model file","code":""},{"path":"/reference/sklearnToJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Saves sklearn python model object to json in path — sklearnToJson","text":"nothing, saves model path json","code":""},{"path":[]},{"path":"/reference/splitData.html","id":null,"dir":"Reference","previous_headings":"","what":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","title":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","text":"Split plpData test/train sets using splitting settings class splitSettings","code":""},{"path":"/reference/splitData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","text":"","code":"splitData(   plpData = plpData,   population = population,   splitSettings = createDefaultSplitSetting(splitSeed = 42) )"},{"path":"/reference/splitData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","text":"plpData object type plpData - patient level prediction data extracted CDM. population population created using createStudyPopulation define used develop model splitSettings object type splitSettings specifying split - default can created using createDefaultSplitSetting","code":""},{"path":"/reference/splitData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","text":"Returns list containing training data (Train) optionally test data (Test). Train Andromeda object containing covariateRef: table covariate information labels: table (rowId, outcomeCount, ...) data point train data (outcomeCount class label) folds: table (rowId, index) specifying training fold data point . Test Andromeda object containing covariateRef: table covariate information labels: table (rowId, outcomeCount, ...) data point test data (outcomeCount class label)","code":""},{"path":"/reference/splitData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split the plpData into test/train sets using a splitting settings of class splitSettings — splitData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n = 1000) #> Generating covariates #> Generating cohorts #> Generating outcomes population <- createStudyPopulation(plpData) #> Outcome is 0 or 1 #> Population created with: 962 observations, 962 unique subjects and 428 outcomes #> Population created in 0.0474 secs splitSettings <- createDefaultSplitSetting(testFraction = 0.50,                                             trainFraction = 0.50, nfold = 5) data = splitData(plpData, population, splitSettings) #> seed: 1585 #> Creating a 50% test and 50% train (into 5 folds) random stratified split by class #> Data split into 481 test cases and 481 train cases (97, 97, 96, 96, 95) #> Data split in 0.259 secs # test data should be ~500 rows (changes because of study population) nrow(data$Test$labels) #> [1] 481 # train data should be ~500 rows nrow(data$Train$labels) #> [1] 481 # should be five fold in the train data length(unique(data$Train$folds$index)) #> [1] 5"},{"path":"/reference/summary.plpData.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize a plpData object — summary.plpData","title":"Summarize a plpData object — summary.plpData","text":"Summarize plpData object","code":""},{"path":"/reference/summary.plpData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize a plpData object — summary.plpData","text":"","code":"# S3 method for class 'plpData' summary(object, ...)"},{"path":"/reference/summary.plpData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize a plpData object — summary.plpData","text":"object plpData object summarize ... Additional arguments","code":""},{"path":"/reference/summary.plpData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize a plpData object — summary.plpData","text":"summary object containing number people, outcomes covariates","code":""},{"path":"/reference/summary.plpData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize a plpData object — summary.plpData","text":"","code":"data(\"simulationProfile\") plpData <- simulatePlpData(simulationProfile, n=10) #> Generating covariates #> Generating cohorts #> Generating outcomes summary(plpData) #> plpData object summary #>  #> At risk cohort concept ID: 1 #> Outcome concept ID(s): 3 #>  #> People: 10 #>  #> Outcome counts: #>   Event count Person count #> 3           4            4 #>  #> Covariates: #> Number of covariates: 98 #> Number of non-zero covariate values: 44"},{"path":"/reference/toSparseM.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the plpData in COO format into a sparse R matrix — toSparseM","title":"Convert the plpData in COO format into a sparse R matrix — toSparseM","text":"Converts standard plpData sparse matrix","code":""},{"path":"/reference/toSparseM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the plpData in COO format into a sparse R matrix — toSparseM","text":"","code":"toSparseM(plpData, cohort = NULL, map = NULL)"},{"path":"/reference/toSparseM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the plpData in COO format into a sparse R matrix — toSparseM","text":"plpData object type plpData covariate coo format - patient level prediction data extracted CDM. cohort specified plpData restricted rowIds cohort (otherwise plpData$labels used) map covariate map (telling us column number covariates)","code":""},{"path":"/reference/toSparseM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the plpData in COO format into a sparse R matrix — toSparseM","text":"Returns list, containing data sparse matrix, plpData covariateRef data.frame named map tells us covariate corresponds column object list following components: data sparse matrix rows corresponding person plpData columns corresponding covariates. covariateRef plpData covariateRef. map data.frame containing data column ids corresponding covariateId covariateRef.","code":""},{"path":"/reference/toSparseM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert the plpData in COO format into a sparse R matrix — toSparseM","text":"function converts covariates Andromeda table COO format sparse matrix package Matrix","code":""},{"path":[]},{"path":"/reference/validateExternal.html","id":null,"dir":"Reference","previous_headings":"","what":"validateExternal - Validate model performance on new data — validateExternal","title":"validateExternal - Validate model performance on new data — validateExternal","text":"validateExternal - Validate model performance new data","code":""},{"path":"/reference/validateExternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"validateExternal - Validate model performance on new data — validateExternal","text":"","code":"validateExternal(   validationDesignList,   databaseDetails,   logSettings = createLogSettings(verbosity = \"INFO\", logName = \"validatePLP\"),   outputFolder,   cohortDefinitions = NULL )"},{"path":"/reference/validateExternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"validateExternal - Validate model performance on new data — validateExternal","text":"validationDesignList list objects created createValidationDesign databaseDetails list objects class databaseDetails created using createDatabaseDetails logSettings object logSettings created using createLogSettings outputFolder directory save validation results cohortDefinitions cohortDefinitionSet object created CohortGenerator (subfolders created per database validationDatabaseDetails)","code":""},{"path":"/reference/validateExternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"validateExternal - Validate model performance on new data — validateExternal","text":"list results","code":""},{"path":[]},{"path":"/reference/validateMultiplePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"externally validate the multiple plp models across new datasets — validateMultiplePlp","title":"externally validate the multiple plp models across new datasets — validateMultiplePlp","text":"function loads models multiple plp analysis folder validates models new data","code":""},{"path":"/reference/validateMultiplePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"externally validate the multiple plp models across new datasets — validateMultiplePlp","text":"","code":"validateMultiplePlp(   analysesLocation,   validationDatabaseDetails,   validationRestrictPlpDataSettings = createRestrictPlpDataSettings(),   recalibrate = NULL,   cohortDefinitions = NULL,   saveDirectory = NULL )"},{"path":"/reference/validateMultiplePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"externally validate the multiple plp models across new datasets — validateMultiplePlp","text":"analysesLocation location multiple plp analyses validationDatabaseDetails single list validation database settings created using createDatabaseDetails() validationRestrictPlpDataSettings settings specifying extra restriction settings extracting data created using createRestrictPlpDataSettings(). recalibrate vector recalibration methods (currently supports 'RecalibrationintheLarge' /'weakRecalibration') cohortDefinitions list cohortDefinitions saveDirectory location save validation results","code":""},{"path":"/reference/validateMultiplePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"externally validate the multiple plp models across new datasets — validateMultiplePlp","text":"Nothing. results saved saveDirectory","code":""},{"path":"/reference/validateMultiplePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"externally validate the multiple plp models across new datasets — validateMultiplePlp","text":"Users need input location results multiple plp analyses found connection database settings new data","code":""},{"path":[]},{"path":"/reference/viewDatabaseResultPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","title":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","text":"open local shiny app viewing result PLP analyses database","code":""},{"path":"/reference/viewDatabaseResultPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","text":"","code":"viewDatabaseResultPlp(   mySchema,   myServer,   myUser,   myPassword,   myDbms,   myPort = NULL,   myTableAppend )"},{"path":"/reference/viewDatabaseResultPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","text":"mySchema Database result schema containing result tables myServer server result database myUser Username connection result database myPassword Password connection result database myDbms database management system result database myPort Port connection result database myTableAppend string appended results tables (optional)","code":""},{"path":"/reference/viewDatabaseResultPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","text":"Opens shiny app interactively viewing results","code":""},{"path":"/reference/viewDatabaseResultPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"open a local shiny app for viewing the result of a PLP analyses from a database — viewDatabaseResultPlp","text":"Opens shiny app viewing results models database","code":""},{"path":[]},{"path":"/reference/viewMultiplePlp.html","id":null,"dir":"Reference","previous_headings":"","what":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","title":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","text":"open local shiny app viewing result multiple PLP analyses","code":""},{"path":"/reference/viewMultiplePlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","text":"","code":"viewMultiplePlp(analysesLocation)"},{"path":"/reference/viewMultiplePlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","text":"analysesLocation directory containing results (analysis_x folders)","code":""},{"path":"/reference/viewMultiplePlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","text":"Opens shiny app interactively viewing results","code":""},{"path":"/reference/viewMultiplePlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"open a local shiny app for viewing the result of a multiple PLP analyses — viewMultiplePlp","text":"Opens shiny app viewing results models various T,O, Tar settings settings.","code":""},{"path":[]},{"path":"/reference/viewPlp.html","id":null,"dir":"Reference","previous_headings":"","what":"viewPlp - Interactively view the performance and model settings — viewPlp","title":"viewPlp - Interactively view the performance and model settings — viewPlp","text":"shiny app viewing interactive plots performance settings","code":""},{"path":"/reference/viewPlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"viewPlp - Interactively view the performance and model settings — viewPlp","text":"","code":"viewPlp(runPlp, validatePlp = NULL, diagnosePlp = NULL)"},{"path":"/reference/viewPlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"viewPlp - Interactively view the performance and model settings — viewPlp","text":"runPlp output runPlp() (object class 'runPlp') validatePlp output externalValidatePlp (object class 'validatePlp') diagnosePlp output diagnosePlp()","code":""},{"path":"/reference/viewPlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"viewPlp - Interactively view the performance and model settings — viewPlp","text":"Opens shiny app interactively viewing results","code":""},{"path":"/reference/viewPlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"viewPlp - Interactively view the performance and model settings — viewPlp","text":"Either result runPlp view plots","code":""},{"path":[]},{"path":"/news/index.html","id":"patientlevelprediction-641","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.4.1","title":"PatientLevelPrediction 6.4.1","text":"[Bug] Fix bug sampleSize NULL restricting dates getPlpData (PR #559) [Bug] Add cohort definitions external validation (PR #562) [Bug] Copy attributes correctly new FE methods (PR #557) [Bug] Specify settings existing models (GLM Scikit-learn) (PR #555) [Bug] temporal data compatibility (PR #554) [Bug] Specify start endDays cohortCovariates (PR #553) [Bug] Fix bugs uploading cohorts parallel (PR #544)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-640","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.4.0","title":"PatientLevelPrediction 6.4.0","text":"CRAN release: 2025-02-11 Official maintainer updated Egill Fridgeirsson Simple imputation using median mean Iterative imputation chained equations using predictive mean matchin Min/Max normalization Robust normalization normalizes interquartile range optionally squashes features range -3 3 Feature: Added existingSplit settings users can split data bases existing split Feature: Added net benefit plot plotNetBenefit Feature: Added timings printed log functions pipeline Feature: Now model trained scikit-learn python can converted createSklearnModel used predict new data package Feature: Added convenience function getEunomiaPlpData get data one line. Docs: Added new GIS example docs (thanks @jshoughtaling) Docs: exported functions (120) now runnable examples. examples now conditinally executed required. Shiny examples executed interactive sessions, examples using packages suggests executed installed . Docs: Fixed linting errors R codestyle docs conform HADES style Docs: Remove links pdf’s, point website instead. Docs: Fix broken links Readme BuildingPredictiveModels vignette create indexes sqlite backed Andromeda allow us quickly simulate small datasets examples Manually specified outcome model tailored Eunomia Internal: Went tests put skip_if_offline skip_if_not_installed appropriate. Tests using Eunomia need internet tests using suggest packages need skip_if_not_installed. Put skip_on_cran tests requiring python Internal: Cleaned dependencies. Moved moved suggest put behind rlang::check_installed flag. means installation much lighter using basic functionality package, e.g. develop model using Cyclops. CI: Use UV python management github actions upgrade ubuntu version 24.04 CI: Added action detect broken links repo","code":""},{"path":"/news/index.html","id":"patientlevelprediction-639","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.9","title":"PatientLevelPrediction 6.3.9","text":"Hotfix adding schema DatabaseConnector::getTableNames creating results tables","code":""},{"path":"/news/index.html","id":"patientlevelprediction-638","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.8","title":"PatientLevelPrediction 6.3.8","text":"Add support R4.4 Fix notes around documentation (vignette engine brackets itemize) Use webp image format possible (pdfs) smaller size Make sure random table names unique tests Remove remote info Eunomia since ’s CRAN","code":""},{"path":"/news/index.html","id":"patientlevelprediction-637","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.7","title":"PatientLevelPrediction 6.3.7","text":"Clean dependencies, tibble removed IHT ParallelLogger CRAN Use cohortIds cohortCovariates comply FeatureExtraction Add cdmDatabaseName DatabaseDetails model output Fix bug attributes weren’t preserved trainData$covariateData split Fix warnings tests speed Fix bug assignment operator configurePython Delay evaluation plpData using .call like learningCurves runMultiplePlp Speed population generation subjectId’s distinct Fix bug population still generated provided runPlp","code":""},{"path":"/news/index.html","id":"patientlevelprediction-636","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.6","title":"PatientLevelPrediction 6.3.6","text":"fix bug ohdsi shiny modules version check (issue 415)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-635","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.5","title":"PatientLevelPrediction 6.3.5","text":"Fix sklearnToJson compatible scikit-learn>=1.3 Fix github actions ’s hardcoded use python 3.7","code":""},{"path":"/news/index.html","id":"patientlevelprediction-634","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.4","title":"PatientLevelPrediction 6.3.4","text":"added spline feature engineering added age/sex stratified imputation feature engineering changed result table execution date types varchar updated covariateSummary use feature engineering","code":""},{"path":"/news/index.html","id":"patientlevelprediction-633","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.3","title":"PatientLevelPrediction 6.3.3","text":"fixed bug introduced new reticulate update model saving json tests","code":""},{"path":"/news/index.html","id":"patientlevelprediction-632","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.2","title":"PatientLevelPrediction 6.3.2","text":"fixed bug database insert result incomplete updated/fixed documentation (Egill) added model path models (Henrik) updated hyper-parameter saving data.frame made consistent","code":""},{"path":"/news/index.html","id":"patientlevelprediction-631","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.3.1","title":"PatientLevelPrediction 6.3.1","text":"fixed bug multiple covariate settings diagnose plp added min cell count exporting database results csv files light GBM added (thanks Jin Choi Chungsoo Kim) fixed minor bugs uploading results database","code":""},{"path":"/news/index.html","id":"patientlevelprediction-621","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.2.1","title":"PatientLevelPrediction 6.2.1","text":"added ensure_installed(“ResultModelManager”) getDataMigrator()","code":""},{"path":"/news/index.html","id":"patientlevelprediction-610","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.1.0","title":"PatientLevelPrediction 6.1.0","text":"shiny app now using ShinyAppBuilder config saved /inst folder","code":""},{"path":"/news/index.html","id":"patientlevelprediction-6011","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.11","title":"PatientLevelPrediction 6.0.11","text":"fixed bugs introduced sklearn inputs changed added sklearn model saved jsons made changes around DatabaseConnection get table names function make work updated DatabaseConnection removed check RAM stop (now just warns)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-6010","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.10","title":"PatientLevelPrediction 6.0.10","text":"Updated test skip test FE setting model fit (causing occasional test fail) replaced .data$ “” dplyr::select remove warnings","code":""},{"path":"/news/index.html","id":"patientlevelprediction-609","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.9","title":"PatientLevelPrediction 6.0.9","text":"Fix bug python type required int","code":""},{"path":"/news/index.html","id":"patientlevelprediction-608","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.8","title":"PatientLevelPrediction 6.0.8","text":"Allow priorType passed getCV function case prior ‘laplace’ Seed specified Cyclops model wasn’t passed Cyclops","code":""},{"path":"/news/index.html","id":"patientlevelprediction-607","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.7","title":"PatientLevelPrediction 6.0.7","text":"fixed issue shiny viewer converting connection details large json","code":""},{"path":"/news/index.html","id":"patientlevelprediction-606","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.6","title":"PatientLevelPrediction 6.0.6","text":"added check cdmDatabaseId createDatabaseDetails added test check cdmDatabaseId createDatabaseDetails error NULL removed session$onSessionEnded(shiny::stopApp) shiny server","code":""},{"path":"/news/index.html","id":"patientlevelprediction-605","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.5","title":"PatientLevelPrediction 6.0.5","text":"fixing cox predictions","code":""},{"path":"/news/index.html","id":"patientlevelprediction-604","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.4","title":"PatientLevelPrediction 6.0.4","text":"forcing cdmDatabaseId string integer input","code":""},{"path":"/news/index.html","id":"patientlevelprediction-603","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.3","title":"PatientLevelPrediction 6.0.3","text":"replaced utils::read.csv readr::read_csv inserting results csv","code":""},{"path":"/news/index.html","id":"patientlevelprediction-602","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.2","title":"PatientLevelPrediction 6.0.2","text":"replaced gsub sub inserting csvs database","code":""},{"path":"/news/index.html","id":"patientlevelprediction-601","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.1","title":"PatientLevelPrediction 6.0.1","text":"saved result specification csv windows fix odd formating issue","code":""},{"path":"/news/index.html","id":"patientlevelprediction-600","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 6.0.0","title":"PatientLevelPrediction 6.0.0","text":"fixed sample data bugs updated use v1.0.0 OhdsiShinyModules updated plp database result tables use structure cohort database HADES packages added function insert csv results plp database result tables added input databaseId (database version) extracting data consistent HADES packages. saved plp objects.","code":""},{"path":"/news/index.html","id":"patientlevelprediction-544","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.4.4","title":"PatientLevelPrediction 5.4.4","text":"fixed issue ‘preprocess’ vs ‘preprocessing’ inconsistently used across models added metaData tracking feature engineering preprocessing predicting fixed issue FE using trainData$covariateData metaData rather trainData fixed bug using sameData FE","code":""},{"path":"/news/index.html","id":"patientlevelprediction-543","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.4.3","title":"PatientLevelPrediction 5.4.3","text":"pulled multiple bug fixes test improvements Egill pulled fix learning curves Henrik Pulled fix feature engineering Solomon Cleaned check messages comparing class(x) string changing inherits()","code":""},{"path":"/news/index.html","id":"patientlevelprediction-542","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.4.2","title":"PatientLevelPrediction 5.4.2","text":"removed json saving sklearn models since sklearn-json longer working latest sklearn","code":""},{"path":"/news/index.html","id":"patientlevelprediction-541","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.4.1","title":"PatientLevelPrediction 5.4.1","text":"renamed input corresponding string gets appended results table names tablePrefix fixed issues system.file() SqlRender code breaking tests added input fileAppend function exports database tables csv files moved plp model (including preprocessing details) outside result database (specified folder) due size objects (large insert database).","code":""},{"path":"/news/index.html","id":"patientlevelprediction-540","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.4.0","title":"PatientLevelPrediction 5.4.0","text":"added saving plp models result database added default cohortDefinitions runMultiplePlp","code":""},{"path":"/news/index.html","id":"patientlevelprediction-533","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.3.3","title":"PatientLevelPrediction 5.3.3","text":"added modelType models database upload","code":""},{"path":"/news/index.html","id":"patientlevelprediction-532","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.3.2","title":"PatientLevelPrediction 5.3.2","text":"moved FeatureExtraction depends fixed using inherits()","code":""},{"path":"/news/index.html","id":"patientlevelprediction-531","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.3.1","title":"PatientLevelPrediction 5.3.1","text":"moved shiny app code OhdsiShinyModules removed shiny dependencies added OhdsiShinyModules suggests fixed bug linux sklearn saving","code":""},{"path":"/news/index.html","id":"patientlevelprediction-511","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.1.1","title":"PatientLevelPrediction 5.1.1","text":"replaced cohortId targetId consistency throughout code","code":""},{"path":"/news/index.html","id":"patientlevelprediction-510","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.1.0","title":"PatientLevelPrediction 5.1.0","text":"replaced targetId model design cohortId consistency throughout code replaced plpDataSettings restrictPlpDataSettings improve naming consistency added ability use initial population runPlp adding population plpData$population added splitSettings modelDesign replaced saving json settings ParallelLogger function updated database result schema (removed researcher_id tables - desired new table setting_ids researcher_id added, removed study tables revised results table performances table reference model_design_id development_database_id enable validation results without model inserted) added diagnostic code based PROBAST added diagnostic shiny module added code create sqlite database populate uploadToDatabase add code convert runPlp+val sqlite database viewing shiny added code extract database results csv files: extractDatabaseToCsv()","code":""},{"path":"/news/index.html","id":"patientlevelprediction-505","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.0.5","title":"PatientLevelPrediction 5.0.5","text":"pulled GBM update (default hyper-parameters variable importance fix) work done Egill (egillax)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-504","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.0.4","title":"PatientLevelPrediction 5.0.4","text":"updated installation documents added tryCatch around plots prevent code stopping","code":""},{"path":"/news/index.html","id":"patientlevelprediction-503","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.0.3","title":"PatientLevelPrediction 5.0.3","text":"updated result schema (added model_design table settings added attrition table) updated shiny app new database result schema removed C++ code AUC Rcpp dependency, now using pROC instead faster made covariate summary optional externally validating","code":""},{"path":"/news/index.html","id":"patientlevelprediction-502","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.0.2","title":"PatientLevelPrediction 5.0.2","text":"updated json structure specifying study design (made friendlier read) includes smooth calibration plot fix - work done Alex (rekkasa) fixed bug multiple sample methods feature engineering settings causing invalid error","code":""},{"path":"/news/index.html","id":"patientlevelprediction-500","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 5.0.0","title":"PatientLevelPrediction 5.0.0","text":"plpModel now saved json files possible Updated runPlp make modular now possible customise data splitting, feature engineering, sampling (/) learning algorithm added function extracting cohort covariates updated evalaution evaluate per strata (evaluation column) updated plpModel structure updated runPlp structure updated shiny package use tidyr reshape2 sklearn learning algorithms share fit function r learning algorithms share fit function interface cyclops code revised ensemble learning removed (separate package) deep learning removed (DeepPatientLevelPrediction package)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-442","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.4.2","title":"PatientLevelPrediction 4.4.2","text":"revised toSparseM() conversion one go check RAM availablility beforehand. removed temporal plpData conversion toSparseM (done DeepPatientLevelPrediction)","code":""},{"path":"/news/index.html","id":"patientlevelprediction-441","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.4.1","title":"PatientLevelPrediction 4.4.1","text":"shiny can now read csv results objects loaded via loadPlpFromCsv() can saved using savePlpResult()","code":""},{"path":"/news/index.html","id":"patientlevelprediction-440","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.4.0","title":"PatientLevelPrediction 4.4.0","text":"added database result storage added interface database results shiny merged shinyRepo changed shiny app make modular added new features removed deep learning added new OHDSI package DeepPatientLevelPrediction","code":""},{"path":"/news/index.html","id":"patientlevelprediction-4310","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.10","title":"PatientLevelPrediction 4.3.10","text":"save xgboost model json file transparency set connectionDetails NULL getPlpData","code":""},{"path":"/news/index.html","id":"patientlevelprediction-439","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.9","title":"PatientLevelPrediction 4.3.9","text":"updated andromeda functions - restrict pop tidy covs speed quick fix GBM survival predicting negative values fixed occasional demoSum error survival models updated index creation use Andromeda function","code":""},{"path":"/news/index.html","id":"patientlevelprediction-438","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.8","title":"PatientLevelPrediction 4.3.8","text":"fixed bug normalize data false fixed bugs single feature (gbm + python) updated GBM","code":""},{"path":"/news/index.html","id":"patientlevelprediction-437","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.7","title":"PatientLevelPrediction 4.3.7","text":"updated calibration slope fixed missing age/gender prediction fixed shiny intercept bug fixed diagnostic fixed missing covariateSettings load cvs plp","code":""},{"path":"/news/index.html","id":"patientlevelprediction-436","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.6","title":"PatientLevelPrediction 4.3.6","text":"Removed plpData evaluation Added recalibration externalVal Updated shiny app recalibration Added population creation setting use cohortEndDate timeAtRisk end fixed tests","code":""},{"path":"/news/index.html","id":"patientlevelprediction-433","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.3","title":"PatientLevelPrediction 4.3.3","text":"Reduced imports adding code install dependencies used","code":""},{"path":"/news/index.html","id":"patientlevelprediction-432","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.2","title":"PatientLevelPrediction 4.3.2","text":"fixed csv result saving bug model param","code":""},{"path":"/news/index.html","id":"patientlevelprediction-431","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.1","title":"PatientLevelPrediction 4.3.1","text":"fixed r check vignette issues added conda install test","code":""},{"path":"/news/index.html","id":"patientlevelprediction-430","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.3.0","title":"PatientLevelPrediction 4.3.0","text":"finalised permutation feature importance","code":""},{"path":"/news/index.html","id":"patientlevelprediction-4210","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.10","title":"PatientLevelPrediction 4.2.10","text":"fixed deepNN index issue (reported github - thanks dapritchard) add compression python pickles removed requirement outcomeCount prediction python models","code":""},{"path":"/news/index.html","id":"patientlevelprediction-429","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.9","title":"PatientLevelPrediction 4.2.9","text":"cleaned checks fixed bug python toSparseMatrix fixed warning studyPop","code":""},{"path":"/news/index.html","id":"patientlevelprediction-428","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.8","title":"PatientLevelPrediction 4.2.8","text":"fixed bug (identified Chungsoo) covariateSummary fixed bug thresholdSummary edited threshold summary function make cleaner added ensemble can combine multiple models ensemble cleaned notes tests updated simulated data covariateId tests use integer64 fixed description imports (sorted )","code":""},{"path":"/news/index.html","id":"patientlevelprediction-427","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.7","title":"PatientLevelPrediction 4.2.7","text":"fixed Cox model calibration plots fixed int64 conversion bug","code":""},{"path":"/news/index.html","id":"patientlevelprediction-426","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.6","title":"PatientLevelPrediction 4.2.6","text":"added baseline risk Cox model","code":""},{"path":"/news/index.html","id":"patientlevelprediction-423","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.3","title":"PatientLevelPrediction 4.2.3","text":"updated shiny: added attrition hyper-parameter grid search settings","code":""},{"path":"/news/index.html","id":"patientlevelprediction-422","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.2","title":"PatientLevelPrediction 4.2.2","text":"updated shiny app added 95% CI AUC summary, size now complete data size column valPercent tells percentage data used validation","code":""},{"path":"/news/index.html","id":"patientlevelprediction-421","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.1","title":"PatientLevelPrediction 4.2.1","text":"updated GBMsurvival use survival metrics c-stat","code":""},{"path":"/news/index.html","id":"patientlevelprediction-420","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.2.0","title":"PatientLevelPrediction 4.2.0","text":"added survival metrics","code":""},{"path":"/news/index.html","id":"patientlevelprediction-410","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.1.0","title":"PatientLevelPrediction 4.1.0","text":"added updates fixes master development branch","code":""},{"path":"/news/index.html","id":"patientlevelprediction-406","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.6","title":"PatientLevelPrediction 4.0.6","text":"fixed bug pdw data extraction due multiple person_id columns fixed bug shiny app converting covariate values due tibble","code":""},{"path":"/news/index.html","id":"patientlevelprediction-405","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.5","title":"PatientLevelPrediction 4.0.5","text":"added calibration updates: cal--large, weak cal updated smooth cal plot (sample speed big data) defaulted 100 values calibrationSummary + updated cal plot","code":""},{"path":"/news/index.html","id":"patientlevelprediction-404","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.4","title":"PatientLevelPrediction 4.0.4","text":"fixed backwards compat normalization fixed python joblib dependancy","code":""},{"path":"/news/index.html","id":"patientlevelprediction-402","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.2","title":"PatientLevelPrediction 4.0.2","text":"fixed bug preprocessing added cross validation aucs LR, GBM, RF MLP added settings MLP added threads option LR","code":""},{"path":"/news/index.html","id":"patientlevelprediction-401","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.1","title":"PatientLevelPrediction 4.0.1","text":"fixed minor bug shiny dependency fixed tests added standardizedMeanDiff covariatesummary updated createStudyPopulation make cleaner read count outcome per TAR","code":""},{"path":"/news/index.html","id":"patientlevelprediction-400","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 4.0.0","title":"PatientLevelPrediction 4.0.0","text":"Andromeda replaced ff data objects added age/gender cohort fixed python warnings updated shiny plp viewer","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3016","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.16","title":"PatientLevelPrediction 3.0.16","text":"Fixed bug running multiple analyses using data extraction sample multiple covariate settings","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3015","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.15","title":"PatientLevelPrediction 3.0.15","text":"improved shiny PLP viewer added diagnostic shiny viewer","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3014","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.14","title":"PatientLevelPrediction 3.0.14","text":"updated external validate code enable custom covariates using ATLAS cohorts fixed issues startAnchor endAnchor","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3013","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.13","title":"PatientLevelPrediction 3.0.13","text":"Deprecating addExposureDaysToStart addExposureDaysToEnd arguments createStudyPopulation, adding new arguments called startAnchor endAnchor. hope less confusing. fixed transfer learning code (can now transfer fine-tune model) made view plp shiny apps work results missing","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3012","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.12","title":"PatientLevelPrediction 3.0.12","text":"set testing fixed build warnings","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3011","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.11","title":"PatientLevelPrediction 3.0.11","text":"added tests get >70% coverage (keras tests slow travis) Fixed minor bugs Fixed deep learning code removed pythonInR dependancy combined shiny one file one interface","code":""},{"path":"/news/index.html","id":"patientlevelprediction-3010","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.10","title":"PatientLevelPrediction 3.0.10","text":"added recalibration using 25% sample existing models added option provide score probabilities existing models fixed warnings plots","code":""},{"path":"/news/index.html","id":"patientlevelprediction-309","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.9","title":"PatientLevelPrediction 3.0.9","text":"Small bug fixes: - added analysisId model saving/loading - made external validation saving recursive - added removal patients negative TAR creating population - added option apply model without preprocessing settings (make NULL) - updated create study population remove patients negative time--risk","code":""},{"path":"/news/index.html","id":"patientlevelprediction-308","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.8","title":"PatientLevelPrediction 3.0.8","text":"Changes: - merged bug fix Martijn - fixed AUC bug causing crash big data - update SQL code compatible v6.0 OMOP CDM - added save option external validate PLP","code":""},{"path":"/news/index.html","id":"patientlevelprediction-307","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.7","title":"PatientLevelPrediction 3.0.7","text":"Changes: - Updated splitting functions include splitby subject renamed personSplitter randomSplitter - Cast indices integer python functions fix bug non integer sparse matrix indices","code":""},{"path":"/news/index.html","id":"patientlevelprediction-305","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.5","title":"PatientLevelPrediction 3.0.5","text":"Changes: - Added GLM status log (now inform fitting issue log) - Added GBM survival model (still development) - Added RF quantile regression (still development) - Updated viewMultiplePlp() match PLP skeleton package app - Updated single plp vignette additional example - Merge deep learning updates Chan","code":""},{"path":"/news/index.html","id":"patientlevelprediction-304","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.4","title":"PatientLevelPrediction 3.0.4","text":"Changes: - Updated website","code":""},{"path":"/news/index.html","id":"patientlevelprediction-303","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.3","title":"PatientLevelPrediction 3.0.3","text":"Changes: - Added tests - test files now match R files","code":""},{"path":"/news/index.html","id":"patientlevelprediction-302","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.2","title":"PatientLevelPrediction 3.0.2","text":"Changes: - Fixed ensemble stacker","code":""},{"path":"/news/index.html","id":"patientlevelprediction-301","dir":"Changelog","previous_headings":"","what":"PatientLevelPrediction 3.0.1","title":"PatientLevelPrediction 3.0.1","text":"Changes: - Using reticulate python interface - Speed improvements - Bug fixes","code":""}]
